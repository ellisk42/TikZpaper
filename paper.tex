\documentclass{article}

% if you need to pass options to natbib, use, e.g.:
% \PassOptionsToPackage{numbers, compress}{natbib}
% before loading nips_2017
%
% to avoid loading the natbib package, add option nonatbib:
% \usepackage[nonatbib]{nips_2017}

\usepackage{nips_2017}
\usepackage{dsfont}
% to compile a camera-ready version, add the [final] option, e.g.:
% \usepackage[final]{nips_2017}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography

\usepackage{graphicx} % more modern
%\usepackage{epsfig} % less modern
%\usepackage{subfig} 
\usepackage{fancyvrb}

\usepackage{caption}
\usepackage{subcaption}

\fvset{fontsize=\footnotesize}


\usepackage{multirow}
\usepackage{array}

\usepackage{amssymb}
\usepackage{listings}
\usepackage{wrapfig}
\usepackage{tabularx}


\usepackage{verbatim}
 \usepackage{booktabs}
 % For algorithms
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{tikz}
\usetikzlibrary{fit}
\usetikzlibrary{arrows.meta}
\usetikzlibrary{positioning}
\usetikzlibrary{decorations.text}
\usetikzlibrary{decorations.pathmorphing}

% As of 2011, we use the hyperref package to produce hyperlinks in the
% resulting PDF.  If this breaks your system, please commend out the
% following usepackage line and replace \usepackage{icml2016} with
% \usepackage[nohyperref]{icml2016} above.
\usepackage{amsmath}
\usepackage{hyperref}
\DeclareMathOperator*{\argmin}{arg\,min} % thin space, limits underneath in displays
\DeclareMathOperator{\argmin}{argmin} % no space, limits underneath in displays
\DeclareMathOperator*{\argmax}{arg\,max} % thin space, limits underneath in displays
\DeclareMathOperator{\argmax}{argmax} % no space, limits underneath in displays

\newcommand{\expect}{\mathds{E}} %{{\rm I\kern-.3em E}}
\newcommand{\probability}{\mathds{P}} %{{\rm I\kern-.3em P}}

\newcommand{\remark}[1]{\textcolor{red}{[#1]}}


\title{Inferring Graphics Programs from Images}

% The \author macro works with any number of authors. There are two
% commands used to separate the names and addresses of multiple
% authors: \And and \AND.
%
% Using \And between authors leaves it to LaTeX to determine where to
% break the lines. Using \AND forces a line break at that point. So,
% if LaTeX puts 3 of 4 authors names on the first line, and the last
% on the second line, try using \AND instead of \And before the third
% author name.

\author{
  David S.~Hippocampus\thanks{Use footnote for providing further
    information about author (webpage, alternative
    address)---\emph{not} for acknowledging funding agencies.} \\
  Department of Computer Science\\
  Cranberry-Lemon University\\
  Pittsburgh, PA 15213 \\
  \texttt{hippo@cs.cranberry-lemon.edu} \\
  %% examples of more authors
  %% \And
  %% Coauthor \\
  %% Affiliation \\
  %% Address \\
  %% \texttt{email} \\
  %% \AND
  %% Coauthor \\
  %% Affiliation \\
  %% Address \\
  %% \texttt{email} \\
  %% \And
  %% Coauthor \\
  %% Affiliation \\
  %% Address \\
  %% \texttt{email} \\
  %% \And
  %% Coauthor \\
  %% Affiliation \\
  %% Address \\
  %% \texttt{email} \\
}

\begin{document}
% \nipsfinalcopy is no longer used

\maketitle

\begin{abstract}
\end{abstract}

\section{Introduction}

 How could an agent go from noisy, high-dimensional perceptual input
 to a symbolic, abstract object, like a computer program?  Here we
 consider this problem within a graphics program synthesis domain.  We
 develop an approach for converting natural images, such as hand
 drawings, into executable source code for drawing the original image.
 The graphics programs in our domain draw simple figures like those found in
 machine learning papers (see Fig.\ref{firstPageExamples}).
 \remark{The use of `graphics programs / visual programs' in the paper
   title, title of this section, and the body of this section feels
   too broad. `Graphics program' could conjur a lot of different ideas
   (esp. 3D graphics); don't want to set the reader up to expect one
   thing and then be disappointed that what you've done isn't
   that. You bring up diagram-drawing later in the intro; I think it
   should be made clear sooner (and certainly mentioned explicitly in
   the abstract, when you get around to writing that).}

\newcommand{\exampleImageSize}{2cm}
\begin{figure}[h]
  \begin{minipage}[t]{0.7\linewidth}  
\begin{tabular}{llll}
  \includegraphics[width = \exampleImageSize]{figures/expert-60.png}&
  \includegraphics[width = \exampleImageSize]{figures/expert-5.png}&
    \includegraphics[width = \exampleImageSize]{figures/expert-17.png}&
    \includegraphics[width = \exampleImageSize]{figures/expert-58.png}\\
  \includegraphics[width = \exampleImageSize]{figures/60.png}&
  \includegraphics[width = \exampleImageSize]{figures/5.png}&
    \includegraphics[width = \exampleImageSize]{figures/17.png}&
    \includegraphics[width = \exampleImageSize]{figures/58.png}
\end{tabular}
\subcaption{}
  \end{minipage}%
  \begin{minipage}[t]{0.3\linewidth}
    \begin{tikzpicture}
      \node(picture) at (0,0) {\includegraphics[width = 2cm]{figures/expert-31.png}};
      \draw[very thick,->] (picture.south)  -- (0,-2);
    \end{tikzpicture}\\
    \texttt{for} $0\leq i < 4$:\\
    \texttt{rectangle} $(-3i + 9,2i - 2, -3i + 11,6)$\\
    \texttt{for} $0\leq j < i$:\\
    \texttt{circle} $(3i-2,-2j+5)$\\
    \subcaption{}
  \end{minipage}
  \caption{(a): Model learns to convert hand drawings (top) into \LaTeX~(bottom). (b) Synthesizes high-level \emph{graphics program} from hand drawing.}\label{firstPageExamples}
  \end{figure}

High dimensional perceptual input may seem ill matched to the abstract
semantics of a programming language. But programs with constructs like
recursion or iteration produce a simpler \emph{execution trace} of
primitive actions; for our domain the primitive actions are drawing
commands. Our hypothesis is that the execution trace of the program is
better aligned with the perceptual input, and that the trace can act
as a kind of bridge between perception and programs. We test this
hypothesis by developing a model that learns to map from an image to
the execution trace of the graphics program that drew it.  With the
execution trace in hand, we can bring to bear techniques from the
program synthesis community to recover the latent graphics program.
This family of techniques, called \emph{constraint-based program synthesis}~\cite{solar2008program},
work by modeling a set of possible programs inside of a constraint solver,
like a SAT or SMT solver~\cite{de2008z3}.
These techniques excel at uncovering high-level symbolic structure,
but are not well equipped to deal with real-valued perceptual inputs.


We develop a hybrid architecture for inferring graphics programs.  Our
approach uses a deep neural network infer an execution trace from an
image; this network recovers primitive drawing operations such as
lines, circles, or arrows, along with their parameters. For added
robustness, we use the deep network as a proposal distribution for a
stochastic search over execution traces.  Finally, we use techniques
in the program synthesis community to recover the program from its
trace.  The program synthesizer discovers constructs like loops and
geometric operations like reflections and affine transformations.
\remark{This paragraph is all about making things a bit more specific,
  so you really need more specifics about program synth here.}

Each of these three components -- the deep network, the stochastic
search, the program synthesizer -- confers its own advantages. From
the deep network, we get a fast system that can recover plausible
execution traces in about a minute~\remark{A minute seems slow to me, for deep net inference. Are you talking about training time, here, or...?}. From the stochastic search we get
added robustness; essentially, the stochastic search can correct
mistakes made by the deep network's proposals.  From the program
synthesizer, we get abstraction: our system recovers coordinate
transformations, for loops, and subroutines, which are useful for
downstream tasks and can help correct some mistakes of the earlier stages.
\remark{I wonder if this would work even better as a bulleted list...}


\section{Related work}

Our work bears resemblance to the Attend-Infer-Repeat (AIR) system, which learns to decompose an image into its constituent objects~\cite{eslami1603attend}. AIR learns an iterative inference scheme which infers objects one by one and also decides when to stop inference; this is similar to our approach's first stage, which parses images into program execution traces. Our approach further produces interpretable, symbolic programs which generate those execution traces. The two approaches also differ in their architectures and training regimes: AIR learns a recurrent auto-encoding model via variational inference, whereas our parsing stage learns an autoregressive-style model from randomly-generated (execution trace, image) pairs. Finally, while AIR was evaluated on multi-MNIST images and synthetic 3D scenes, we focus on parsing and interpreting hand-drawn sketches.
% attend infer repeat:~\cite{eslami1603attend}. Crucial distinction is
% that they focus on learning the generative model jointly with the
% inference network. Advantages of our system is that we learn symbolic
% programs, and that we do it from hand sketches rather than synthetic
% renderings.

Our image-to-execution-trace parsing architecture builds on prior work on controlling procedural graphics programs~\cite{ritchie2016neurally}. Given a program which generates random 2D recursive structures such as vines, that system learns a structurally-identical ``guide program'' whose output can be directed, via neural networks, to resemble a given target image. 
We adapt this method to a different visual domain (figures composed of multiple objects), using a broad prior over possible scenes as the initial program and viewing the execution trace through the guide program as a symbolic parse of the target image.
We then show how to synthesize higher-level programs from these execution traces.

In the computer graphics literature, there have been other systems which convert sketches into procedural representations. One uses a convolutional network to match a sketch to the output of a parametric 3D modeling system~\cite{huang2017shape}. Another uses convolutional networks to support sketch-based instantiation of procedural primitives within an interactive architectural modeling system~\cite{Nishida:2016:ISU:2897824.2925951}. Both systems focus on inferring fixed-dimensional parameter vectors. In contrast, we seek to automatically learn a structured, programmatic representation of a sketch which captures higher-level visual patterns.

Prior work has also applied sketch-based program synthesis to authoring graphics programs. In particular, Sketch-n-Sketch presents a bi-directional editing system in which direct manipulations to a program's output automatically propagate to the program source code~\cite{Hempel:2016:SSP:2984511.2984575}. We see this work as complementary to our own: programs produced by our method could be provided to a Sketch-n-Sketch-like system as a starting point for further editing.

\remark{Do you also want to cite your own work on ``Unsupverised Learning by Program Synthesis'' here?}



\section{Neural architecture for inferring drawing execution traces}

We developed a deep network architecture for efficiently inferring a
execution trace, $T$, from an image, $I$.  Our model constructs the
trace one drawing command at a time.  When predicting the next drawing
command, the network takes as input the target image $I$ as well as
the rendered output of previous drawing commands.  Intuitively, the
network looks at the image it wants to explain, as well as what it has
already drawn.  It then decides either to stop drawing or proposes
another drawing command to add to the execution trace; if it decides
to continue drawing, the predicted primitive is rendered to its
``canvas'' and the process repeats.

Figure~\ref{architecture} illustrates this architecture.  We first
pass a $256\times 256$ target image and a rendering of the trace so
far to a convolutional network -- these two inputs are represented as
separate channels for the convnet. Given the features extracted by the
convnet, a multilayer perceptron then predicts a distribution over the
next drawing command to add to the trace.  We predict the drawing
command token-by-token, and condition each token both on the image
features and on the previously generated tokens.  For example, the
network first decides to emit the \verb|circle| token conditioned on
the image features, then it emits the $x$ coordinate of the circle
conditioned on the image features and the \verb|circle| token, and
finally it predicts the $y$ coordinate of the circle conditioned on
the image features, the \verb|circle| token, and the $x$ coordinate.
\remark{There are some more details that are important to provide
  about this architecture, though possibly in an Appendix: the
  functional form(s) of the probability distributions over tokens, the
  network layer sizes, which MLPs share parameters, etc.}

\remark{Planning to move the description of SMC / beam search up here, too?}

The distribution over the next drawing command factorizes:
\begin{equation}
  \probability_\theta [t_1t_2\cdots t_K | I,T] = \prod_{k = 1}^K \probability_\theta [t_k | f_\theta(I,\text{render}(T)), \{t_j\}_{j = 1}^{k - 1}]
\end{equation}
where $t_1t_2\cdots t_K$ are the tokens in the drawing command, $I$ is
the target image, $T$ is an execution trace, $\theta$ are the
parameters of the neural network, and $f_\theta(\cdot,\cdot)$ is the
image feature extractor (convolutional network). The distribution over
execution traces factorizes as:
\begin{equation}
  \probability_\theta [T|I] = \prod_{n = 1}^{|T|} \probability_\theta [T_n | I,T_{1:(n-1)}]\times\probability_\theta [\verb|STOP| | I,T]\label{objective}
\end{equation}
where $|T|$ is the length of execution trace $T$, and the \verb|STOP|
token is emitted by the network to signal that the execution trace
explains the image.

We train the network by sampling execution traces $T$ and target
images $I$ for randomly generated scenes, and maximizing
(\ref{objective}) wrt $\theta$ by gradient ascent.
Training does not require backpropagation across the entire sequence of drawing commands:
drawing to the canvas `blocks' the gradients,
effectively offloading memory to an external visual store.
In a sense, this model is like an autoregressive variant of AIR~\cite{eslami1603attend} without attention.

\remark{I like that you make this connection, but it could be made more precisely. Specifically, (1) the architecture isn't \emph{really} recurrent (it uses no hidden state cells), so it'd be good to use a different term or drop this part of the point: (2) training of recurrent nets is also typically fully-supervised (Most RNNs lack latent variables per timestep)---if you're thinking about AIR specifically, maybe just say that, and (3) it's like an autogressive AIR \emph{without attention}.}
\remark{Something related to this that's also cool to point out: training this model doesn't require backpropagation across the entire sequence of drawing commands (drawing to the canvas `blocks' the gradients, effectively offloading memory to an external (visual) store, so in principle it might be scalable to much longer sequences.}

%% When we
%% have the generative model (the rendering function) and treat the trace
%% as fully observed, we need not solve an unsupervised or reinforcement
%% learning problem.
\tikzset{>=latex}
\begin{figure}
  \begin{tikzpicture}
  \node[draw,blue,ultra thick,anchor = west,inner sep=0pt,label=below:Target image: $I$](observation) at (0,-1) {\includegraphics[width = 2cm]{figures/expert-18.png}};
  \node[draw,blue,thick,anchor = west,inner sep=0pt,minimum width = 2cm,minimum height = 2cm,label=below:Canvas: render$(T)$] (canvas) at (0,-4) {};
  \draw[lightgray,ultra thin,step = 0.125] ([xshift = 0.5,yshift = 0.5]canvas.south west) grid ([xshift = -0.5,yshift = -0.5]canvas.north east);
  % draw partial image on canvas
  \draw (0.5cm,-4cm) circle (0.1875cm);
  \draw (1.2cm,-4.3) circle (0.1875cm);
  \draw (0.85cm,-4.6) circle (0.1875cm);

  \node[draw,ultra thick,anchor = west,inner sep=0pt,minimum width = 2cm,minimum height = 3cm] (CNN) at (4.5,-2.5) {CNN};
  \node[inner sep = 0pt](tensorProduct) at ([xshift = -1.5cm]CNN.west) {$\bigotimes$};

  \node[rotate = -90,draw,ultra thick,inner sep=0pt,minimum width = 3cm,minimum height = 0.5cm] (features) at ([xshift = 1.5cm]CNN.east) {Image features};

  \node[draw,ultra thick,minimum size = 1cm](c1) at ([xshift = 1.5cm]features.north) {MLP};
  \node(l1) at ([yshift = -2cm]c1.south) {\verb|circle(|};
  \node[draw,ultra thick,minimum size = 1cm](c2) at ([xshift = 1cm]c1.east) {MLP};
  \draw[->,ultra thick,red] (c1.south) -- (l1.north);
  \node(l2) at ([yshift = -2cm]c2.south) {\verb|X=3,|};
  \draw[->,ultra thick,red] (c2.south) -- (l2.north);
  \node[draw,ultra thick,minimum size = 1cm](c3) at ([xshift = 1cm]c2.east) {MLP};
  \node(l3) at ([yshift = -2cm]c3.south) {\verb|Y=14)|};
  \draw[->,ultra thick,red] (c3.south) -- (l3.north);

  
  \draw[->,ultra thick] (features.north) -- (c1.west);
  \draw[->,ultra thick] (features.north) to[out = 45,in = 90] (c2.north);
  \draw[->,ultra thick] (features.north) to[out = 70,in = 90] (c3.north);
  \draw[->,ultra thick] ([xshift = 0.25cm]l1.north) -- (c2.west);
  \draw[->,ultra thick] ([xshift = 0.25cm]l1.north) -- (c3.west);
  \draw[->,ultra thick] ([xshift = 0.25cm]l2.north) -- ([yshift = -0.2cm]c3.west);

  \node(next)[draw,very thick,fit = (l1) (l2) (l3), dashed, label = below:{Next line of code}] {};

  \draw[-{>[scale = 1.5]},very thick,dashed] (next.west) -- ([yshift = -0.2cm]canvas.east) node [midway, below, sloped] (TextNode) {Renderer: \LaTeX};
  
  \draw[->,ultra thick] (canvas.east) -- (tensorProduct.south);%[yshift = -0.5cm]CNN.west);
  \draw[->,ultra thick] (observation.east) -- (tensorProduct.north);%([yshift = 0.5cm]CNN.west);
  \draw[->,ultra thick] (tensorProduct.east)  -- node[center,fill = white,rotate = 90] {{\tiny $256\times 256\times 2$}}  (CNN.west);
  \draw[->,ultra thick] (CNN.east) -- node[center,fill = white,rotate = 90] {{\tiny $16\times 16\times 10$}} (features.south);
%  \draw[]
  
%  \node at (canvas.x,canvas.y) {Canvas};
\end{tikzpicture}
\caption{Our neural architecture for inferring the execution trace of a graphics program from its output. \textcolor{blue}{Blue}: network inputs. Black: network operations. \textcolor{red}{Red}: samples from a multinomial. \texttt{Typewriter font}: network outputs. Renders snapped to a $16\times 16$ grid, illustrated in \textcolor{gray}{gray}.~\remark{Thoughts on improving this figure: (1) Convnet diagrams typically show the sequence of layers, if possible (space might not permit it here, but those thin arrows just aren't doing it for me). (2) Are the target image / canvas convolved down independently, or jointly (i.e. starting as a 2-channel image)? That's an important detail that's not clear with the current figure/explanation. (3) The three circles downstream from `Image Features' are supposed to be MLPs, I assume(?), but it took me a little while to parse that. Having some visual way of clearly separating network operations from data (color, perhaps) would go a long way.}}  \label{architecture}
\end{figure}
\begin{table}
\begin{tabular}{ll}\toprule
  \begin{tabular}{l}
    \verb|circle|$(x,y)$
  \end{tabular}& \begin{tabular}{l}
    Circle at $(x,y)$
    \end{tabular}\\
  \begin{tabular}{l}
    \verb|rectangle|$(x_1,y_1,x_2,y_2)$
  \end{tabular}&\begin{tabular}{l}
    Rectangle with corners at $(x_1,y_1)$ \& $(x_2,y_2)$
    \end{tabular}\\
  \begin{tabular}{l}
    \verb|LINE|$(x_1,y_1,x_2,y_2,$\\
    \hspace{1cm}$\text{arrow}\in\{0,1\},\text{dashed}\in\{0,1\})$
  \end{tabular}&\begin{tabular}{l}
    Line from $(x_1,y_1)$ to  $(x_2,y_2)$,\\\hspace{1cm}optionally with an arrow and/or dashed
    \end{tabular}\\
  \begin{tabular}{l}
    \verb|STOP|
  \end{tabular}&\begin{tabular}{l}
    Finishes execution trace inference
    \end{tabular}
\\  \bottomrule
\end{tabular}
\caption{The deep network in (\ref{architecture}) predicts drawing commands, shown above.}
\label{drawingCommandTable}
\end{table}



This network suffices to ``derender'' images like those shown in
Figure~\ref{trainingData}.  We can perform a beam search decoding to
recover what the network thinks is the most likely execution trace for
images like these. But, if the network makes a mistake (predicts an
incorrect line of code), it has no way of recovering from the error.
In order to derender an image with $n$ objects, it must correctly
predict $n$ drawing commands -- so its probability of success will
decrease exponentially in $n$, assuming it has any nonzero chance of
making a mistake.  For added robustness as $n$ becomes large, we treat
the neural network outputs as proposals for a SMC sampling scheme.
For the SMC sampler, we use pixel wise distance as a surrogate for a
likelihood function; see supplement. Figure~\ref{syntheticResults}
compares the neural network with SMC against the neural network by
itself or SMC by itself.  Only the combination of the two passes a
critical test of generalization: when trained on images with $\leq 8$
objects, it successfully parses scenes with many more objects than the
training data.
\begin{wrapfigure}{R}{0.5\textwidth}
  \begin{center}
    \begin{minipage}[t]{2.2cm}\fbox{\includegraphics[width=2cm]{figures/randomScene-58-7.png}}\end{minipage}
    \begin{minipage}[t]{2.2cm}\fbox{\includegraphics[width=2cm]{figures/randomScene-33-5.png}}\end{minipage}
    \begin{minipage}[t]{2.2cm}\fbox{\includegraphics[width=2cm]{figures/randomScene-25-4.png}}\end{minipage}
%    \begin{minipage}[t]{2cm}\includegraphics[width=2cm]{figures/randomScene-26-7.png}\end{minipage}
%    \begin{minipage}[t]{2cm}\includegraphics[width=2cm]{figures/randomScene-19-3.png}\end{minipage}
  \end{center}
  \caption{Network is trained to infer execution traces for figures like the three shown above.}\label{trainingData}
\end{wrapfigure}

\begin{figure}
  \begin{minipage}[t]{15cm}
    \includegraphics[width = 15cm]{figures/editDistance.png}
  \end{minipage}\\
  \begin{minipage}[t]{15cm}
    \includegraphics[width = 15cm]{figures/accuracy.png}
  \end{minipage}\\
  \begin{minipage}[t]{15cm}
    \includegraphics[width = 15cm]{figures/time.png}
  \end{minipage}
  \caption{Using the model to parse latex output. The model is trained on diagrams with up to 8 objects. As shown above it generalizes to scenes with many more objects. Neither the stochastic search nor the neural network are sufficient on their own.}\label{syntheticResults}
\end{figure}

\subsection{Generalizing to hand drawings}
We believe that converting synthetic, noiseless images into a restricted subset of
\LaTeX has limited usefulness.  A more practical
application is one that extends to hand drawings.  We train the model
to generalize to hand drawings by introducing noise into the
renderings of the training target images.
We designed this noise process to introduce the kinds of variations found in hand drawings (figure~\ref{handDrawingExamples}).
We drew 100 figures by hand; see figure~\ref{lotsOfHandDrawings}.
These were drawn reasonably carefully but not perfectly.
Because our model assumes that objects are snapped to a $16\times 16$ grid,
we made the drawings on graph paper.
\begin{figure}[H]%
  \begin{minipage}[t]{0.2\textwidth}\includegraphics[width = 2cm]{figures/expert-60-reduced.png}
    \subcaption{}
  \end{minipage}%
   \begin{minipage}[t]{0.2\textwidth}\includegraphics[width = 2cm]{figures/60-groundTruth-reduced.png}
    \subcaption{}
  \end{minipage}%
  \begin{minipage}[t]{0.2\textwidth}\includegraphics[width = 2cm]{figures/60-1-reduced.png}
    \subcaption{}
  \end{minipage}%
  \begin{minipage}[t]{0.2\textwidth}\includegraphics[width = 2cm]{figures/60-2-reduced.png} 
    \subcaption{}
  \end{minipage}
  \caption{(a): a hand drawing. (b): Rendering of the parse our model infers for (a). We can generalize to hand drawings like these because we train the model on images corrupted by a noise process designed to resemble the kind of noise introduced by hand drawings - see (c) \& (d) for noisy renderings of (b).}\ref{handDrawingExamples}
\end{figure}

\begin{figure}[H]
  \begin{minipage}[t]{2.25cm}
    \includegraphics[width = 2cm]{figures/expert-10.png}
    \includegraphics[width = 2cm]{figures/10-parse.png}    
    \subcaption{Noisy input}
  \end{minipage}
  \begin{minipage}[t]{2.25cm}
    \includegraphics[width = 2cm]{figures/expert-21.png}
    \includegraphics[width = 2cm]{figures/21-parse.png}    
    \subcaption{A graphical model}
  \end{minipage}
  \begin{minipage}[t]{2.25cm}
    \includegraphics[width = 2cm]{figures/expert-77.png}
    \includegraphics[width = 2cm]{figures/77-parse.png}    
    \subcaption{A figure from a deep learning textbook}
    \end{minipage}
  \begin{minipage}[t]{2.25cm}
    \includegraphics[width = 2cm]{figures/expert-0.png}
    \includegraphics[width = 2cm]{figures/0-parse.png}    
    \subcaption{Near miss}
  \end{minipage}
  \begin{minipage}[t]{2.25cm}
    \includegraphics[width = 2cm]{figures/expert-38.png}
    \includegraphics[width = 2cm]{figures/38-parse.png}    
    \subcaption{Failing on a Ising model}
  \end{minipage}
  \begin{minipage}[t]{2.25cm}
    \includegraphics[width = 2cm]{figures/expert-34.png}
    \includegraphics[width = 2cm]{figures/34-parse.png}     
    \subcaption{Illusory contours}
    \end{minipage}
  \caption{Example drawings above model outputs. See also Fig.~\ref{firstPageExamples}}\label{handDrawingExamples}
  \end{figure}

\section{Synthesizing graphics programs from execution traces}
Although the execution trace of a graphics program describes the parts
of a scene, it fails to encode higher-level features of the image,
such as repeated motifs, symmetries or reflections.  A \emph{graphics
  program} better describe structures like these,
and we now take as our goal to synthesize simple graphics programs from
their execution traces.

We constrain the space of allowed programs by writing down a context
free grammar over a space of programs. Although it might be desirable
to synthesize programs in a Turing-complete language like Lisp or
Python, a more tractable approach is to specify what in the program
languages community is called a Domain Specific Language (DSL). Our DSL (Table~\ref{DSL})
encodes prior knowledge of what graphics programs tend to look like.

\begin{table}[H]
  \begin{tabular}{rl}
  Program$\to$&Command; $\cdots$; Command\\
  Command$\to$&\texttt{circle}(Expression,Expression)\\
  Command$\to$&\texttt{rectangle}(Expression,Expression,Expression,Expression)\\
  Command$\to$&\texttt{LINE}(Expression,Expression,Expression,Expression,Boolean,Boolean)\\
  Command$\to$&\texttt{for}$(0\leq \text{Var}  < \text{Expression})$\texttt{ \{ }Program\texttt{ \}}\\
  Command$\to$&\texttt{REFLECT}$(\text{Axis})$\texttt{ \{ }Program\texttt{ \}}\\
  Expression$\to$&\mathcal{Z}\texttt{ * }Var\texttt{ + }\mathcal{Z}\\
  Var$\to$&A free (unused) variable\\
  \mathcal{Z}$\to$&an integer\\
  Axis$\to$&\texttt{X = }\mathcal{Z}\\
  Axis$\to$&\texttt{Y = }\mathcal{Z}
  \end{tabular}
  \caption{Grammar over graphics programs. We allow loops (\texttt{for}), vertical/horizontal reflections (\texttt{REFLECT}), and affine transformations (\mathcal{Z}\texttt{ * }Var\texttt{ + }\mathcal{Z}).}\label{DSL}
\end{table}

Given the DSL and a trace $T$, we want a program that evaluates to $T$
and also minimizes some measure of program cost:
\begin{equation}
  \text{program}(T) = \argmin_{\substack{p\in \text{DSL}\\p \text{ evaluates to } T}} \text{cost}(p)\label{programObjective}
\end{equation}
An intuitive measure of program cost is its length.  We define the
cost of a program to be the number of statements it contains, where a
statement is a ``Command'' in Table~\ref{DSL}.
\remark{The flow here is a bit off/backwards. ``We want a program that evaluates to $T$ and also minimizes some measure of program cost''---why do we care about cost? It'd be better to start by making a ``Bayesian Occam's razor'' appeal (e.g. the most compact/general program is the more likely explanation) and then saying that one way to do this is to minimize a cost function which is proportional to program length.}

The constrained optimization problem in
equation~\ref{programObjective} is intractable in general, but there
exist efficient-in-practice tools for finding exact solutions to
program synthesis problems like these. We use the state-of-the-art Sketch
tool~\cite{solar2008program}. Describing Sketch's program synthesis
algorithm is beyond the scope of this paper; see supplement.  At a
high level, Sketch takes as input a space of programs, along with a
specification of the program 's behavior and optionally a cost
function.  It translates the synthesis problem into a constraint
satisfaction problem, and then uses a quasiboolean solver to find a
minimum cost program satisfying the specification.  In exchange for
not having any guarantees on how long it will take to find a minimum
cost solution, it comes with the guarantee that it will always find a
globally optimal program.

Why synthesize a graphics program,
if the execution trace already suffices to recover the objects in an image?
Within our domain of hand-drawn figures, graphics program synthesis has several important uses:
\subsection{Extrapolating figures}
Having access to the source code of a graphics program facilitates coherent, high-level edits to the figure generated by that program. 
For example,
we can change all of the circles to squares,
were make all of the lines be dashed.
We can also \textbf{extrapolate} figures
by increasing the number of times that loops are executed.
\input{extrapolations.tex}

\subsection{Modeling similarity between figures}

\includegraphics[width = 8in]{figures/similarity.png}

\subsection{Correcting errors made by the neural network}

\remark{Seems like you're still fleshing this part out, but I'll give my feedback anyway: (1) This subsection could really use a motivational introduction, e.g. ``The program synthesizer can help correct errors/bad proposals from the neural network by favoring execution traces which lead to more concise/general programs.'' (2) The image likelihood function should probably be introduced sooner, when you talk about SMC/beam search. (3) Where does $\theta$ come from? Is it set by hand? Learned? (4) How does Equation 4 get used? Is this a modification to the beam search objective / SMC posterior? If so, it'd be great to have set up the version without it in an earlier section, and then be able to refer to this as a small modification of the previous equation.}

Let $L(\cdot | \cdot):\text{image}^2\to \mathcal{R}$ be our likelihood
function: it takes two images, an observed target image and a
hypothesized program output, and gives the likelihood of the observed
image conditioned on the program output.
Write $\hat{T}(I)$ for the trace the model predicts for image $I$.

We can extract a few basic features of a program, like its size or how many
loops it has,
and use these features to help predict
whether a trace is the correct explanation for an image.
\begin{equation}
\hat{T}(I) = \argmax_{T} L(I | \text{render}(T)) + \theta\cdot  \phi \left( \text{program}(T) \right)
\end{equation}
where $\phi (\cdot)$ is a feature extractor for programs.
This is equivalent to doing MAP inference in a generative model where the program
is first drawn from a log linear distribution $\propto \exp (\theta\cdot \phi (\text{program}))$,
than the program is executed deterministically,
and then we observe a noisy version of the program's output,
where $L$ is the noise model.



  \pagebreak
  \section{Preliminary extrapolation results} 



\pagebreak
  \section{Preliminary Synthesis results} 
\input{synthesizerOutputs.tex}


\bibliographystyle{unsrt}
{\small \bibliography{main}}


\end{document}
