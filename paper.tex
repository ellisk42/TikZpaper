\documentclass{article}

% if you need to pass options to natbib, use, e.g.:
% \PassOptionsToPackage{numbers, compress}{natbib}
% before loading nips_2017
%
% to avoid loading the natbib package, add option nonatbib:
% \usepackage[nonatbib]{nips_2017}

\usepackage{nips_2017}
\usepackage{dsfont}
% to compile a camera-ready version, add the [final] option, e.g.:
% \usepackage[final]{nips_2017}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography

\usepackage{graphicx} % more modern
%\usepackage{epsfig} % less modern
%\usepackage{subfig} 
\usepackage{fancyvrb}

\usepackage{caption}
\usepackage{subcaption}

\fvset{fontsize=\footnotesize}


\usepackage{multirow}
\usepackage{array}

\usepackage{amssymb}
\usepackage{listings}
\usepackage{wrapfig}
\usepackage{tabularx}


\usepackage{verbatim}
 \usepackage{booktabs}
 % For algorithms
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{tikz}
\usetikzlibrary{fit}
\usetikzlibrary{arrows.meta}
\usetikzlibrary{positioning}
\usetikzlibrary{decorations.text}
\usetikzlibrary{decorations.pathmorphing}

% As of 2011, we use the hyperref package to produce hyperlinks in the
% resulting PDF.  If this breaks your system, please commend out the
% following usepackage line and replace \usepackage{icml2016} with
% \usepackage[nohyperref]{icml2016} above.
\usepackage{amsmath}
\usepackage{hyperref}
\DeclareMathOperator*{\argmin}{arg\,min} % thin space, limits underneath in displays
\DeclareMathOperator{\argmin}{argmin} % no space, limits underneath in displays
\DeclareMathOperator*{\argmax}{arg\,max} % thin space, limits underneath in displays
\DeclareMathOperator{\argmax}{argmax} % no space, limits underneath in displays

\newcommand{\expect}{\mathds{E}} %{{\rm I\kern-.3em E}}
\newcommand{\probability}{\mathds{P}} %{{\rm I\kern-.3em P}}

\newcommand{\remark}[1]{\textcolor{red}{[#1]}}


% \title{Inferring Graphics Programs from Images}
\title{Learning to Infer Graphics Programs from Hand-Drawn Images}

% The \author macro works with any number of authors. There are two
% commands used to separate the names and addresses of multiple
% authors: \And and \AND.
%
% Using \And between authors leaves it to LaTeX to determine where to
% break the lines. Using \AND forces a line break at that point. So,
% if LaTeX puts 3 of 4 authors names on the first line, and the last
% on the second line, try using \AND instead of \And before the third
% author name.

\author{
  David S.~Hippocampus\thanks{Use footnote for providing further
    information about author (webpage, alternative
    address)---\emph{not} for acknowledging funding agencies.} \\
  Department of Computer Science\\
  Cranberry-Lemon University\\
  Pittsburgh, PA 15213 \\
  \texttt{hippo@cs.cranberry-lemon.edu} \\
  %% examples of more authors
  %% \And
  %% Coauthor \\
  %% Affiliation \\
  %% Address \\
  %% \texttt{email} \\
  %% \AND
  %% Coauthor \\
  %% Affiliation \\
  %% Address \\
  %% \texttt{email} \\
  %% \And
  %% Coauthor \\
  %% Affiliation \\
  %% Address \\
  %% \texttt{email} \\
  %% \And
  %% Coauthor \\
  %% Affiliation \\
  %% Address \\
  %% \texttt{email} \\
}

\begin{document}
% \nipsfinalcopy is no longer used

\maketitle

\begin{abstract}
  We introduce a model that learns to convert simple hand drawings
  into graphics programs written in a subset of \LaTeX.~The model
  combines techniques from deep learning and program synthesis.  We
  learn a convolutional neural network that proposes plausible drawing primitives
  that explain an image. This set of drawing primitives is like an execution trace for a graphics program. From this trace we use
  program synthesis techniques to recover a graphics
  program with constructs like variable bindings, iterative loops, or
  simple kinds of conditionals. With a graphics program in hand,
  we can correct errors made by the deep network, extrapolate drawings, and cluster drawings by use of similar high-level geometric structures.
  Taken together these results are a step towards
  agents that induce useful, human-readable programs from perceptual input.  
\end{abstract}

\section{Introduction}

 How can an agent convert noisy, high-dimensional perceptual input
 to a symbolic, abstract object, such as a computer program?  Here we
 consider this problem within a graphics program synthesis domain.  We
 develop an approach for converting natural images, such as hand
 drawings, into executable source code for drawing the original image.
 The graphics programs in our domain draw simple figures like those found in
 machine learning papers (see Figure~\ref{firstPageExamples}).
 %% \remark{The use of `graphics programs / visual programs' in the paper
 %%   title, title of this section, and the body of this section feels
 %%   too broad. `Graphics program' could conjur a lot of different ideas
 %%   (esp. 3D graphics); don't want to set the reader up to expect one
 %%   thing and then be disappointed that what you've done isn't
 %%   that. You bring up diagram-drawing later in the intro; I think it
 %%   should be made clear sooner (and certainly mentioned explicitly in
 %%   the abstract, when you get around to writing that).}

\newcommand{\exampleImageSize}{2cm}
\begin{figure}[h]
  \begin{minipage}[t]{0.7\linewidth}  
\begin{tabular}{llll}
  \includegraphics[width = \exampleImageSize]{figures/expert-60.png}&
  \includegraphics[width = \exampleImageSize]{figures/expert-5.png}&
    \includegraphics[width = \exampleImageSize]{figures/expert-17.png}&
    \includegraphics[width = \exampleImageSize]{figures/expert-58.png}\\
  \includegraphics[width = \exampleImageSize]{figures/60.png}&
  \includegraphics[width = \exampleImageSize]{figures/5.png}&
    \includegraphics[width = \exampleImageSize]{figures/17.png}&
    \includegraphics[width = \exampleImageSize]{figures/58.png}
\end{tabular}
\subcaption{}
  \end{minipage}%
  \begin{minipage}[t]{0.3\linewidth}
    \begin{tikzpicture}
      \node(picture) at (0,0) {\includegraphics[width = 2cm]{figures/expert-31.png}};
      \draw[very thick,->] (picture.south)  -- (0,-2);
    \end{tikzpicture}\\
    \texttt{for} $0\leq i < 4$:\\
    \texttt{rectangle} $(-3i + 9,2i - 2, -3i + 11,6)$\\
    \texttt{for} $0\leq j < i$:\\
    \texttt{circle} $(3i-2,-2j+5)$\\
    \subcaption{}
  \end{minipage}
  \caption{(a): Model learns to convert hand drawings (top) into \LaTeX~(bottom). (b) Synthesizes high-level \emph{graphics program} from hand drawing.}\label{firstPageExamples}
  \end{figure}

High dimensional perceptual input may seem ill matched to the abstract
semantics of a programming language. But programs with constructs such as
recursion or iteration produce a simpler \emph{execution trace} of
primitive actions; for our domain, the primitive actions are drawing
commands. Our hypothesis is that the execution trace of the program is
better aligned with the perceptual input, and that the trace can act
as a kind of bridge between perception and programs. We test this
hypothesis by developing a model that learns to map from an image to
the execution trace of the graphics program that drew it.  With the
execution trace in hand, we can bring to bear techniques from the
program synthesis community to recover the latent graphics program.
This family of techniques, called \emph{constraint-based program synthesis}~\cite{solar2008program},
work by modeling a set of possible programs inside of a constraint solver,
such as a SAT or SMT solver~\cite{de2008z3}.
These techniques excel at uncovering high-level symbolic structure,
but are not well equipped to deal with real-valued perceptual inputs.


We develop a hybrid architecture for inferring graphics programs.  Our
approach uses a deep neural network infer an execution trace from an
image; this network recovers primitive drawing operations such as
lines, circles, or arrows, along with their parameters. For added
robustness, we use the deep network as a proposal distribution for a
stochastic search over execution traces.  Section~\ref{neuralNetworkSection} describes this first stage of the architecture where we infer drawing commands from images, and explains how we handle noisy hand drawings.
Finally, we use program synthesis techniques to recover the program from its
trace.  The program synthesizer discovers constructs such as loops and
geometric operations such as reflections and affine transformations.
% \remark{This paragraph is all about making things a bit more
Section~\ref{programSynthesisSection} describes how the architecture
synthesizes programs from execution traces,
and how those programs are used for measuring similarity between hand drawings,
extrapolating figures, and correcting errors made by the deep network.


%   so you really need more specifics about program synth here.}

%% Each of these three components -- the deep network, the stochastic
%% search, the program synthesizer -- confers its own advantages. From
%% the deep network, we get a fast system that can recover plausible
%% execution traces in about a minute~\remark{A minute seems slow to me, for deep net inference...}. From the stochastic search, we get
%% added robustness: essentially, the stochastic search can correct
%% mistakes made by the deep network's proposals.  From the program
%% synthesizer, we get abstraction: our system recovers coordinate
%% transformations, for loops, and subroutines, which are useful for
%% downstream tasks and can help correct some mistakes of the earlier stages.
%% % \remark{I wonder if this would work even better as a bulleted list...}
%% \remark{Tie these claims into the paper results: state what the `downstream' tasks are that you actually do, and refer to the sections where they occur}


\section{Related work}

The CogSketch~\cite{forbus2011cogsketch} system also aims to have a high-level understanding of hand-drawn figures. Their primary goal is cognitive modeling, whereas we are interested in building an automated AI application.

%% open domain sketch understanding. structure mapping style model. also taps into cyc? the user segments the ink and labels them with concepts from knowledge base. like us they focus on compositional understanding. ``cognitive simulation", solves things like Raven matrices in IQ tests. not really trying right now to be automated like our system.

Our work bears resemblance to the Attend-Infer-Repeat (AIR) system, which learns to decompose an image into its constituent objects~\cite{eslami1603attend}. AIR learns an iterative inference scheme which infers objects one by one and also decides when to stop inference; this is similar to our approach's first stage, which parses images into program execution traces. Our approach further produces interpretable, symbolic programs which generate those execution traces. The two approaches also differ in their architectures and training regimes: AIR learns a recurrent auto-encoding model via variational inference, whereas our parsing stage learns an autoregressive-style model from randomly-generated (execution trace, image) pairs. Finally, while AIR was evaluated on multi-MNIST images and synthetic 3D scenes, we focus on parsing and interpreting hand-drawn sketches.
% attend infer repeat:~\cite{eslami1603attend}. Crucial distinction is
% that they focus on learning the generative model jointly with the
% inference network. Advantages of our system is that we learn symbolic
% programs, and that we do it from hand sketches rather than synthetic
% renderings.

Our image-to-execution-trace parsing architecture builds on prior work on controlling procedural graphics programs~\cite{ritchie2016neurally}. Given a program which generates random 2D recursive structures such as vines, that system learns a structurally-identical ``guide program'' whose output can be directed, via neural networks, to resemble a given target image. 
We adapt this method to a different visual domain (figures composed of multiple objects), using a broad prior over possible scenes as the initial program and viewing the execution trace through the guide program as a symbolic parse of the target image.
We then show how to synthesize higher-level programs from these execution traces.

In the computer graphics literature, there have been other systems which convert sketches into procedural representations. One uses a convolutional network to match a sketch to the output of a parametric 3D modeling system~\cite{huang2017shape}. Another uses convolutional networks to support sketch-based instantiation of procedural primitives within an interactive architectural modeling system~\cite{Nishida:2016:ISU:2897824.2925951}. Both systems focus on inferring fixed-dimensional parameter vectors. In contrast, we seek to automatically infer a structured, programmatic representation of a sketch which captures higher-level visual patterns.

Prior work has also applied sketch-based program synthesis to authoring graphics programs. In particular, Sketch-n-Sketch presents a bi-directional editing system in which direct manipulations to a program's output automatically propagate to the program source code~\cite{Hempel:2016:SSP:2984511.2984575}. We see this work as complementary to our own: programs produced by our method could be provided to a Sketch-n-Sketch-like system as a starting point for further editing.

\remark{Do you also want to cite your own work on ``Unsupverised Learning by Program Synthesis'' here?}



\section{Neural architecture for inferring drawing execution traces}\label{neuralNetworkSection}

We developed a deep network architecture for efficiently inferring a
execution trace, $T$, from an image, $I$.  Our model constructs the
trace one drawing command at a time.  When predicting the next drawing
command, the network takes as input the target image $I$ as well as
the rendered output of previous drawing commands.  Intuitively, the
network looks at the image it wants to explain, as well as what it has
already drawn.  It then decides either to stop drawing or proposes
another drawing command to add to the execution trace; if it decides
to continue drawing, the predicted primitive is rendered to its
``canvas'' and the process repeats.

Figure~\ref{architecture} illustrates this architecture.  We first
pass a $256\times 256$ target image and a rendering of the trace so
far (encoded as a two-channel image) to a convolutional network. Given the features extracted by the
convnet, a multilayer perceptron then predicts a distribution over the
next drawing command to add to the trace.  We predict the drawing
command token-by-token, conditioning each token both on the image
features and on the previously generated tokens.  For example, the
network first decides to emit the \verb|circle| token conditioned on
the image features, then it emits the $x$ coordinate of the circle
conditioned on the image features and the \verb|circle| token, and
finally it predicts the $y$ coordinate of the circle conditioned on
the image features, the \verb|circle| token, and the $x$ coordinate.
\remark{There are some more details that are important to provide
  about this architecture in the supplement: the
  functional form(s) of the probability distributions over tokens, the
  network layer sizes, which MLPs share parameters, etc.}

The distribution over the next drawing command factorizes as:
\begin{equation}
  \probability_\theta [t_1t_2\cdots t_K | I,T] = \prod_{k = 1}^K \probability_\theta [t_k | f_\theta(I,\text{render}(T)), \{t_j\}_{j = 1}^{k - 1}]
\end{equation}
where $t_1t_2\cdots t_K$ are the tokens in the drawing command, $I$ is
the target image, $T$ is an execution trace, $\theta$ are the
parameters of the neural network, and $f_\theta(\cdot,\cdot)$ is the
image feature extractor (convolutional network). The distribution over
execution traces factorizes as:
\begin{equation}
  \probability_\theta [T|I] = \prod_{n = 1}^{|T|} \probability_\theta [T_n | I,T_{1:(n-1)}]\times\probability_\theta [\verb|STOP| | I,T]\label{objective}
\end{equation}
where $|T|$ is the length of execution trace $T$, and the \verb|STOP|
token is emitted by the network to signal that the execution trace
explains the image.~\remark{Make explicit that a $T_n$ in Equation 2 is a concise way of referring to a sequence of tokens from Equation 1?}

We train the network by sampling execution traces $T$ and target
images $I$ for randomly generated scenes~\remark{this process ought to be explained, perhaps in supplement if it is at all detailed}, and maximizing
(\ref{objective}) with respect to $\theta$ by gradient ascent.
Training does not require backpropagation across the entire sequence of drawing commands:
drawing to the canvas `blocks' the gradients,
effectively offloading memory to an external visual store.
In a sense, this model is like an autoregressive variant of AIR~\cite{eslami1603attend} without attention.
~\remark{Somewhere (not necessarily here) you should probably cite the deep learning toolkit you used.}

%% \remark{I like that you make this connection, but it could be made more precisely. Specifically, (1) the architecture isn't \emph{really} recurrent (it uses no hidden state cells), so it'd be good to use a different term or drop this part of the point: (2) training of recurrent nets is also typically fully-supervised (Most RNNs lack latent variables per timestep)---if you're thinking about AIR specifically, maybe just say that, and (3) it's like an autogressive AIR \emph{without attention}.}
%% \remark{Something related to this that's also cool to point out: training this model doesn't require backpropagation across the entire sequence of drawing commands (drawing to the canvas `blocks' the gradients, effectively offloading memory to an external (visual) store, so in principle it might be scalable to much longer sequences.}

%% When we
%% have the generative model (the rendering function) and treat the trace
%% as fully observed, we need not solve an unsupervised or reinforcement
%% learning problem.
\tikzset{>=latex}
\begin{figure}
  \begin{tikzpicture}
  \node[draw,blue,ultra thick,anchor = west,inner sep=0pt,label=below:Target image: $I$](observation) at (0,-1) {\includegraphics[width = 2cm]{figures/expert-18.png}};
  \node[draw,blue,thick,anchor = west,inner sep=0pt,minimum width = 2cm,minimum height = 2cm,label=below:Canvas: render$(T)$] (canvas) at (0,-4) {};
  \draw[lightgray,ultra thin,step = 0.125] ([xshift = 0.5,yshift = 0.5]canvas.south west) grid ([xshift = -0.5,yshift = -0.5]canvas.north east);
  % draw partial image on canvas
  \draw (0.375cm,-3.25cm) circle (0.125cm);
  \draw (0.625cm,-3.625) circle (0.125cm);
  \draw (0.75cm,-3.375) circle (0.125cm);

  \node[draw,ultra thick,anchor = west,inner sep=0pt,minimum width = 2cm,minimum height = 3cm] (CNN) at (4.5,-2.5) {CNN};
  \node[inner sep = 0pt](tensorProduct) at ([xshift = -1.5cm]CNN.west) {$\bigotimes$};

  \node[rotate = -90,draw,ultra thick,inner sep=0pt,minimum width = 3cm,minimum height = 0.5cm] (features) at ([xshift = 1.5cm]CNN.east) {Image features};

  \node[draw,ultra thick,minimum size = 1cm](c1) at ([xshift = 1.5cm]features.north) {MLP};
  \node(l1) at ([yshift = -2cm]c1.south) {\verb|circle(|};
  \node[draw,ultra thick,minimum size = 1cm](c2) at ([xshift = 1cm]c1.east) {MLP};
  \draw[->,ultra thick,red] (c1.south) -- (l1.north);
  \node(l2) at ([yshift = -2cm]c2.south) {\verb|X=3,|};
  \draw[->,ultra thick,red] (c2.south) -- (l2.north);
  \node[draw,ultra thick,minimum size = 1cm](c3) at ([xshift = 1cm]c2.east) {MLP};
  \node(l3) at ([yshift = -2cm]c3.south) {\verb|Y=14)|};
  \draw[->,ultra thick,red] (c3.south) -- (l3.north);

  
  \draw[->,ultra thick] (features.north) -- (c1.west);
  \draw[->,ultra thick] (features.north) to[out = 45,in = 90] (c2.north);
  \draw[->,ultra thick] (features.north) to[out = 70,in = 90] (c3.north);
  \draw[->,ultra thick] ([xshift = 0.25cm]l1.north) -- (c2.west);
  \draw[->,ultra thick] ([xshift = 0.25cm]l1.north) -- (c3.west);
  \draw[->,ultra thick] ([xshift = 0.25cm]l2.north) -- ([yshift = -0.2cm]c3.west);

  \node(next)[draw,very thick,fit = (l1) (l2) (l3), dashed, label = below:{Next line of code}] {};

  \draw[-{>[scale = 1.5]},very thick,dashed] (next.west) -- ([yshift = -0.2cm]canvas.east) node [midway, below, sloped] (TextNode) {Renderer: \LaTeX~Ti\emph{k}Z};
  
  \draw[->,ultra thick] (canvas.east) -- (tensorProduct.south);%[yshift = -0.5cm]CNN.west);
  \draw[->,ultra thick] (observation.east) -- (tensorProduct.north);%([yshift = 0.5cm]CNN.west);
  \draw[->,ultra thick] (tensorProduct.east)  -- node[center,fill = white,rotate = 90] {{\tiny $256\times 256\times 2$}}  (CNN.west);
  \draw[->,ultra thick] (CNN.east) -- node[center,fill = white,rotate = 90] {{\tiny $16\times 16\times 10$}} (features.south);
%  \draw[]
  
%  \node at (canvas.x,canvas.y) {Canvas};
\end{tikzpicture}
\caption{Our neural architecture for inferring the execution trace of a graphics program from its output. \textcolor{blue}{Blue}: network inputs. Black: network operations. \textcolor{red}{Red}: samples from a multinomial. \texttt{Typewriter font}: network outputs. Renders snapped to a $16\times 16$ grid, illustrated in \textcolor{gray}{gray}.}  \label{architecture}
\end{figure}
\begin{table}
\begin{tabular}{ll}\toprule
  \begin{tabular}{l}
    \verb|circle|$(x,y)$
  \end{tabular}& \begin{tabular}{l}
    Circle at $(x,y)$
    \end{tabular}\\
  \begin{tabular}{l}
    \verb|rectangle|$(x_1,y_1,x_2,y_2)$
  \end{tabular}&\begin{tabular}{l}
    Rectangle with corners at $(x_1,y_1)$ \& $(x_2,y_2)$
    \end{tabular}\\
  \begin{tabular}{l}
    \verb|line|$(x_1,y_1,x_2,y_2,$\\
    \hspace{1cm}$\text{arrow}\in\{0,1\},\text{dashed}\in\{0,1\})$
  \end{tabular}&\begin{tabular}{l}
    Line from $(x_1,y_1)$ to  $(x_2,y_2)$,\\\hspace{1cm}optionally with an arrow and/or dashed
    \end{tabular}\\
  \begin{tabular}{l}
    \verb|STOP|
  \end{tabular}&\begin{tabular}{l}
    Finishes execution trace inference
    \end{tabular}
\\  \bottomrule
\end{tabular}
\caption{The deep network in (\ref{architecture}) predicts drawing commands, shown above.}
\label{drawingCommandTable}
\end{table}




This network suffices to ``derender'' synthetic images like those shown in
Figure~\ref{trainingData}.  We can perform a beam search decoding to
recover what the network thinks is the most likely execution trace for
images like these, recovering traces maximizing $\probability_\theta
[T|I]$. But, if the network makes a mistake (predicts an incorrect
line of code), it has no way of recovering from the error.  In order
to derender an image with $n$ objects, it must correctly predict $n$
drawing commands -- so its probability of success will decrease
exponentially in $n$, assuming it has any nonzero chance of making a
mistake.  For added robustness as $n$ becomes large, we treat the
neural network outputs as proposals for a Sequential Monte Carlo (SMC) sampling scheme~\cite{SMCBook}.  For
the SMC sampler, we use pixel-wise distance as a surrogate for a
likelihood function. The SMC sampler is designed to produce samples
from the distribution $\propto L(I|\text{render}(T))
\probability_\theta[T|I]$, where $L(\cdot | \cdot):\text{image}^2\to
\mathcal{R}$ uses the distance between two images as a proxy for a
likelihood.

Figure~\ref{syntheticResults}
compares the neural network with SMC against the neural network by
itself or SMC by itself.  Only the combination of the two passes a
critical test of generalization: when trained on images with $\leq 8$
objects, it successfully parses scenes with many more objects than the
training data.

\begin{figure}\centering
  \begin{minipage}{0.35\textwidth}
    \begin{minipage}[t]{2.2cm}\fbox{\includegraphics[width=2cm]{figures/randomScene-58-7.png}}\end{minipage}
    \begin{minipage}[t]{2.2cm}\fbox{\includegraphics[width=2cm]{figures/randomScene-33-5.png}}\end{minipage}
    \caption{Network is trained to infer execution traces for randomly generated figures like the two shown above.}\label{trainingData}
  \end{minipage}\hfill
  \begin{minipage}{0.6\textwidth}
    \includegraphics[width = 4cm]{figures/editDistance.png}
    \includegraphics[width = 4cm]{figures/accuracy.png}
      \caption{Using the model to parse latex output. The model is trained on diagrams with up to 8 objects. As shown above it generalizes to scenes with many more objects. Neither the stochastic search nor the neural network are sufficient on their own. \# particles varies by model: we compare the models \emph{with equal runtime} ($\approx 1$ sec/object)}\label{syntheticResults}
    \end{minipage}
  
  \end{figure}

\subsection{Generalizing to hand drawings}
A practical application of our neural network is the automatic conversion of hand drawings into a subset of \LaTeX.
 We train the model
to generalize to hand drawings by introducing noise into the
renderings of the training target images.
We designed this noise process to introduce the kinds of variations found in hand drawings (Figure~\ref{handDrawingExamples}; see supplement for details).
Our neurally-guided SMC procedure
used pixel-wise distance as a surrogate for a likelihood function ($L(\cdot|\cdot)$ in section~\ref{neuralNetworkSection}).
But pixel-wise distance fares poorly on hand drawings, which never exactly match
the model's renders.
So, for hand drawings,
we \emph{learn} a surrogate likelihood function,
$L_{\text{learned}}(\cdot|\cdot)$.
The density $L_{\text{learned}}(\cdot|\cdot)$ is predicted by a convolutional network that we train to predict
the distance between two traces conditioned upon their renderings.
We train our likelihood surrogate to approximate the symmetric difference,
which is  the number of drawing commands by which two traces differ:
\begin{equation}
-\log L_{\text{learned}}(\text{render}(T_1)|\text{render}(T_2))\approx |T_1 - T_2| + |T_2 - T_1|\label{symmetricDistance}
\end{equation}
Intuitively, Eq.~\ref{symmetricDistance} says that
$L_{\text{learned}}(\cdot|\cdot)$ approximates the distance between
the trace we want and the trace we have so far.  Pixel-wise distance
metrics are sensitive to the fine details of how and exactly where
arrows, dashes, and corners are drawn -- but we wish to be invariant
to these details. So, we learn a distance metric over images that
approximates the distance metric in the search space over traces.

We drew 100 figures by hand; see figure~\ref{lotsOfHandDrawings}.
These were drawn reasonably carefully but not perfectly.
Because our model assumes that objects are snapped to a $16\times 16$ grid,
we made the drawings on graph paper.
\begin{figure}\centering
  \begin{minipage}{0.45\textwidth}
  \begin{minipage}[t]{0.3\textwidth}\includegraphics[width = 1.5cm]{figures/expert-60-reduced.png}
    \subcaption{}
  \end{minipage}%
   \begin{minipage}[t]{0.3\textwidth}\includegraphics[width = 1.5cm]{figures/60-groundTruth-reduced.png}
    \subcaption{}
  \end{minipage}%
  \begin{minipage}[t]{0.3\textwidth}\includegraphics[width = 1.5cm]{figures/60-1-reduced.png}
    \subcaption{}
  \end{minipage}%
    \caption{(a): a hand drawing. (b): Rendering of the trace our model infers for (a). We can generalize to hand drawings like these because we train the model on images corrupted by a noise process designed to resemble the kind of noise introduced by hand drawings - see (c) for a noisy rendering of (b).}\label{handDrawingExamples}
  \end{minipage}\hfill
  \begin{minipage}{0.45\textwidth}
  \includegraphics[width = 6cm]{figures/drawingAccuracy.png}
  \caption{How close are the model's outputs to the ground truth on hand drawings, as we consider larger sets of samples (1, 5, 10, 100 samples)?
  Distance to the ground truth trace is measured by the intersection over union of predicted vs. ground truth traces (sets of drawing commands).}\label{drawingIntersectionOverUnion}
    \end{minipage}
\end{figure}


\begin{figure}[H]
  \begin{minipage}[t]{2.25cm}
    \includegraphics[width = 2cm]{figures/expert-10.png}
    \includegraphics[width = 2cm]{figures/10-parse.png}    
    \subcaption{Noisy input}
  \end{minipage}
  \begin{minipage}[t]{2.25cm}
    \includegraphics[width = 2cm]{figures/expert-21.png}
    \includegraphics[width = 2cm]{figures/21-parse.png}    
    \subcaption{A graphical model}
  \end{minipage}
  \begin{minipage}[t]{2.25cm}
    \includegraphics[width = 2cm]{figures/expert-77.png}
    \includegraphics[width = 2cm]{figures/77-parse.png}    
    \subcaption{A figure from a deep learning textbook}
    \end{minipage}
  \begin{minipage}[t]{2.25cm}
    \includegraphics[width = 2cm]{figures/expert-0.png}
    \includegraphics[width = 2cm]{figures/0-parse.png}    
    \subcaption{Near miss}
  \end{minipage}
  \begin{minipage}[t]{2.25cm}
    \includegraphics[width = 2cm]{figures/expert-38.png}
    \includegraphics[width = 2cm]{figures/38-parse.png}    
    \subcaption{Failing on a Ising model}
  \end{minipage}
  \begin{minipage}[t]{2.25cm}
    \includegraphics[width = 2cm]{figures/expert-34.png}
    \includegraphics[width = 2cm]{figures/34-parse.png}     
    \subcaption{Illusory contours}
    \end{minipage}
  \caption{Example drawings above model outputs. See also Fig.~\ref{firstPageExamples}}\label{lotsOfHandDrawings}~\remark{Showing `failure cases' (the last three above) out-of-context seems not great. Perhaps remind people in the caption that stochastic search can help correct some of these issues?}
\end{figure}
For each drawing we annotated a ground truth trace, and evaluated the
model by asking it to sample many candidate traces for each drawing.
For 57\% of the drawings the Top-1 most likely sample exactly matches the
ground truth; as we consider more samples the model encounters traces
that are closer to the ground truth annotation (Fig.~\ref{drawingIntersectionOverUnion}).
Because our current model sometimes makes mistakes on hand drawings,
we envision the current system working as follows:
a user sketches a diagram,
and the system responds by proposing a few candidate interpretations.
The user could then select the one closest to their intention and edit it if necessary.

\section{Synthesizing graphics programs from execution traces}\label{programSynthesisSection}
Although the execution trace of a graphics program describes the parts
of a scene, it fails to encode higher-level features of the image,
such as repeated motifs or symmetries.  A \emph{graphics
  program} better describe structures like these,
and we now take as our goal to synthesize simple graphics programs from
their execution traces.

We constrain the space of allowed programs by writing down a context
free grammar over a space of programs. Although it might be desirable
to synthesize programs in a Turing-complete language such as Lisp or
Python, a more tractable approach is to specify what in the program
languages community is called a Domain Specific Language (DSL)~\remark{citation?}. Our DSL (Table~\ref{DSL})
encodes prior knowledge of what graphics programs tend to look like.

\begin{table}[H]
  \begin{tabular}{rl}\toprule
  Program$\to$&Command; $\cdots$; Command\\
  Command$\to$&\texttt{circle}(Expression,Expression)\\
  Command$\to$&\texttt{rectangle}(Expression,Expression,Expression,Expression)\\
  Command$\to$&\texttt{line}(Expression,Expression,Expression,Expression,Boolean,Boolean)\\
  Command$\to$&\texttt{for}$(0\leq \text{Var}  < \text{Expression})$\texttt{ \{ if }$(\text{Var} > 0)$\texttt{ \{ }Program\texttt{ \}; }Program\texttt{ \}}\\
  Command$\to$&\texttt{reflect}$(\text{Axis})$\texttt{ \{ }Program\texttt{ \}}\\
  Expression$\to$&\mathcal{Z}\texttt{ * }Var\texttt{ + }\mathcal{Z}\\
  Var$\to$&A free (unused) variable\\
  \mathcal{Z}$\to$&an integer\\
  Axis$\to$&\texttt{X = }\mathcal{Z}\\
  Axis$\to$&\texttt{Y = }\mathcal{Z}\\\bottomrule
  \end{tabular}
  \caption{Grammar over graphics programs. We allow loops (\texttt{for}) with conditionals (\texttt{if}), vertical/horizontal reflections (\texttt{reflect}), variables (Var) and affine transformations (\mathcal{Z}\texttt{ * }Var\texttt{ + }\mathcal{Z}).}\label{DSL}
\end{table}

Given the DSL and a trace $T$, we want to recover a program that both evaluates to $T$
and, at the same time, is the ``best'' explanation of $T$.
For example, we might prefer more general programs or, in the spirit of Occam's razor,
prefer shorter programs.
We wrap these intuitions up into a cost function over programs,
and seek the minimum cost program consistent with $T$:
\begin{equation}
  \text{program}(T) = \argmin_{\substack{p\in \text{DSL}\\p \text{ evaluates to } T}} \text{cost}(p)\label{programObjective}
\end{equation}
We define the
cost of a program to be the number of statements it contains, where a
statement is a ``Command'' in Table~\ref{DSL}.
We also penalize using many different numerical constants; see supplement.
%\remark{The flow here is a bit off/backwards. ``We want a program that evaluates to $T$ and also minimizes some measure of program cost''---why do we care about cost? It'd be better to start by making a ``Bayesian Occam's razor'' appeal (e.g. the most compact/general program is the more likely explanation) and then saying that one way to do this is to minimize a cost function which is proportional to program length.}

The constrained optimization problem in
equation~\ref{programObjective} is intractable in general, but there
exist efficient-in-practice tools for finding exact solutions to
program synthesis problems like these. We use the state-of-the-art Sketch
tool~\cite{solar2008program}. Describing Sketch's program synthesis
algorithm is beyond the scope of this paper; see supplement.  At a
high level, Sketch takes as input a space of programs, along with a
specification of the program 's behavior and optionally a cost
function.  It translates the synthesis problem into a constraint
satisfaction problem, and then uses a SAT solver to find a
minimum cost program satisfying the specification.  In exchange for
not having any guarantees on how long it will take to find a minimum
cost solution, it comes with the guarantee that it will always find a
globally optimal program.

Why synthesize a graphics program,
if the execution trace already suffices to recover the objects in an image?
Within our domain of hand-drawn figures, graphics program synthesis has several uses:
\remark{I'm of two minds about how these subsections should be ordered. The current ordering leads with the coolest results, which is nice. But it's a bit...deflating?...to start with such cool results and then end on the somewhat technical point of how the synthesizer can help correct parse errors. An alternative would be to flip the ordering, starting with the technical point and building toward progressively cooler results. This requires a bit more patience on the part of the reader, but the overall narrative flow/payoff feels better. In any case, whatever ordering you pick should be the same order as these applications are mentioned in the abstract.}

\subsection{Extrapolating figures}
Having access to the source code of a graphics program facilitates coherent, high-level edits to the figure generated by that program. 
For example,
we can change all of the circles to squares or make all of the lines be dashed.
We can also \textbf{extrapolate} figures
by increasing the number of times that loops are executed.~\remark{Pick a few of these to show off and put the rest in supplement?}
Extrapolating repetitive visuals patterns comes naturally to humans,
and building this ability into an application is practical:
imagine hand drawing a repetitive graphical model structure
and having our system automatically induce and extend the pattern.
Fig.~\ref{extrapolationFigure} shows extrapolations of programs synthesized from ground truth traces;
see supplement for our full set of extrapolations.

\begin{figure}
  \includegraphics[width = 8in]{figures/extrapolationMatrix.png}
  \caption{Left: hand drawings. Right: extrapolations produced by
    running different parts of different loops either forward or
    backward an extra iteration.}\label{extrapolationFigure}
  \end{figure}
%\input{extrapolations.tex}


\subsection{Modeling similarity between figures}

\begin{figure}
  \begin{minipage}{0.4\textwidth}
  \includegraphics[width = 3in]{figures/NMF.png}
  \caption{NMF on features of the programs that were synthesized for each image. Horizontal component roughly corresponds to ``symmetry'' while vertical component roughly corresponds to ``loopyness'', with images on the diagonal having both of these.}    
    \end{minipage}\hfill
\begin{minipage}{0.4\textwidth}
  \includegraphics[width = 3in]{figures/imageSimilarity.png} 
  \caption{MDS on drawings using the learned distance metric, $L_{\text{learned}}(\cdot|\cdot)$. Drawings with similar looking parts in similar locations are clustered together.}
    \end{minipage}
  \end{figure}

\subsection{Correcting errors made by the parsing network}
The program synthesizer can help correct errors from the parse proposal network by favoring execution traces which lead to more concise or general programs.
For example, one generally prefers figures with perfectly aligned objects over figures whose parts are slightly misaligned -- and precise alignment lends itself to short programs.
Similarly, figures often have repeated parts,
which the program synthesizer might be able to model as a loop or reflectional symmetry.
So, in considering several candidate traces proposed by the neural network,
we might prefer traces whose best programs have desirable features such being short or having iterated structures.

Concretely, we implemented the following scheme: the neurally guided sampling scheme of section~\ref{neuralNetworkSection} for image $I$ samples candidate traces $\mathcal{F}(I)$.
Instead of predicting the most likely trace in $\mathcal{F}(I)$ according to the neural network,
we can take into account the programs that best explain the traces. 
Writing $\hat{T}(I)$ for the trace the model predicts for image $I$,
\begin{equation}
\hat{T}(I) = \argmax_{T\in \mathcal{F}(I)} L_{\text{learned}}(I | \text{render}(T)) \times\probability_{\beta} [ \text{program}(T)] 
\end{equation}
where $\probability_{\beta} [\cdot]$ is a prior probability
distribution over programs parameterized by $\beta$.
This is equivalent to doing
MAP inference in a generative model where the program is first drawn
from $\probability_{\beta} [\cdot]$, then the program is executed deterministically,
and then we observe a noisy version of the program's output, where $L$
is the noise model.

Given a corpus of graphics program synthesis problems with annotated ground truth traces (i.e. $(I,T)$ pairs),
we find a maximum likelihood estimate of $\beta$:
\begin{equation}
  \beta^* = \argmax_{\beta} \expect \left[ \log \frac{\probability_{\beta} [\text{program}(T)]\times L_{\text{learned}}(I|\text{render}(T))}{\sum_{T'\in \mathcal{F}(I)} \probability_{\beta} [\text{program}(T')]\times L_{\text{learned}}(I|\text{render}(T'))} \right]
\end{equation}
where the expectation is taken both over the model predictions and the
$(I,T)$ pairs in the training corpus.  We define $\probability_{\beta}
[\cdot]$ to be a log linear distribution $\propto \exp
(\beta\cdot \phi(\text{program}))$, where $\phi(\cdot)$ is a feature
extractor for programs.  We extract a few basic features of a
program, such as its size and how many loops it has, and use these
features to help predict whether a trace is the correct explanation
for an image.

% \remark{Seems like you're still fleshing this part out, but I'll give my feedback anyway: (1) This subsection could really use a motivational introduction, e.g. ``The program synthesizer can help correct errors/bad proposals from the neural network by favoring execution traces which lead to more concise/general programs.'' (2) The image likelihood function should probably be introduced sooner, when you talk about SMC/beam search. (3) Where does $\theta$ come from? Is it set by hand? Learned? (4) How does Equation 4 get used? Is this a modification to the beam search objective / SMC posterior? If so, it'd be great to have set up the version without it in an earlier section, and then be able to refer to this as a small modification of the previous equation.}


\section{Conclusion}

We have presented a system for inferring graphics programs which generate \LaTeX-style figures from hand-drawn images. The system uses a combination of deep neural networks and stochastic search to parse drawings into symbolic execution traces; it then feeds these traces to a general-purpose program synthesis engine to infer a structured graphics program. We evaluated our model's performance at parsing novel images, and we demonstrated its ability to extrapolate from provided drawings and to organize them according to high-level geometric features.

There are many directions for future work. In the parsing phase, the proposal network currently samples positional variables on a discrete grid. More general types of drawings could be supported by instead sampling from continuous distributions, e.g. using Mixture Density Networks~\cite{MDN}. The proposal network also currently handles only a very small subset of \LaTeX drawing commands, though there is no reason that it could not be extended to handle more with a higher-capacity network. Exploring more sophisticated network architectures, including ones that utilize attention, could also help correct some of the errors the network makes.
In the synthesis phase, a more expressive DSL---including subroutines, recursion, and symmetry groups beyond reflections---would allow the system to effectively model a wider variety of graphical phenomena. The synthesizer itself could also be the subject of future work: the system currently uses the general-purpose Sketch synthesizer, which can take minutes to hours to run, whereas program synthesizers which are custom-built for special problem domains can run much faster or even interactively~\cite{le2014flashextract}.

In the not-too-distant future, we believe it should be possible to produce professional-looking figures just by drawing them and then letting an artificially-intelligent agent write the corresponding code.
More generally, we believe that the two-phase system we have proposed---parsing into execution traces, then searching for a low-cost symbolic program which generates those traces---may be a useful paradigm for other domains in which agents must programmatically reason about noisy perceptual input.



%\input{synthesizerOutputs.tex}


\bibliographystyle{unsrt}
{\small \bibliography{main}}


\end{document}
