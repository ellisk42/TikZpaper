\documentclass{article}

% if you need to pass options to natbib, use, e.g.:
% \PassOptionsToPackage{numbers, compress}{natbib}
% before loading nips_2017
%
% to avoid loading the natbib package, add option nonatbib:
% \usepackage[nonatbib]{nips_2017}

\usepackage{nips_2017}
\usepackage{dsfont}
% to compile a camera-ready version, add the [final] option, e.g.:
% \usepackage[final]{nips_2017}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography

\usepackage{graphicx} % more modern
%\usepackage{epsfig} % less modern
%\usepackage{subfig} 
\usepackage{fancyvrb}

\usepackage{caption}
\usepackage{subcaption}

\fvset{fontsize=\footnotesize}


\usepackage{multirow}
\usepackage{array}

\usepackage{amssymb}
\usepackage{listings}
\usepackage{wrapfig}
\usepackage{tabularx}


\usepackage{verbatim}
 \usepackage{booktabs}
 % For algorithms
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{tikz}
\usetikzlibrary{fit}
% As of 2011, we use the hyperref package to produce hyperlinks in the
% resulting PDF.  If this breaks your system, please commend out the
% following usepackage line and replace \usepackage{icml2016} with
% \usepackage[nohyperref]{icml2016} above.
\usepackage{amsmath}
\usepackage{hyperref}
\DeclareMathOperator*{\argmin}{arg\,min} % thin space, limits underneath in displays
\DeclareMathOperator{\argmin}{argmin} % no space, limits underneath in displays

\newcommand{\expect}{\mathds{E}} %{{\rm I\kern-.3em E}}
\newcommand{\probability}{\mathds{P}} %{{\rm I\kern-.3em P}}

\newcommand{\remark}[1]{\textcolor{red}{[#1]}}


\title{Inferring Graphics Programs from Images}

% The \author macro works with any number of authors. There are two
% commands used to separate the names and addresses of multiple
% authors: \And and \AND.
%
% Using \And between authors leaves it to LaTeX to determine where to
% break the lines. Using \AND forces a line break at that point. So,
% if LaTeX puts 3 of 4 authors names on the first line, and the last
% on the second line, try using \AND instead of \And before the third
% author name.

\author{
  David S.~Hippocampus\thanks{Use footnote for providing further
    information about author (webpage, alternative
    address)---\emph{not} for acknowledging funding agencies.} \\
  Department of Computer Science\\
  Cranberry-Lemon University\\
  Pittsburgh, PA 15213 \\
  \texttt{hippo@cs.cranberry-lemon.edu} \\
  %% examples of more authors
  %% \And
  %% Coauthor \\
  %% Affiliation \\
  %% Address \\
  %% \texttt{email} \\
  %% \AND
  %% Coauthor \\
  %% Affiliation \\
  %% Address \\
  %% \texttt{email} \\
  %% \And
  %% Coauthor \\
  %% Affiliation \\
  %% Address \\
  %% \texttt{email} \\
  %% \And
  %% Coauthor \\
  %% Affiliation \\
  %% Address \\
  %% \texttt{email} \\
}

\begin{document}
% \nipsfinalcopy is no longer used

\maketitle

\begin{abstract}
\end{abstract}

\section{Introducing visual programs}

\textbf{Comment for reader: this paragraph is a little grandiose and goes
  beyond what we've actually done. But these are the kinds of things
  that motivate the work, and I think that in some form these ideas
  should be in the introduction or the conclusion.} How could an agent go from noisy,
high-dimensional perceptual input to a symbolic, abstract object, like
a computer program?  Here we consider this problem within the context
of \emph{graphics program synthesis}.  We develop an approach for
converting natural images, such as hand drawings, into executable
source code for drawing the original image.
\remark{The use of `graphics programs / visual programs' in the paper title, title of this section, and the body of this section feels too broad. `Graphics program' could conjur a lot of different ideas (esp. 3D graphics); don't want to set the reader up to expect one thing and then be disappointed that what you've done isn't that. You bring up diagram-drawing later in the intro; I think it should be made clear sooner (and certainly mentioned explicitly in the abstract, when you get around to writing that).}

\newcommand{\exampleImageSize}{2cm}
\begin{figure}[h]
  \begin{minipage}[t]{0.8\linewidth}  
\begin{tabular}{llll}
  \includegraphics[width = \exampleImageSize]{figures/expert-60.png}&
  \includegraphics[width = \exampleImageSize]{figures/expert-5.png}&
    \includegraphics[width = \exampleImageSize]{figures/expert-17.png}&
    \includegraphics[width = \exampleImageSize]{figures/expert-58.png}\\
  \includegraphics[width = \exampleImageSize]{figures/60.png}&
  \includegraphics[width = \exampleImageSize]{figures/5.png}&
    \includegraphics[width = \exampleImageSize]{figures/17.png}&
    \includegraphics[width = \exampleImageSize]{figures/58.png}
\end{tabular}
\subcaption{}
  \end{minipage}%
  \begin{minipage}[t]{0.2\linewidth}
    \begin{tikzpicture}
      \node(picture) at (0,0) {\includegraphics[width = 2cm]{figures/expert-31.png}};
      \draw[very thick,->] (picture.south)  -- (0,-2);
    \end{tikzpicture}
    \texttt{FOR} $0\leq i\leq 3$:\\
    \texttt{RECTANGLE} $()$\\
    \texttt{FOR} $0\leq j < i$:\\
    \texttt{CIRCLE} $()$\\
    \subcaption{}
  \end{minipage}
  \caption{(a): Model parses hand drawings (top) into \LaTeX~(bottom). (b) Synthesizes high-level \emph{graphics program} from hand drawing.}
  \end{figure}

High dimensional perceptual input is ill matched to the abstract
semantics of a programming language. But programs with constructs like
recursion or iteration produce a simpler \emph{execution trace} of
primitive actions. Our hypothesis is that the execution trace of the
program is better aligned with the perceptual input, and that the
trace can act as a kind of bridge between perception and programs. We
test this hypothesis by developing a model that learns to map from an
image to the execution trace of the graphics program that drew it.
With the execution trace in hand, we can bring to bear techniques from
the program synthesis community to recover the latent graphics program.
\remark{This is an \emph{excellent} explanation! I would add maybe one more sentence / citation to elaborate on what you mean by `techniques from the program synthesis community', as this will be an unfamiliar concept to some readers, I imagine.}



In this work we consider programs that draw diagrams, similar to those found in papers.

We develop a hybrid architecture for inferring graphics programs.  Our
approach uses a deep neural network infer an execution trace from an image;
this network recovers primitive drawing operations such as lines, circles, or
arrows~\remark{and their parameters?}
. For added robustness, we use the deep network as a proposal
distribution for a stochastic search over execution traces.
Finally, we use techniques in the program synthesis community to
recover the program from its trace.
\remark{This paragraph is all about making things a bit more specific, so you really need more specifics about program synth here.}

Each of these three components -- the deep network, the stochastic
search, the program synthesizer -- confers its own advantages. From
the deep network, we get a very fast system that can recover plausible
execution traces in about a minute~\remark{A minute seems slow to me, for deep net inference. Are you talking about training time, here, or...?}. From the stochastic search we get
added robustness; essentially, the stochastic search can correct
mistakes made by the deep network's proposals.  From the program
synthesizer, we get abstraction: our system recovers coordinate
transformations, for loops, and subroutines, which are useful for
downstream tasks~\remark{and can help correct some mistakes of the earlier stages?}.
\remark{I wonder if this would work even better as a bulleted list...}


\section{Related work}
attend infer repeat:~\cite{eslami1603attend}. Crucial distinction is
that they focus on learning the generative model jointly with the
inference network. Advantages of our system is that we learn symbolic
programs, and that we do it from hand sketches rather than synthetic
renderings.

ngpm:~\cite{ritchie2016neurally}. We build on the idea of a guide program, extending it to scenes composed of objects, and then show how to learn programs from the objects we discover.

Sketch-n-Sketch:~\cite{Hempel:2016:SSP:2984511.2984575}. Semiautomated synthesis presented in a nice user interface. Complementary to our work: you could pass a sketch to our system and then pass the program to sketch-n-sketch


Converting hand drawings into procedural models using deep networks:~\cite{huang2017shape,Nishida:2016:ISU:2897824.2925951}.



\section{Neural architecture for inferring image parses}

We developed a deep network architecture for efficiently inferring a
execution trace, $T$, from an image, $I$.  Our model constructs the
trace one drawing command at a time.  When predicting the next drawing
command, the network takes as input the target image $I$ as well as
the rendered output of previous drawing commands.  Intuitively, the
network looks at the image it wants to explain, as well as what it has
already drawn.  It then decides either to stop drawing or proposes
another drawing command to add to the execution trace; if it decides
to continue drawing, the predicted primitive is rendered to its
``canvas'' and the process repeats.

Figure~\ref{architecture} illustrates this architecture.  We first
pass the target image and a rendering of the trace so far to a
convolutional network. Given the features extracted by the
convolutionional network, a multilayer perceptron then predicts a
distribution over the next drawing command to add to the trace.  We
predict the drawing command token-by-token, and condition each token
both on the image features and on the previously generated tokens.
For example, the network first decides to emit the \verb|CIRCLE| token
conditioned on the image features, then it emits the $x$ coordinate of
the circle conditioned on the image features and the \verb|CIRCLE|
token, and finally it predicts the $y$ coordinate of the circle
conditioned on the image features, the \verb|CIRCLE| token, and the
$x$ coordinate.
\remark{There are some more details that are important to provide about this architecture, though possibly in an Appendix: the functional form(s) of the probability distributions over tokens, the network layer sizes, which MLPs share parameters, etc.}

\remark{Planning to move the description of SMC / beam search up here, too?}

The distribution over the next drawing command factorizes:
\begin{equation}
  \probability_\theta [t_1t_2\cdots t_K | I,T] = \prod_{k = 1}^K \probability_\theta [t_k | f_\theta(I,\text{render}(T)), \{t_j\}_{j = 1}^{k - 1}]
\end{equation}
where $t_1t_2\cdots t_K$ are the tokens in the drawing command, $I$ is
the target image, $T$ is an execution trace, $\theta$ are the
parameters of the neural network, and $f_\theta(\cdot,\cdot)$ is the
image feature extractor (convolutional network). The distribution over
execution traces factorizes as:
\begin{equation}
  \probability_\theta [T|I] = \prod_{n = 1}^{|T|} \probability_\theta [T_n | I,T_{1:(n-1)}]\times\probability_\theta [\verb|STOP| | I,T]\label{objective}
\end{equation}
where $|T|$ is the length of execution trace $T$, and the \verb|STOP|
token is emitted by the network to signal that the execution trace
explains the image.

We train the network by sampling execution traces $T$ and target
images $I$ for randomly generated scenes, and maximizing
(\ref{objective}) wrt $\theta$ by gradient ascent. Despite the
architecture being recurrent, training is fully supervised. In a
sense, this model is like an autoregressive variant of AIR.
\remark{I like that you make this connection, but it could be made more precisely. Specifically, (1) the architecture isn't \emph{really} recurrent (it uses no hidden state cells), so it'd be good to use a different term or drop this part of the point: (2) training of recurrent nets is also typically fully-supervised (Most RNNs lack latent variables per timestep)---if you're thinking about AIR specifically, maybe just say that, and (3) it's like an autogressive AIR \emph{without attention}.}
\remark{Something related to this that's also cool to point out: training this model doesn't require backpropagation across the entire sequence of drawing commands (drawing to the canvas `blocks' the gradients, effectively offloading memory to an external (visual) store, so in principle it might be scalable to much longer sequences.}

%% When we
%% have the generative model (the rendering function) and treat the trace
%% as fully observed, we need not solve an unsupervised or reinforcement
%% learning problem.

\begin{figure}
  \begin{tikzpicture}
  \node[draw,ultra thick,anchor = west,inner sep=0pt,label=below:Target image: $I$](observation) at (0,0) {\includegraphics[width = 3cm]{figures/expert-18.png}};
  \node[draw,thick,anchor = west,inner sep=0pt,minimum width = 3cm,minimum height = 3cm,label=below:Canvas: render$(T)$] (canvas) at (0,-5) {};
  % draw partial image on canvas
  \draw (0.5cm,-4cm) circle (0.1875cm);
  \draw (1.2cm,-4.3) circle (0.1875cm);
  \draw (0.85cm,-4.6) circle (0.1875cm);

  \node[draw,thick,anchor = west,inner sep=0pt,minimum width = 3cm,minimum height = 3cm] (CNN) at (4.5,-2.5) {CNN};

  \node[rotate = -90,draw,thick,inner sep=0pt,minimum width = 3cm,minimum height = 0.5cm] (features) at ([xshift = 1cm]CNN.east) {Image features};

  \node[draw,thick,circle,minimum size = 1cm](c1) at ([xshift = 1.5cm]features.north);
  \node(l1) at ([yshift = -2cm]c1.south) {\verb|CIRCLE(|};
  \node[draw,thick,circle,minimum size = 1cm](c2) at ([xshift = 1cm]c1.east);
  \draw[->,thick] (c1.south) -- (l1.north);
  \node(l2) at ([yshift = -2cm]c2.south) {\verb|X=3,|};
  \draw[->,thick] (c2.south) -- (l2.north);
  \node[draw,thick,circle,minimum size = 1cm](c3) at ([xshift = 1cm]c2.east);
  \node(l3) at ([yshift = -2cm]c3.south) {\verb|Y=14)|};
  \draw[->,thick] (c3.south) -- (l3.north);

  
  \draw[->,thick] (features.north) -- (c1.west);
  \draw[->,thick] (c1.east) -- (c2.west);
  \draw[->,thick] (c2.east) -- (c3.west);

  \node(next)[draw,fit = (l1) (l2) (l3), dashed, label = below:{Next line of code}] {};

  \draw[->,thick,dashed] (next.west) -- node[below] {Renderer: \LaTeX} (canvas.east);
  
  \draw[->,thick] (canvas.east) -- ([yshift = -0.5cm]CNN.west);
  \draw[->,thick] (observation.east) -- ([yshift = 0.5cm]CNN.west);
  \draw[->,thick] (CNN.east) -- (features.south);
%  \draw[]
  
%  \node at (canvas.x,canvas.y) {Canvas};
\end{tikzpicture}
\caption{Our neural architecture for inferring the execution trace of a graphics program from its output.~\remark{Thoughts on improving this figure: (1) Convnet diagrams typically show the sequence of layers, if possible (space might not permit it here, but those thin arrows just aren't doing it for me). (2) Are the target image / canvas convolved down independently, or jointly (i.e. starting as a 2-channel image)? That's an important detail that's not clear with the current figure/explanation. (3) The three circles downstream from `Image Features' are supposed to be MLPs, I assume(?), but it took me a little while to parse that. Having some visual way of clearly separating network operations from data (color, perhaps) would go a long way.}}  \label{architecture}
  \end{figure}

\begin{table}
\begin{tabular}{ll}\toprule
  \begin{tabular}{l}
    \verb|CIRCLE|$(x,y)$
    \end{tabular}& Circle at $(x,y)$\\
  \begin{tabular}{l}
    \verb|RECTANGLE|$(x_1,y_1,x_2,y_2)$
    \end{tabular}&Rectangle with corners at $(x_1,y_1)$ \& $(x_2,y_2)$\\
  \begin{tabular}{l}
    \verb|LINE|$(x_1,y_1,x_2,y_2,$\\
    \hspace{1cm}$\text{arrow}\in\{0,1\},\text{dashed}\in\{0,1\})$
    \end{tabular}&Line from $(x_1,y_1)$ to  $(x_2,y_2)$, optionally with an arrow and/or dashed\\
  \begin{tabular}{l}
    \verb|STOP|
  \end{tabular}&Finishes execution trace inference
\\  \bottomrule
\end{tabular}
\caption{The deep network in (\ref{architecture}) predicts drawing commands, shown above.}
\label{drawingCommandTable}
\end{table}


\begin{figure}
  \begin{minipage}[t]{15cm}
    \includegraphics[width = 15cm]{figures/editDistance.png}
  \end{minipage}\\
  \begin{minipage}[t]{15cm}
    \includegraphics[width = 15cm]{figures/accuracy.png}
  \end{minipage}\\
  \begin{minipage}[t]{15cm}
    \includegraphics[width = 15cm]{figures/time.png}
  \end{minipage}
  \caption{Using the model to parse latex output. The model is trained on diagrams with up to 8 objects. As shown above it generalizes to scenes with many more objects. Neither the stochastic search nor the neural network are sufficient on their own.}
\end{figure}

\section{Generalizing to hand drawings}

\begin{figure}
  \begin{minipage}[b]{0.3\textwidth}
    \centering\includegraphics[width = 3cm]{figures/expert-60.png}
    \subcaption{(a)}
  \end{minipage}%
  \begin{minipage}[t]{0.3\textwidth}\includegraphics[width = 3cm]{figures/60-groundTruth.png}
    \subcaption{(b)}
  \end{minipage}\\
  \begin{minipage}[t]{0.3\textwidth}\includegraphics[width = 3cm]{figures/60-1.png}
    \subcaption{(c)}
  \end{minipage}%
  \begin{minipage}[t]{0.3\textwidth}\includegraphics[width = 3cm]{figures/60-2.png}
    \subcaption{(d)}
  \end{minipage}
  \caption{(a): a hand drawing. (b): Rendering of the parse our model infers for (a). We can generalize to hand drawings like these because we train the model on images corrupted by a noise process designed to resemble the kind of noise introduced by hand drawings - see (c) \& (d) for noisy renderings of (b).}
  \end{figure}

\section{Synthesizing graphics programs from execution traces}

\begin{table}
  \begin{tabular}{rl}
  Program$\to$&Command; $\cdots$; Command\\
  Command$\to$&\texttt{CIRCLE}(Expression,Expression)\\
  Command$\to$&\texttt{RECTANGLE}(Expression,Expression,Expression,Expression)\\
  Command$\to$&\texttt{LINE}(Expression,Expression,Expression,Expression,Boolean,Boolean)\\
  Command$\to$&\texttt{FOR}$(0\leq \text{Var}  < \text{Expression})$\texttt{ \{ }Program\texttt{ \}}\\
  Command$\to$&\texttt{REFLECT}$(\text{Axis})$\texttt{ \{ }Program\texttt{ \}}\\
  Expression$\to$&\mathcal{Z}\texttt{ * }Var\texttt{ + }\mathcal{Z}\\
  Var$\to$&A free (unused) variable\\
  \mathcal{Z}$\to$&an integer\\
  Axis$\to$&\texttt{X = }\mathcal{Z}\\
  Axis$\to$&\texttt{Y = }\mathcal{Z}
  \end{tabular}
  \caption{Grammar over graphics programs. We allow loops (\texttt{FOR}), vertical/horizontal reflections (\texttt{REFLECT}), and affine transformations (\mathcal{Z}\texttt{ * }Var\texttt{ + }\mathcal{Z}).}
  \end{table}

\section{Neural networks for guiding SMC}



Let $L(\cdot | \cdot):\text{image}^2\to \mathcal{R}$ be our likelihood
function: it takes two images, an observed target image and a
hypothesized program output, and gives the likelihood of the observed
image conditioned on the program output. We want to sample from:
\begin{equation}
\probability [p|x]  \propto L(x | \text{render}(p)) \probability [p]
\end{equation}
where $\probability [p]$ is the prior probability of program $p$, and $x$ is the observed image.

Let $p$ be a program with $L$ lines, which we will write as $p = (p_1,p_2,\cdots,p_L)$. Assume the prior factors into:
\begin{equation}
  \probability [p]\propto \prod_{l\leq L}\probability [p_l]
\end{equation}
Define the distribution $q_L(\cdot)$, which happens to be proportional to the above posterior:
\begin{equation}
  q_L(p_1,p_2,\cdots,p_{L - 1},p_L)\propto q_{L - 1}(p_1,p_2,\cdots,p_{L - 1})\times \frac{L(x | \text{render}(p_1,p_2,\cdots,p_{L - 1},p_L))}{L(x | \text{render}(p_1,p_2,\cdots,p_{L - 1}))}\times\probability [p_L]
\end{equation}
Now suppose we have some samples from $q_{L - 1}(\cdot)$, and that we
then sample a $p_L$ from a distribution proportional to $\frac{L(x |
  \text{render}(p_1,p_2,\cdots,p_{L - 1},p_L))}{L(x |
  \text{render}(p_1,p_2,\cdots,p_{L - 1}))}\times\probability [p_L]$.
The resulting programs $p$ are distributed according to $q_L$, and so
are also distributed according to $\probability [p|x]$.

How do we sample $p_L$ from a distribution proportional to $\frac{L(x
  | \text{render}(p_1,p_2,\cdots,p_{L - 1},p_L))}{L(x |
  \text{render}(p_1,p_2,\cdots,p_{L - 1}))}\times\probability [p_L]$?
We have a neural network that takes as input the target image $x$ and
the program so far, and produces a distribution over next lines of
code ($p_L$).  We write $\text{NN}(p_L | p_1,\cdots,p_{L - 1};x)$ for
the distribution output by the neural network. So we can sample from NN and then weight the samples by:
\begin{equation}
  w(p_L) = \frac{\probability [p_L]}{\text{NN}(p_L | p_1,\cdots,p_{L - 1};x)}\times \frac{L(x | \text{render}(p_1,p_2,\cdots,p_{L - 1},p_L))}{L(x | \text{render}(p_1,p_2,\cdots,p_{L - 1}))}
\end{equation}
Then we can resample from these now weighted samples to get a new
population of particles (here programs are particles), where each
program now has $L$ lines instead of $L - 1$.

This procedure can be seen as a particle filter, where each successive
latent variable is another line of code, and the emission
probabilities are successive ratios of likelihoods under $L(\cdot |
\cdot)$.

\textbf{Comments for Dan}. Right now I'm not actually sampling from
the neural network - instead, I enumerate the top few hundred lines of
code suggested by the network, and then weight them by their
likelihoods.
So actually the form of NN is:
\begin{equation}
  \text{NN}(p_L | p_1,\cdots,p_{L - 1};x)\propto \begin{cases}
    1,&\text{ if }p_L \in \text{top hundred neural network proposals}\\
0,&\text{otherwise}.    \end{cases}
\end{equation}
Do you think this is a problem? The neural network puts almost all of its mass on a few guesses.
In order to get the correct line of code I sometimes need to get something like the 50th  top guess,
so I don't want to literally just sample from the distribution suggested by the neural network.


  \begin{algorithm}[tb]
   \caption{Neurally guided SMC}
   \label{guideAlgorithm}
\begin{algorithmic}
  \STATE {\bfseries Input:} Neural network NN, beam size $N$, maximum length $L$, target image $x$
  \STATE {\bfseries Output:} Samples of the program trace
  \STATE Set $B_0 = \{\text{empty program}\}$
  \FOR{$1\leq l\leq L$}
  \FOR{$1\leq n\leq N$}
  \STATE{ $p_n\sim \text{Uniform}(B_{l - 1})$}
  \STATE{ $p'_{n}\sim \text{NN}(\text{render}(p),x)$}
  \STATE{ Define $r_n = p'_n\cdot p_n$}
  \STATE{ Set $\tilde{w}(r_n) = \frac{L(x|r_n)}{L(x|p_n)}\times\frac{\probability [p'_n]}{\probability [p'_n = \text{NN}(\text{render}(p),x)]}$}
  \ENDFOR
  \STATE{ Define $w(p) = \frac{\tilde{w}(p)}{\sum_{p'}\tilde{w}(p')}$}
  \STATE{ Set $B_l$ to be $N$ samples from $r_n$ distributed according to $w(\cdot)$}
  \ENDFOR
  \STATE {\bfseries return} $\{p : p\in B_{l\leq L}, p \text{ is finished}\}$
\end{algorithmic}
  \end{algorithm}


\bibliographystyle{unsrt}
{\small \bibliography{main}}


\end{document}
