\documentclass{article}

% if you need to pass options to natbib, use, e.g.:
% \PassOptionsToPackage{numbers, compress}{natbib}
% before loading nips_2018

% ready for submission
\usepackage[final]{nips_2018}

% to compile a preprint version, e.g., for submission to arXiv, add
% add the [preprint] option:
% \usepackage[preprint]{nips_2018}

% to compile a camera-ready version, add the [final] option, e.g.:
% \usepackage[final]{nips_2018}

% to avoid loading the natbib package, add option nonatbib:
% \usepackage[nonatbib]{nips_2018}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{dsfont}
\usepackage{verbatimbox}
\usepackage{listings}
\lstset{basicstyle = \small\ttfamily}
\usepackage{graphicx} % more modern
%\usepackage{epsfig} % less modern
%\usepackage{subfig} 
\usepackage{fancyvrb}

\usepackage{caption}
\usepackage{capt-of}
\usepackage{subcaption}

\fvset{fontsize=\footnotesize}


\usepackage{multirow}
\usepackage{array}

\usepackage{amssymb}
\usepackage{listings}
\usepackage{wrapfig}
\usepackage{tabularx}


\usepackage{verbatim}
 \usepackage{booktabs}
 % For algorithms
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{tikz}
\usetikzlibrary{fit}
\usetikzlibrary{arrows.meta}
\usetikzlibrary{positioning}
\usetikzlibrary{decorations.text,decorations.pathreplacing}
\usetikzlibrary{decorations.pathmorphing}

% As of 2011, we use the hyperref package to produce hyperlinks in the
% resulting PDF.  If this breaks your system, please commend out the
% following usepackage line and replace \usepackage{icml2016} with
% \usepackage[nohyperref]{icml2016} above.
\usepackage{amsmath}
\usepackage{hyperref}
\DeclareMathOperator*{\argmin}{arg\,min} % thin space, limits underneath in displays
% \DeclareMathOperator{\argmin}{argmin} % no space, limits underneath in displays
\DeclareMathOperator*{\argmax}{arg\,max} % thin space, limits underneath in displays
% \DeclareMathOperator{\argmax}{argmax} % no space, limits underneath in displays

\newcommand{\expect}{\mathds{E}} %{{\rm I\kern-.3em E}}
\newcommand{\probability}{\mathds{P}} %{{\rm I\kern-.3em P}}
\newcommand{\indicator}{\mathds{1}} %{{\rm I\kern-.3em P}}

\newcommand{\remark}[1]{\textcolor{red}{[#1]}}
\newcommand{\exampleImageSize}{2cm}


\usepackage{amsthm}
 
\theoremstyle{definition}
\newtheorem{definition}{Definition}[section]
\newtheorem{experiment}{Experiment} %[section]


\title{Learning to Infer Graphics Programs from Hand-Drawn Images}

% The \author macro works with any number of authors. There are two
% commands used to separate the names and addresses of multiple
% authors: \And and \AND.
%
% Using \And between authors leaves it to LaTeX to determine where to
% break the lines. Using \AND forces a line break at that point. So,
% if LaTeX puts 3 of 4 authors names on the first line, and the last
% on the second line, try using \AND instead of \And before the third
% author name.
%% Kevin Ellis$^{1,*}$\qquad Daniel Ritchie$^2$\qquad  Armando Solar-Lezama$^1$\qquad Joshua B. Tenenbaum$^1$\\
  %% $^1$Massachusetts Institute of Technology\qquad $^2$Brown University\\
  %% $^*$Primary contact: \texttt{ellisk@mit.edu}%% \\
\author{Kevin Ellis\\MIT\\\texttt{ellisk@mit.edu}\\
 \And
 Daniel Ritchie\\Brown University\\\texttt{daniel\_ritchie@brown.edu}\\
 \AND
 Armando Solar-Lezama\\MIT\\\texttt{asolar@csail.mit.edu}\\
 \And
 Joshua B. Tenenbaum \\MIT\\\texttt{jbt@mit.edu}\\
  %% Affiliation \\
  %% Address \\
  %% \texttt{email} \\
  %% \And
  %% Coauthor \\
  %% Affiliation \\
  %% Address \\
  %% \texttt{email} \\
  %% \And
  %% Coauthor \\
  %% Affiliation \\
  %% Address \\
  %% \texttt{email} \\
}

\begin{document}
% \nipsfinalcopy is no longer used

\maketitle

\begin{abstract}
  We introduce a model that learns to convert simple hand drawings
  into graphics programs written in a subset of \LaTeX.~The model
  combines techniques from deep learning and program synthesis.  We
  learn a convolutional neural network that proposes plausible drawing
  primitives that explain an image. These drawing primitives are a
  specification (spec) of what the graphics program needs to draw.  We
  learn a model that uses program synthesis techniques to recover a
  graphics program from that spec. These programs have constructs like
  variable bindings, iterative loops, or simple kinds of
  conditionals. With a graphics program in hand, we can correct errors
  made by the deep network and extrapolate drawings.
\end{abstract}

\section{Introduction}

Human vision is rich -- we infer shape, objects, parts of objects,
and relations between objects -- and vision is also abstract:
we can perceive the radial symmetry of a spiral staircase,
the iterated repetition in the Ising model,
see the forest for the trees, and also the recursion within the trees.
How could we build an agent with similar visual inference abilities?
As a small step in this direction, 
we cast this problem as program learning,
and take as our goal to learn high--level
graphics programs from simple 2D drawings.
The graphics programs we consider make figures like those found in machine learning papers
(Fig.~\ref{firstPageExamples}),
and capture high-level features like %of a drawing like
symmetry, repetition, and reuse of structure.

%% Vision as a kind of ``inverse graphics'' is an idea that is both intuitive and old,
%% with many diverse recent takes on this framing -- neural network approaches like Attend--Infer--Repeat or Capsule Nets,
%% or more structured approaches leveraging probabilistic programming~\cite{}.
%% This vision--as--inverse--graphics metaphor addresses the important problem of
%% inferring, from an image, the identities and poses of
%% objects. But visual perception is richer than object identity and pose:




 %% How can an agent convert noisy, high-dimensional perceptual input
 %% to a symbolic, abstract object, such as a computer program?  Here we
 %% consider this problem within a graphics program synthesis domain.  We
 %% develop an approach for converting  hand
 %% drawings into executable source code for drawing the original image.
 %% The graphics programs in our domain draw simple figures like those found in
 %% machine learning papers
% (see Fig.~\ref{firstPageExamples}a)
\begin{myverbbox}[\small]{\firstFirstPageCode}
for (i < 3)
 rectangle(3*i,-2*i+4,
           3*i+2,6)
 for (j < i + 1)
  circle(3*i+1,-2*j+5)
\end{myverbbox}
\begin{myverbbox}[\small]{\secondFirstPageCode}
reflect(y=8)
 for(i<3)
  if(i>0)
   rectangle(3*i-1,2,3*i,3)
  circle(3*i+1,3*i+1)
\end{myverbbox}

 \begin{figure}[H]%\vspace{-0.7cm}
  \begin{minipage}[b]{0.35\linewidth}  \centering
\begin{tabular}{ll}
  \includegraphics[width = \exampleImageSize]{figures/expert-60.png}&
%  \includegraphics[width = \exampleImageSize]{figures/expert-5.png}&
%    \includegraphics[width = \exampleImageSize]{figures/expert-17.png}&
    \includegraphics[width = \exampleImageSize]{figures/expert-58.png}\\
  \includegraphics[width = \exampleImageSize]{figures/60.png}&
%  \includegraphics[width = \exampleImageSize]{figures/5.png}&
%    \includegraphics[width = \exampleImageSize]{figures/17.png}&
    \includegraphics[width = \exampleImageSize]{figures/58.png}
\end{tabular}
\subcaption{}
  \end{minipage}
  %
  \begin{minipage}[b]{0.6\linewidth}\centering
    \begin{tikzpicture}
      \node(picture1) at (0,-1) {\includegraphics[height = 1.5cm]{figures/expert-31.png}};
      \node(picture2) [below = 0.3cm of picture1] {\includegraphics[height = 1.5cm]{figures/expert-72-trim.png}};
      \node[right=1cm of picture1] (c1) {\fbox{\firstFirstPageCode}};
      \node[right=1cm of picture2] (c2) {\fbox{\secondFirstPageCode}};
      \draw[very thick,->] (picture1.east)  -- (c1.west);
      \draw[very thick,->] (picture2.east)  -- (c2.west);
    \end{tikzpicture}
    \subcaption{}
  \end{minipage}
  \caption{(a): Model learns to convert hand drawings (top) into \LaTeX~(rendered below). (b) Learns to synthesize high-level graphics program from hand drawing.}\label{firstPageExamples}
\end{figure}
 
 The key observation behind our work is that going from pixels to programs involves two distinct steps, each requiring different technical approaches. The first step involves inferring what objects make up an image -- for diagrams, these are things like as rectangles, lines and arrows. The second step involves identifying the higher-level visual concepts that describe how the objects were drawn. In Fig. 1(b), it means identifying a pattern in how the circles and rectangles are being drawn that is best described with two nested loops, and which can easily be extrapolated to a bigger diagram.

This two-step factoring can be framed as probabilistic inference in a generative model where a latent program
 is executed to produce a set of drawing commands,
 which are then rendered to form an image (Fig.~\ref{roadmap}).
 We refer to this set of drawing commands as a \textbf{specification (spec)} because it specifies what the graphics program drew while lacking the high-level structure determining how the program decided to draw it.
 We infer a spec from an image using stochastic search (Sequential Monte Carlo)
 and infer a program from a spec using  constraint-based program synthesis~\citep{solar2008program} --
 synthesizing structures like symmetries, loops, or conditionals.
 In practice, both stochastic search and program synthesis are
 prohibitively slow,
 and so we learn models that accelerate inference for both programs and specs,
 in the spirit of ``amortized inference''~\cite{paige2016inference},
 training a neural network to amortize the cost of inferring specs from images and using
 a variant of Bias--Optimal Search~\cite{schmidhuber2004optimal}
 to amortize the cost of synthesizing programs from specs.
 \begin{figure}[h]
  \centering\begin{tikzpicture}
  \node[ thick,anchor = west,inner sep=0pt,label={[yshift = 0.3cm]{\small \begin{tabular}{c}
          \textbf{Image}\\
          \textbf{(Observed)}
  \end{tabular}}}](observation) at (0,0) {\includegraphics[width = 1.5cm]{figures/expert-39-trimmed.png}};
    \node[ultra thick,anchor = west,inner sep=0pt](traceSource) at (3.7,0.5){    \begin{lstlisting}[basicstyle = \scriptsize\ttfamily]
line, line,
rectangle,
line, ...
\end{lstlisting}};
    \node[ultra thick,anchor = west,inner sep=0pt](traceImage) at (4,-0.5) {
      \includegraphics[width = 0.9cm]{figures/39-parse.png}}; 
    \node(trace)[draw,thin,fit = (traceImage) (traceSource), label = above:{{\small \begin{tabular}{c}
            \textbf{Spec/Drawing Commands}\\
            \textbf{(Latent)}
    \end{tabular}}}] {};
    
    \node[draw, thick,anchor = west,inner sep=2pt,label=above:{\small \begin{tabular}{c}
          \textbf{Program}\\
          \textbf{(Latent)}
    \end{tabular}}](program) at (7.7,0) {
      \begin{lstlisting}
for (j < 3)
for (i < 3)
if (...)
 line(...)
 line(...)
rectangle(...)
    \end{lstlisting}};
    \node[ultra thick,anchor = west,inner sep=0pt,label=below:{\small Extrapolation}](extrapolate) at (12,0.5) {\includegraphics[width = 1.3cm]{figures/39-extrapolated.png}};
%    \node[ultra thick,anchor = west,inner sep=0pt,label=below:{\small Similarity}](similarity) at (11.3,0) {\includegraphics[width = 1cm]{figures/expert-38-trim.png}};
    \node[ultra thick,anchor = west,text width = 1.3cm,inner sep=0pt](errors) at (12.1,-1.2) {\small Error correction};

    \draw[->, ultra thick] ([yshift=10]trace.west) to[out = 150,in = 30] node[midway,yshift = 6]{{\small Rendering}} ([xshift=5,yshift=10]observation.east); % -- node[fill = white,rotate = -90] {{\small Rendering}}
    \draw[->, very thick, red] ([yshift = -15,xshift=3]observation.east) to[out = -30,in = -150] node[midway,yshift = -12,xshift=5]{{\small \begin{tabular}{l}
          Learning + \\Stochastic search
    \end{tabular}}} ([yshift = -15]trace.west);
    \draw[->, ultra thick] ([yshift=10]program.west) to[out = 150,in = 30] node[midway,yshift = 6] {{\small Execution}} ([yshift=10]trace.east);
    \draw[->, very thick, red] ([yshift = -15,xshift=0]trace.east) to[out = -30,in = -150] node[midway,yshift = -12,xshift=9]{{\small \begin{tabular}{l}
          Learning + \\Program synthesis
    \end{tabular}}} ([yshift = -15]program.west);

%    \draw[->, thick, red, very thick] ([xshift = 10]trace.south) to[out = -10,in = -170] node[midway,yshift = -6]{{\small Learning + Program synthesis}} ([xshift = 10]program.south);
%    \draw[->, thick, red] (program.east) to[out = 80,in = 180] (errors.west);
%    \draw[->, thick] (program.east) to[out = 40,in = -230] (similarity.west);
    \draw[->, thick, red, very thick] (program.east) to[out = 60,in = 180] (extrapolate.west);

    \draw[decoration = {brace,mirror,raise = 5pt},decorate,thick]
    ([yshift = -11,xshift = -130]trace.south) -- node[below = 6pt] {{\small Section~\ref{neuralNetworkSection}: Image$\to$Spec}} ([yshift = -11,xshift = -5]trace.south);
    \draw[decoration = {brace,mirror,raise = 5pt},decorate,thick]
    ([xshift = 5,yshift = -11]trace.south) -- node[below = 6pt] {{\small Section~\ref{programSynthesisSection}: Spec$\to$Program}} ([xshift = 170,yshift = -11]trace.south);
    \draw[decoration = {brace,mirror,raise = 5pt},decorate,thick]
([xshift = 180,yshift = -11]trace.south) -- node[below = 6pt] {{\small Section~\ref{applicationsSection}: Applications}} ([xshift = 255,yshift = -11]trace.south);
  \end{tikzpicture}
  \caption{Black arrows: Top--down generative model; Program$\to$Spec$\to$Image. {\color{red}Red} arrows: Bottom--up inference procedure. \textbf{Bold:} Random variables (image/spec/program)}\label{roadmap}
 \end{figure}

 The new contributions of this work are (1) a working model that can infer high-level symbolic programs from perceptual input, and (2) a technique for using learning to amortize the cost of program synthesis, described in Section~\ref{learningASearchPolicy}.

\section{Neural architecture for inferring specs}\label{neuralNetworkSection}

We developed a deep network architecture for efficiently inferring a
spec, $S$, from a hand-drawn image, $I$.
Our model combines ideas from
Neurally-Guided Procedural Models~\citep{ritchie2016neurally}
and Attend-Infer-Repeat~\citep{eslami1603attend}, but
we wish to emphasize
that one could use
many different approaches from the computer vision toolkit to
parse an image in to primitive drawing commands  (in our terminology, a ``spec'')~\cite{nsd}.
Our network constructs the
spec one drawing command at a time, conditioned on what it has drawn so far (Fig.~\ref{architecture}).
%When predicting the next drawing
%command, the network takes as input the target image $I$ as well as
%the rendered output of previous drawing commands.
%Intuitively, the network looks at the image it wants to explain, as well as what it has already drawn.  It then decides either to stop drawing or proposes another drawing command to add to the trace set; if it decides to continue drawing, the predicted primitive is rendered to its ``canvas'' and the process repeats.
We first
pass a $256\times 256$ target image and a rendering of the drawing commands so
far (encoded as a two-channel image) to a convolutional network. Given
the features extracted by the convnet, a multilayer perceptron then
predicts a distribution over the next drawing command to execute
(see Tbl.~\ref{drawingCommandTable}).
%We predict the drawing command token-by-token,
%\footnote{Tokens are atomic symbols in the programming language  -- so the model is \emph{not} predicting lines of code character-by-character, but instead the model knows about the syntactic structure of primitive drawing commands}
%%   conditioning each token both on the image
%% features and on the previously generated tokens. 
We also use a
differentiable attention mechanism (Spatial Transformer
Networks:~\cite{jaderberg2015spatial}) to let the model attend to
different regions of the image while predicting drawing commands.
%In Fig.~\ref{architecture} the network first decided to emit the \verb|circle| token
%conditioned on the image features, then it emitted the $x$ coordinate of
%the circle conditioned on the image features and the \verb|circle|
%token, and finally it predicted the $y$ coordinate of the circle
%conditioned on the image features, the \verb|circle| token, and the
%$x$ coordinate.
We currently constrain
coordinates to lie on a discrete $16\times 16$ grid,
but the grid could be made arbitrarily fine.

\tikzset{>=latex}
\begin{figure}[t]
  \begin{minipage}[c]{0.65\textwidth}
    \begin{tikzpicture}
  \node[draw,blue,ultra thick,anchor = west,inner sep=0pt,label=below:Target image: $I$](observation) at (0,-1) {\includegraphics[width = 2cm]{figures/expert-18.png}};
  \node[draw,blue,thick,anchor = west,inner sep=0pt,minimum width = 2cm,minimum height = 2cm,label=below:Canvas: render$(S)$] (canvas) at (0,-4) {};
  \draw[lightgray,ultra thin,step = 0.125] ([xshift = 0.5,yshift = 0.5]canvas.south west) grid ([xshift = -0.5,yshift = -0.5]canvas.north east);
  % draw partial image on canvas
  \draw (0.375cm,-3.25cm) circle (0.125cm);
  \draw (0.625cm,-3.625) circle (0.125cm);
  \draw (0.75cm,-3.375) circle (0.125cm);

  \node[draw,ultra thick,anchor = west,inner sep=0pt,minimum width = 1cm,minimum height = 2cm] (CNN) at (4,-2.5) {CNN};
  \node[inner sep = 0pt](tensorProduct) at ([xshift = -1.3cm]CNN.west) {$\bigoplus$};

  \node[draw,ultra thick,inner sep=0pt] (features) at ([xshift = 0.01cm]CNN.east) {};

  \node[draw,ultra thick,minimum size = 1cm](c1) at ([xshift = 1cm]features) {MLP};
  \node(l1) at ([yshift = -1.5cm]c1.south) {\verb|circle(|};
  \node[draw,ultra thick,minimum size = 1cm](a2) at ([xshift = 1cm,yshift = 1.5cm]c1.east) {STN};
  \node[draw,ultra thick,minimum size = 1cm](c2) at ([xshift = 1cm]c1.east) {MLP};
  \draw[->,ultra thick] (a2.south) -- (c2.north);
  \draw[->,ultra thick,red] (c1.south) -- (l1.north);
  \node(l2) at ([yshift = -1.5cm]c2.south) {\verb|X=7,|};
  \draw[->,ultra thick,red] (c2.south) -- (l2.north);
  \node[draw,ultra thick,minimum size = 1cm](a3) at ([xshift = 1cm,yshift = 1.5cm]c2.east) {STN};
  \node[draw,ultra thick,minimum size = 1cm](c3) at ([xshift = 1cm]c2.east) {MLP};
  
  \draw[->,ultra thick] (a3.south) -- (c3.north);
  \node(l3) at ([yshift = -1.5cm]c3.south) {\verb|Y=12)|};
  \draw[->,ultra thick,red] (c3.south) -- (l3.north);

  
  \draw[->,ultra thick] (features) -- (c1.west);
  \draw[->,ultra thick] (features) to[out = 45,in = 160] (a2.north);
  \draw[->,ultra thick] (features) to[out = 80,in = 140] (a3.north);
  \draw[->,ultra thick] ([xshift = 0.25cm]l1.north) -- (c2.west);
  \draw[->,ultra thick] ([xshift = 0.25cm]l1.north) -- ([yshift = -0.2cm]c3.west);
  \draw[->,ultra thick] ([xshift = 0.25cm]l2.north) -- ([yshift = -0.25cm]c3.west);

  \node(next)[draw,very thick,fit = (l1) (l2) (l3), dashed, label = below:{Next drawing command}] {};

  \draw[-{>[scale = 1.5]},very thick,dashed] (next.west) -- ([yshift = -0.2cm]canvas.east) node [midway, below, sloped] (TextNode) {Renderer}; %: \LaTeX~Ti\emph{k}Z};
  
  \draw[->,ultra thick] (canvas.east) -- (tensorProduct.south);%[yshift = -0.5cm]CNN.west);
  \draw[->,ultra thick] (observation.east) -- (tensorProduct.north);%([yshift = 0.5cm]CNN.west);
  \draw[->,ultra thick] (tensorProduct.east)  -- node[fill = white,rotate = 90] {{\tiny $256\times 256\times 2$}}  (CNN.west);
  \draw[-,ultra thick] (CNN.east) -- %node[fill = white,rotate = 90] {{\tiny $16\times 16\times 10$}}
  (features);
%  \draw[]
  
%  \node at (canvas.x,canvas.y) {Canvas};
\end{tikzpicture}
  \end{minipage}  \hfill%
  \begin{minipage}[c]{0.2\textwidth}
    \caption{Neural architecture for inferring specs from images. \textcolor{blue}{Blue}: network inputs. Black: network operations. \textcolor{red}{Red}: draws from a multinomial. \texttt{Typewriter font}: network outputs. Renders on a $16\times 16$ grid, shown in \textcolor{gray}{gray}. STN: differentiable attention mechanism~\citep{jaderberg2015spatial}.}  \label{architecture}
    \end{minipage}
\end{figure}

\begin{table}[h]
  \caption{Primitive drawing commands currently supported by our model.}
\label{drawingCommandTable}
\begin{tabular}{ll}\toprule
  \begin{tabular}{l}
    \verb|circle|$(x,y)$
  \end{tabular}& \begin{tabular}{l}
    Circle at $(x,y)$
    \end{tabular}\\
  \begin{tabular}{l}
    \verb|rectangle|$(x_1,y_1,x_2,y_2)$
  \end{tabular}&\begin{tabular}{l}
    Rectangle with corners at $(x_1,y_1)$ \& $(x_2,y_2)$
    \end{tabular}\\
  \begin{tabular}{l}
    \verb|line|$(x_1,y_1,x_2,y_2,$\\
    \hspace{1cm}$\text{arrow}\in\{0,1\},\text{dashed}\in\{0,1\})$
  \end{tabular}&\begin{tabular}{l}
    Line from $(x_1,y_1)$ to  $(x_2,y_2)$,\\\hspace{1cm}optionally with an arrow and/or dashed
    \end{tabular}\\
  \begin{tabular}{l}
    \verb|STOP|
  \end{tabular}&\begin{tabular}{l}
    Finishes spec inference
    \end{tabular}
\\  \bottomrule
\end{tabular}
\end{table}



%% For the model in Fig.~\ref{architecture}, the distribution over the next drawing command factorizes as:
%% \begin{equation}
%%   \probability_\theta [t_1t_2\cdots t_K | I,T] = \prod_{k = 1}^K \probability_\theta \left[t_k | a_\theta \left(f_\theta(I,\text{render}(T)) | \{t_j\}_{j = 1}^{k - 1}\right), \{t_j\}_{j = 1}^{k - 1}\right]
%% \end{equation}
%% where $t_1t_2\cdots t_K$ are the tokens in the drawing command, $I$ is
%% the target image, $T$ is a trace set, $\theta$ are the
%% parameters of the neural network, $f_\theta(\cdot,\cdot)$ is the
%% image feature extractor (convolutional network), and $a_\theta(\cdot|\cdot)$ is an attention mechanism. The distribution over
%% traces factorizes as:
%% \begin{equation}
%%   \probability_\theta [T|I] = \prod_{n = 1}^{|T|} \probability_\theta [T_n | I,T_{1:(n-1)}]\times\probability_\theta [\verb|STOP| | I,T]\label{objective}
%% \end{equation}
%% where $|T|$ is the length of trace set $T$, the subscripts
%% on $T$ index drawing commands within the trace (so $T_n$ is a sequence of tokens: $t_1t_2\cdots t_K$), and the \verb|STOP|
%% token is emitted by the network to signal that the trace set
%% explains the image.
We trained our network by sampling specs $S$ and target
images $I$ for randomly generated scenes\footnote{Because rendering ignores ordering  we put the drawing commands into a canonical order}
and maximizing $\probability_\theta[S|I]$,
%Eq.~\ref{objective}
 the likelihood of $S$ given $I$, with respect to
  model parameters $\theta$, by gradient ascent.
We trained on $10^5$ scenes, which takes a day on an Nvidia TitanX GPU.
Supplement Section 1 gives the full details of the architecture and training of this network.
%% Training does not require backpropagation across the sequence
%% of drawing commands: drawing to the canvas `blocks' the gradients,
%% effectively offloading memory to an external visual store.  
%% This approach is like an autoregressive version of
%% Attend-Infer-Repeat~\citep{eslami1603attend}, but critically, without a learned
%%   recurrent memory state, e.g. an RNN.  We can do without an RNN
%% because we learn from ground truth (image, trace) pairs.  This
%% allows us to handle scenes with many objects without
%% worrying about the conditioning of gradients as they propagate
%% over long sequences, and makes training more straightforward: it
%% is just maximum likelihood estimation of the model parameters.

Our network can ``derender'' random synthetic images
%those shown in Fig.~\ref{trainingData}.
by doing a beam search to
recover  specs maximizing $\probability_\theta[S|I]$. %% , where $\probability_\theta [\cdot |I]$ is
%% the probability assigned by the network conditioned on image $I$
 But, if the network predicts an incorrect
drawing command, it has no way of recovering from that error.  
%To derender an image with $n$ objects, it must correctly predict $n$ commands -- so its probability of success will decrease exponentially in $n$, assuming it has a nonzero chance of making a mistake.
For added robustness % as $n$ becomes large,
we treat the
 network outputs as proposals for a Sequential Monte Carlo (SMC) sampling scheme~\citep{SMCBook}.
%For the SMC sampler, we use pixel-wise distance as a surrogate for a likelihood function.
Our SMC sampler draws samples
from the distribution $\propto L(I|\text{render}(S))
\probability_\theta[S|I]$, where $L(\cdot | \cdot)$
%:\text{image}^2\to\mathbb{R}$
uses the pixel-wise distance between two images as a proxy for a
likelihood.
Here, the network is learning a proposal distribution to amortize the cost of inverting a generative model (the renderer)~\citep{paige2016inference}.
%% Unconventionally, the target distribution of the SMC sampler
%% includes the likelihood under the proposal distribution.
%% Intuitively, both the proposal
%% distribution and the distance function offer complementary signals for
%% whether a drawing command is correct.


\textbf{Experiment 1: Figure~\ref{syntheticResults}.}
  To evaluate which components of the model are necessary to parse complicated scenes,
  we compared  the neural network
  with SMC against the neural network by
itself (i.e., w/ beam search) or SMC by itself.  Only the combination of the two passes a
critical test of generalization: when trained on images with $\leq 12$
objects, it successfully parses scenes with many more objects than the
training data.
We compare with a baseline that produces the spec in one shot by
using the CNN to extract features of the input which are passed to an LSTM which finally predicts
the spec token-by-token (LSTM in Fig.~\ref{syntheticResults}).
This architecture is used in several successful neural models of image captioning (e.g.,~\cite{vinyals2015show}),
but, for this domain, cannot parse cluttered scenes with many objects.
\begin{figure}[h]\centering
  \begin{minipage}[c]{7cm}
      \includegraphics[width = 7cm]{figures/averageNumberOfErrors.png}    
  \end{minipage}\hspace{0.5cm}%
  \begin{minipage}[c]{6cm}
      \caption{Parsing~\LaTeX~output after training on diagrams with $\leq 12$ objects. Out-of-sample generalization: Model generalizes to scenes with many more objects ($\approx$ at ceiling when tested on twice as many objects as were in the training data). Neither SMC nor the neural network are sufficient on their own. \# particles varies by model: we compare the models \emph{with equal runtime} ($\approx 1$ sec/object). Average number of errors is (\# incorrect drawing commands predicted by model)$+$(\# correct commands that were not predicted by model).}\label{syntheticResults}
    \end{minipage}
  \end{figure}

\subsection{Generalizing to real hand drawings}\label{generalizingTheHandDrawings}
%% \begin{wrapfigure}{R}{6cm}
%% \centering\vspace{-0.7cm}  \begin{minipage}[t]{2.5cm}\centering\includegraphics[width = 2cm]{figures/expert-60-reduced.png}
%%     \subcaption{}
%%   \end{minipage}\\
%%    \begin{minipage}[t]{2.5cm}\includegraphics[width = 2cm]{figures/60-groundTruth-reduced.png}
%%     \subcaption{}
%%   \end{minipage}%
%%   \begin{minipage}[t]{2.5cm}\includegraphics[width = 2cm]{figures/60-1-reduced.png}
%%     \subcaption{}
%%   \end{minipage}%
%%     \caption{(a): hand drawing. (b): Rendering of the trace set our model infers for (a). (c): noisy rendering of (b).}\label{handDrawingExamples}
%% \end{wrapfigure}

%A practical application of our neural network is the automatic conversion of hand drawings into a subset of \LaTeX.
 We trained the model
to generalize to hand drawings by introducing noise into the
renderings of the training target images, where the noise process  mimics the kinds of variations found in hand drawings.
While our neurally-guided SMC procedure
used pixel-wise distance as a surrogate for a likelihood function ($L(\cdot|\cdot)$ in Sec.~\ref{neuralNetworkSection}),
 pixel-wise distance fares poorly on hand drawings, which never exactly match
the model's renders.
So, for hand drawings,
we learn a surrogate likelihood function,
$L_{\text{learned}}(\cdot|\cdot)$.
The density $L_{\text{learned}}(\cdot|\cdot)$ is predicted by a convolutional network that we train to predict
the distance between two specs conditioned upon their renderings.
We train $L_{\text{learned}}(\cdot |\cdot )$  to approximate the symmetric difference,
which is  the number of drawing commands by which two specs  differ:
  \begin{equation}
    -\log L_{\text{learned}}(\text{render}(S_1)|\text{render}(S_2))\approx |S_1 - S_2| + |S_2 - S_1|\label{symmetricDistance}
  \end{equation}
Supplement Section 2 explains the architecture and training of $L_{\text{learned}}$.



%% Pixel-wise distance metrics are sensitive to the details of how
%% arrows, dashes, and corners are drawn -- but we wish to be invariant
%% to these details.
%% So, we learn a distance metric over images that
%% approximates the distance metric in the search space over traces.

  \textbf{Experiment 2: Figures~\ref{drawingSuccesses}--\ref{drawingIntersectionOverUnion}.}
    We evaluated, but did not train, our system on 100 real hand-drawn figures; see Fig.~\ref{drawingSuccesses}--\ref{drawingFailures}.
These were drawn carefully but not perfectly with the aid of graph paper.
%Because our model assumes that objects are snapped to a $16\times 16$ grid, 
%we made the drawings on graph paper.
For each drawing we annotated a ground truth spec and had the neurally guided SMC sampler
produce $10^3$ samples. % for each drawing.
For 63\% of the drawings, the Top-1 most likely sample exactly matches the
ground truth; with more samples, the model finds specs
that are closer to the ground truth annotation (Fig.~\ref{drawingIntersectionOverUnion}).
We will show that the program synthesizer
corrects some of these small errors (Sec.~\ref{synthesizerHelpsParsing}).
%% Because the model sometimes makes mistakes % on hand drawings,
%% we envision it working as follows:
%% a user sketches a diagram,
%% and the system responds by proposing a few candidate interpretations.
%% The user could then select the one closest to their intention and edit it if necessary.

\setlength\tabcolsep{3.5pt}
\begin{figure}[H]
  \begin{minipage}[t]{9cm}
    \begin{tabular}{llll}
      \includegraphics[width = 2cm]{figures/expert-38.png}&
      \includegraphics[width = 2cm]{figures/expert-73.png}&
      \includegraphics[width = 2cm]{figures/expert-77.png}&
      \includegraphics[width = 2cm]{figures/expert-21.png}
      \\
      \includegraphics[width = 2cm]{figures/38-parse.png}     &
      \includegraphics[width = 2cm]{figures/73-parse.png}     &
      \includegraphics[width = 2cm]{figures/77-parse.png}    &
      \includegraphics[width = 2cm]{figures/21-parse.png}    
      \end{tabular}
    \caption{Left to right: Ising model, recurrent network architecture, figure from a deep learning textbook \cite{Goodfellow-et-al-2016}, graphical model}\label{drawingSuccesses}
  \end{minipage}
\hfill  \begin{minipage}[t]{4.25cm}
    \begin{tabular}{ll}
          \includegraphics[width = 2cm]{figures/expert-1.png}&
          \includegraphics[width = 2cm]{figures/expert-34.png}\\
      \includegraphics[width = 2cm]{figures/expert-1-parse.png}     &
              \includegraphics[width = 2cm]{figures/34-parse.png}    
    \end{tabular}


    \caption{Near misses. Rightmost: illusory contours (note: no SMC in rightmost)}\label{drawingFailures}
  \end{minipage}


%  \caption{Example drawings above model outputs. See also Fig.~\ref{firstPageExamples}. Stochastic search (SMC) can help correct for these errors, as can the program synthesizer (Section~\ref{synthesizerHelpsParsing})}\label{lotsOfHandDrawings}
\end{figure}
\setlength\tabcolsep{6pt}

\begin{figure}[h]\vspace{-1cm}\centering
  \begin{minipage}[c]{0.57\textwidth} 
    \centering  \includegraphics[width = 8cm]{figures/drawingAccuracy.png}            \vspace{-0.5cm} 
  \end{minipage}\hfill%
      \begin{minipage}[c]{0.4\textwidth} 
  \caption{How close are the model's outputs to the ground truth on hand drawings, as we consider larger sets of samples (1, 5, 100)?
    Distance to ground truth measured by the intersection over union (IoU) of predicted spec vs. ground truth spec: IoU of sets (specs) $A$ and $B$ is $|A\cap B|/|A\cup B|$. (a) for 63\% of drawings the model's top prediction is exactly correct; (b) for 70\% of drawings the ground truth is in the top 5 model predictions; (c) for 4\% of drawings all of the model outputs have no overlap with the ground truth. Red: the full model. Other colors: lesioned versions of our model.}\label{drawingIntersectionOverUnion}            \vspace{-0.5cm}
      \end{minipage}

\end{figure}

%\pagebreak





\section{Synthesizing graphics programs from specs}\label{programSynthesisSection}
Although the spec describes the contents
of a scene, it does not encode higher-level features of an image
such as repeated motifs or symmetries, which are more naturally captured by a graphics program.
We seek to synthesize graphics programs from their specs.

We constrain the space of programs by writing down a context
free grammar over  programs -- 
%Although it might be desirable to synthesize programs in a Turing-complete language such as Lisp or Python, a more tractable approach is to specify
what in the program
languages community is called a Domain Specific Language (DSL)~\citep{polozov2015flashmeta}. Our DSL (Tbl.~\ref{DSL})
encodes prior knowledge of what graphics programs tend to look like.

\begin{table}[H]
  \caption{Grammar over graphics programs. We allow loops (\texttt{for}) with conditionals (\texttt{if}), vertical/horizontal reflections (\texttt{reflect}), variables (Var) and affine transformations ($\mathbb{Z}\times$Var\texttt{+}$\mathbb{Z}$).}\label{DSL}
  \begin{tabular}{rl}\toprule
  Program$\to$&Statement; $\cdots$; Statement\\
  Statement$\to$&\texttt{circle}(Expression,Expression)\\
  Statement$\to$&\texttt{rectangle}(Expression,Expression,Expression,Expression)\\
  Statement$\to$&\texttt{line}(Expression,Expression,Expression,Expression,Boolean,Boolean)\\
  Statement$\to$&\texttt{for}$(0\leq \text{Var}  < \text{Expression})$\texttt{ \{ if }$(\text{Var} > 0)$\texttt{ \{ }Program\texttt{ \}; }Program\texttt{ \}}\\
  Statement$\to$&\texttt{reflect}$(\text{Axis})$\texttt{ \{ }Program\texttt{ \}}\\
  Expression$\to$&$\mathbb{Z}\times$Var\texttt{+}$\mathbb{Z}$\\
%  Var$\to$&A free (unused) variable\\
  Axis$\to$&\texttt{X = }$\mathbb{Z}$ | \texttt{Y = }$\mathbb{Z}$\\
    $\mathbb{Z}\to$&an integer\\\bottomrule
  \end{tabular}
\end{table}

Given the DSL and a spec $S$, we want a program that both satisfies $S$
and, at the same time, is the ``best'' explanation of $S$.
For example, we might prefer more general programs or, in the spirit of Occam's razor,
prefer shorter programs.
We wrap these intuitions up into a cost function over programs,
and seek the minimum cost program consistent with $S$:
\begin{equation}
  %  \text{program}(S) = \argmin_{p\in \text{DSL, s.t. }p \text{ consistent w/ } S} \text{cost}(p)\label{programObjective}
    \text{program}(S) = \argmax_{p\in \text{DSL}} \indicator\left[p \text{ consistent w/ } S \right]\exp \left( -\text{cost}(p) \right)\label{programObjective}
\end{equation}
We define the
cost of a program to be the number of Statement's it contains (Tbl.~\ref{DSL}).
We also penalize using many different numerical constants; see Supplement Section 3.
Returning to the generative model in Fig.~\ref{roadmap},
this setup is the same as saying that the prior probability of a program $p$ is $\propto \exp\left(-\text{cost}(p) \right)$ and the likelihood of a spec $S$ given a program $p$ is $\indicator[p\text{ consistent w/ }S]$.
%\remark{The flow here is a bit off/backwards. ``We want a program that evaluates to $T$ and also minimizes some measure of program cost''---why do we care about cost? It'd be better to start by making a ``Bayesian Occam's razor'' appeal (e.g. the most compact/general program is the more likely explanation) and then saying that one way to do this is to minimize a cost function which is proportional to program length.}

The constrained optimization problem in
Eq.~\ref{programObjective} is intractable in general, but there
exist efficient-in-practice tools for finding exact solutions to such
program synthesis problems. We use the state-of-the-art Sketch
tool~\citep{solar2008program}.
%At a high level,
Sketch takes as input a space of programs, along with
a specification of the program's behavior and optionally a cost
function.  It translates the synthesis problem into a constraint
satisfaction problem and then uses a SAT solver to find a minimum-cost
program satisfying the specification.  Sketch requires a
 \emph{finite program space}, which here means that the depth of the
program syntax tree is bounded (we set the bound to 3),
but has the guarantee that it 
always eventually finds a globally optimal solution.
In exchange for this optimality guarantee
it comes with no guarantees
on runtime.
For our domain synthesis times vary from minutes to hours,
with 27\% of the drawings timing out the synthesizer after 1 hour.
Tbl.~\ref{exampleSynthesisResults} shows programs recovered by our system.
A main impediment to our use of these general techniques is
the prohibitively high cost of searching for programs.
We next describe how to learn to synthesize programs much faster (Sec.~\ref{learningASearchPolicy}),
timing out on 2\% of the drawings and solving 58\% of problems within a minute.

\newcommand{\exampleProgramSize}{4cm}
\newcommand{\exampleTraceSize}{3.5cm}
\newcommand{\exampleDrawingSize}{1.25cm}
\lstset{basicstyle = \scriptsize\ttfamily}

\begin{table}[t]
  \caption{Drawings (left), their specs (middle left), and programs synthesized from those specs (middle right). Compared to the specs the programs are more compressive (right: programs have fewer lines than specs) and automatically group together related drawing commands. Note the nested loops  and conditionals in the Ising model, combination of symmetry and iteration in the bottom figure,  affine transformations in the top figure, and the complicated program in the second figure to bottom.}\label{exampleSynthesisResults}
%  \vspace{-0.6cm}
\centering  \begin{tabular}{m{1.5cm}llc}
    \toprule
    \textbf{Drawing}&\textbf{Spec}&\textbf{Program}&%\begin{tabular}{c}
      \textbf{Compression factor}%\\\textbf{factor}
      %      \end{tabular}\\
      \\
    \midrule
    \includegraphics[width = \exampleDrawingSize]{figures/expert-29-trim.png}&
\begin{minipage}{\exampleTraceSize}\begin{lstlisting}
Line(2,15, 4,15)
Line(4,9, 4,13)
Line(3,11, 3,14)
Line(2,13, 2,15)
Line(3,14, 6,14)
Line(4,13, 8,13)
\end{lstlisting}
\end{minipage}&     \begin{minipage}{\exampleProgramSize} \begin{lstlisting}
for(i<3)
 line(i,-1*i+6,
      2*i+2,-1*i+6)
 line(i,-2*i+4,i,-1*i+6)
       \end{lstlisting}
     \end{minipage}&$\frac{6}{3} = 2\text{x}$\\\midrule
     \includegraphics[width = \exampleDrawingSize]{figures/expert-52-trim.png}&
\begin{minipage}{\exampleTraceSize}\begin{lstlisting}
Line(5,13,2,10,arrow)
Circle(5,9)
Circle(8,5)
Line(2,8, 2,6,arrow)
Circle(2,5)
\end{lstlisting}
\small\emph{... etc. ...; 13 lines}
\end{minipage}&
             \begin{minipage}{\exampleProgramSize}\begin{lstlisting}
circle(4,10)
for(i<3)
 circle(-3*i+7,5)
 circle(-3*i+7,1)
 line(-3*i+7,4,-3*i+7,2,arrow)
 line(4,9,-3*i+7,6,arrow)
\end{lstlisting}
\end{minipage}&$\frac{13}{6} = 2.2\text{x}$\\\midrule
    \includegraphics[width = \exampleDrawingSize]{figures/expert-38-trim.png}&
\begin{minipage}{\exampleTraceSize}\begin{lstlisting}
Circle(5,8)
Circle(2,8)
Circle(8,11)
Line(2,9, 2,10)
Circle(8,8)
Line(3,8, 4,8)
Line(3,11, 4,11)
\end{lstlisting}
  \small\emph{... etc. ...; 21 lines}
\end{minipage}&\begin{minipage}{\exampleProgramSize}
\begin{lstlisting}
for(i<3)
 for(j<3)
  if(j>0)
   line(-3*j+8,-3*i+7,
        -3*j+9,-3*i+7)
   line(-3*i+7,-3*j+8,
        -3*i+7,-3*j+9)
  circle(-3*j+7,-3*i+7)
\end{lstlisting}
\end{minipage}&$\frac{21}{6} = 3.5\text{x}$\\\midrule
\includegraphics[width = \exampleDrawingSize]{figures/expert-101-trim.png}&
\begin{minipage}{\exampleTraceSize}\begin{lstlisting}
Rectangle(1,10,3,11)
Rectangle(1,12,3,13)
Rectangle(4,8,6,9)
Rectangle(4,10,6,11)
\end{lstlisting}
  \small\emph{... etc. ...; 16 lines}
\end{minipage}&\begin{minipage}{\exampleProgramSize}
\begin{lstlisting}
for(i<4)
 for(j<4)
  rectangle(-3*i+9,-2*j+6,
            -3*i+11,-2*j+7)
\end{lstlisting}
\end{minipage}&$\frac{16}{3} = 5.3\text{x}$\\\midrule
%% \includegraphics[width = \exampleDrawingSize]{figures/expert-75-extra-trim.png}&
%% \begin{minipage}{\exampleTraceSize}\begin{lstlisting}
%% Line(11,14,13,14,arrow)
%% Circle(10,10)
%% Line(10,13,10,11,arrow)
%% Circle(6,10)
%% \end{lstlisting}
%%     \small\emph{... etc. ...; 15 lines}
%% \end{minipage}&
%% \begin{minipage}{\exampleProgramSize}
%%     \begin{lstlisting}
%% for(i<4)
%%  line(-4*i+13,4,-4*i+13,2,arrow)
%%  for(j<3)
%%   if(j>0)
%%    circle(-4*i+13,4*j+-3)
%%   line(-4*j+10,5,-4*j+12,5,
%%        arrow)
%% \end{lstlisting}
%%   \end{minipage}&$\frac{15}{6} = 2.5\text{x}$\\\midrule    

  \includegraphics[width = \exampleDrawingSize]{figures/expert-71-trim.png}&
\begin{minipage}{\exampleTraceSize}\begin{lstlisting}
Line(3,10,3,14,arrow)
Rectangle(11,8,15,10)
Rectangle(11,14,15,15)
Line(13,10,13,14,arrow)
  \end{lstlisting}\small\emph{... etc. ...; 16 lines}
  \end{minipage}&\begin{minipage}{\exampleProgramSize}
%% \begin{lstlisting}
%% for (i<3)
%%  circle(-3*i+7,1)
%%  circle(-3*i+7,6)
%%  line(-3*i+7,-1*i+4,-3*i+7,5)
%% \end{lstlisting}
\begin{lstlisting}
for(i<3)
 line(7,1,5*i+2,3,arrow)
 for(j<i+1)
  if(j>0)
   line(5*j-1,9,5*i,5,arrow)
  line(5*j+2,5,5*j+2,9,arrow)
 rectangle(5*i,3,5*i+4,5)
 rectangle(5*i,9,5*i+4,10)
rectangle(2,0,12,1)
\end{lstlisting}
\end{minipage}&$\frac{16}{9} = 1.8\text{x}$\\\midrule    

  \includegraphics[width = \exampleDrawingSize]{figures/expert-72-trim.png}&

\begin{minipage}{\exampleTraceSize}\begin{lstlisting}
Circle(2,8)
Rectangle(6,9, 7,10)
Circle(8,8)
Rectangle(6,12, 7,13)
Rectangle(3,9, 4,10)
\end{lstlisting}\small\emph{... etc. ...; 9 lines}
  \end{minipage}&\begin{minipage}{\exampleProgramSize}
\begin{lstlisting}
reflect(y=8)
 for(i<3)
  if(i>0)
   rectangle(3*i-1,2,3*i,3)
  circle(3*i+1,3*i+1)
\end{lstlisting}
\end{minipage}&$\frac{9}{5} = 1.8\text{x}$ \\\bottomrule
  \end{tabular}
  \end{table}

\subsection{Learning a search policy for synthesizing programs}\label{learningASearchPolicy}

We want to leverage powerful, domain-general techniques from the program synthesis community,
but make them much faster by
learning a domain-specific \textbf{search policy}.
A search policy poses search problems
like those in Eq.~\ref{programObjective},
but also offers additional constraints on the structure of the program (Tbl.~\ref{policyOutput}).
For example, a policy might decide to first try searching over small programs before searching over large programs,
or decide to prioritize searching over programs that have loops.

A search policy $\pi_\theta(\sigma  | S )$ takes as input a spec $S$ and predicts a distribution over synthesis problems, each of which is written $\sigma $ and corresponds to a set of possible programs to search over (so $\sigma \subseteq \text{DSL}$).
Good policies will prefer tractable program spaces,
so that the search procedure will terminate early, 
but should also prefer program spaces likely to contain
programs that concisely explain the data.
These two desiderata are in tension:
tractable synthesis problems involve searching over smaller spaces,
but smaller spaces are less likely to contain good programs.
%a minimum cost program
%for explaining $T$.
Our goal now is to find the parameters of the policy, written $\theta$, that best navigate this trade-off.


Given a search policy, what is the best way of using it to quickly find minimum cost programs?
We use a bias-optimal search algorithm (c.f. Schmidhuber 2004~\citep{schmidhuber2004optimal}):

\noindent\textbf{Definition: Bias-optimality.} 
   A search algorithm is $n$-\emph{bias optimal}
with respect to a distribution $\probability_{\text{bias}}[\cdot ]$ if it is
guaranteed to find a solution in $\sigma $ after searching for at least time
$n\times\frac{t(\sigma )}{\probability_{\text{bias}}[\sigma ]}$, where $t(\sigma )$ is the time it
takes to verify that $\sigma $ contains a solution to the
search problem.




%Intuitively, an $n$-bias optimal search algorithm spends at least $\probability_{\text{bias}}[\sigma ]/n$ of its time exploring solution space $\sigma $.
Bias-optimal search over program spaces is known as \textbf{Levin
  Search}~\cite{levin1973universal}; an example of a $1$-bias optimal
search algorithm is an ideal time-sharing system that allocates
$\probability_{\text{bias}}[\sigma ]$ of its time to trying $\sigma $.
We construct a $1$-bias optimal search algorithm by identifying
$\probability_{\text{bias}}[\sigma ] = \pi_\theta(\sigma |S)$ and
$t(\sigma ) = t(\sigma|S)$, where $t(\sigma|S)$ is how long the
synthesizer takes to search $\sigma $ for a program for
$S$. Intuitively, this means that the search algorithm explores the
entire program space, but spends most of its time in the regions of
the space that the policy judges to be most promising.  Concretely,
this means that our synthesis strategy is to run many different
program searches \emph{in parallel} (i.e., run in parallel different
instances of the synthesizer, one for each $\sigma$), but to
allocate compute time to a $\sigma $ in proportion to
$\pi_\theta(\sigma |S)$.

%% \begin{figure}\centering
%%   \begin{minipage}[c]{0.4\textwidth}
%%     \begin{tikzpicture}[scale=0.8]
%%         \node[anchor = west] at (0,5.25) {Entire program search space};
%%         \footnotesize
%%     \draw[fill = yellow,fill opacity = 0.15,draw = black] (0,0) rectangle (5,5);
%%     \draw [fill = yellow, opacity = 0.2] (0,0)--(0,2)--(4,0)--(0,0);
%%     \draw [fill = yellow, opacity = 0.4] (0,5)--(0,1)--(3,5);
%%     \draw [fill = yellow, opacity = 0.6] (2.5,5)--(3.2,0)--(5,0)--(5,5);
%%     \draw (0,2) -- node[below,sloped]{short programs} (4,0);
%%     \draw (0,2) -- node[above,sloped]{long programs} (4,0);
%%     \draw (2.5,5) -- node[above,sloped]{programs w/ reflections} (3.2,0);
%%     \draw (0,1) -- node[above,sloped]{ programs w/ loops} (3,5);
%%     %% \node(p1)[anchor =west ] at (5.5,3.5) {$\pi_\theta(\text{short, no loop/reflect}|S) = $};
%%     %% \draw [fill = yellow, fill opacity = 0.2,draw = black]  ([xshift = 0cm,yshift = -0.2cm]p1.east) rectangle ([xshift = 0.4cm,yshift = 0.2cm]p1.east);
%%     %    \node(p2)[anchor =west ] at ([yshift = -0.5cm]p1.west) {$\pi_\theta(\text{long, loops}|S) = $};
    
%%     %% \node(p2)[anchor =west ] at (0,-0.5) {$\pi_\theta(\text{long, loops}|S) = $};
%%     %% \draw [fill = yellow, fill opacity = 0.4,draw = black]  ([xshift = 0cm,yshift = -0.2cm]p2.east) rectangle ([xshift = 0.4cm,yshift = 0.2cm]p2.east);
%%     %% \node(p3)[anchor =west ] at ([yshift = -0.5cm]p2.west) {$\pi_\theta(\text{long, no loop/reflect}|S) = $};
%%     %% \draw [fill = yellow, fill opacity = 0.15,draw = black]  ([xshift = 0cm,yshift = -0.2cm]p3.east) rectangle ([xshift = 0.4cm,yshift = 0.2cm]p3.east);
    
%%     %% \node(p4)[anchor =west ] at ([yshift = -0.5cm]p3.west) {$\pi_\theta(\text{long, reflects}|S) = $};
%%     %% \draw [fill = yellow, fill opacity = 0.6,draw = black]  ([xshift = 0cm,yshift = -0.2cm]p4.east) rectangle ([xshift = 0.4cm,yshift = 0.2cm]p4.east);

%%     %\node(p5)[anchor =west ] at ([yshift = -0.5cm]p3.west) {\emph{etc.}};
%%   \end{tikzpicture}
%%     \end{minipage}
%%   \begin{minipage}[c]{0.5\textwidth}
%%           \caption{The bias-optimal search algorithm divides the entire (intractable) program search space in to (tractable) program subspaces (written $\sigma $), each of which contains a restricted set of programs. For example, one subspace might be short programs which don't loop. The policy $\pi$ predicts a distribution over program subspaces. The weight that $\pi$ assigns to a subspace is indicated by its yellow shading in the above figure, and is conditioned on the spec $S$.}
%%     \end{minipage}
%%   \end{figure}


Now in theory any $\pi_\theta(\cdot |\cdot ) $ is a bias-optimal searcher.
But the actual runtime of the algorithm depends strongly upon
the bias $\probability_{\text{bias}}[\cdot ]$.
Our new approach is to learn $\probability_{\text{bias}}[\cdot ]$
by picking the policy minimizing the
expected bias-optimal time to solve a training corpus, $\mathcal{D}$, of graphics program synthesis problems:
\begin{align}
\textsc{Loss}(\theta ; \mathcal{D})& =  \expect_{S\sim\mathcal{D}}\left[ \min_{\sigma\in \text{\textsc{Best}}(S)}\frac{t(\sigma | S)}{\pi_\theta (\sigma | S)}\right] + \lambda \Vert\theta\Vert_2^2\label{policyLoss}\\
\text{where }  \sigma \in\text{\textsc{Best}}(S) &\text{ if  a minimum cost program for }S \text{ is in }\sigma .\nonumber %\text{cost}(\text{program}(p)) = \min_{p\in \sigma \text{, s.t. }p\text{ evaluates to }S} \text{cost}(p) \nonumber
\end{align}
%Practically, bias optimality has now bought us the following: (1) a guarantee that the policy will always find the minimum cost program; and (2) a differentiable loss function for the policy parameters that takes into account the cost of searching, in contrast to e.g. DeepCoder~\citep{BalGauBroetal16}.

To generate a training corpus for learning a policy,
we synthesized minimum cost programs for each  drawing
and for each $\sigma $,
%Solving these
%\footnote{There are 100 the drawings and $2\times 2\times 2\times 3 = 24$ different values of $\sigma $}
%synthesis problems takes a little over a day on 40 CPUs.
%With the training set in hand we  %This loss is differentiable but nonconvex even if $\pi_{\theta}(\cdot |\cdot )^{-1}$ is convex.
then minimized~\ref{policyLoss} using gradient descent while annealing a softened minimum to the hard minimization equation~\ref{policyLoss}.
%See supplement for details.
Because we want to learn a policy from only $100$ drawings,
we parameterize $\pi$ with a  low-capacity bilinear model with only 96 real-valued parameters. Supplement Section 4 further details the parameterization and training of the policy.
%for $\pi$:
%% \begin{equation}
%%   \pi_{\theta}(\sigma |S)\propto \exp \left( \phi_{\text{params}}(\sigma )^\top\theta \phi_{\text{spec}}(S)\right)
%% \end{equation}
%% where $\phi_{\text{params}}(\sigma  )$ is a one-hot encoding of
%% the parameter settings of $\sigma $ (see Tbl.~\ref{policyOutput})
%% and $\phi_{\text{spec}}(S)$ extracts a few simple features of the trace set $S$;
%% see supplement for details.

\textbf{Experiment 3: Table~\ref{policyEvaluation}; Figure~\ref{policyHistogram}; Supplement Section 4.}
We compare synthesis times for our learned search policy
with 4 alternatives:
 \emph{Sketch}, which poses the
 entire problem wholesale to the Sketch program synthesizer;
 \emph{DC}, a DeepCoder--style model that learns to predict which program components (loops, reflections)
 are likely to be useful~\cite{BalGauBroetal16};
 \emph{End--to-End},
 which trains a recurrent neural network to regress directly from images to programs;
and an \emph{Oracle},
a policy which always picks the quickest to search $\sigma $
 also containing a minimum cost program.
Our approach improves upon Sketch by itself,
and comes close to the Oracle's performance.
One could never construct this Oracle,
because the agent does not know ahead of time which
$\sigma $'s contain minimum cost programs nor does it know how long each
$\sigma $ will take to search.
%The Oracle is an upper bound on the performance of any search policy.
With this learned policy in hand we can synthesize 58\% of programs within a minute.
%
\begin{table}[h]\centering
  \caption{Parameterization of different ways of posing the program synthesis problem. The policy learns to choose parameters likely to quickly yield a minimal cost program.
    %We slightly abuse notation by writing $\sigma $ to mean an assignment to each of these parameters (so $\sigma $ assumes one of 24 different values) and to mean the set of programs selected by that parameterization (so $\sigma \subseteq \text{DSL}$)
  }\label{policyOutput}
  \begin{tabular}{lll}\toprule
  Parameter&Description&Range\\\midrule
  Loops?&Is the program allowed to loop?&$\{\text{True},\text{False}\}$\\
  Reflects?&Is the program allowed to have reflections?&$\{\text{True},\text{False}\}$\\
  Incremental?&Solve the problem piece-by-piece or all at once?&$\{\text{True},\text{False}\}$\\
  Maximum depth& Bound on the depth of the program syntax tree&$\{1,2,3\}$
  \\\bottomrule
  \end{tabular}\vspace{-0.5cm}
  \end{table}
\begin{table}[h]
  \centering
  \begin{minipage}[c]{0.4\textwidth}
  \begin{tabular}{lll}
    \toprule Model&\begin{tabular}{c}
      Median \\search time
    \end{tabular}&\begin{tabular}{c}
      Timeouts\\(1 hr)
      \end{tabular}\\\midrule
    Sketch&274 sec&27\%\\
    DC&187 sec&2\%\\
    End--to--End&63 sec&94\%\\
    Oracle&6 sec&2\%\\
    Ours&28 sec&2\%\\\bottomrule 
    \end{tabular}
  \end{minipage}\hfill%
  \begin{minipage}[c]{0.5\textwidth}
    \caption{Time to synthesize a minimum cost program. Sketch: out-of-the-box performance of Sketch~\citep{solar2008program}. DC: Deep--Coder style baseline that predicts program components, trained like~\cite{BalGauBroetal16}. End--to--End: neural net trained to regress directly from images to programs, which fails to find valid programs 94\% of the time. Oracle:  upper bounds  the performance of any bias--optimal search policy. Ours:  %% $\infty = $ timeout. Red dashed line is median time.
           evaluated w/ 20-fold cross validation.}\label{policyEvaluation}
  \end{minipage}

\vspace{0.2cm}
  
  \includegraphics[width = \textwidth]{figures/policyComparison_basic_DC+bias_20.png}%
  \captionof{figure}{Time to synthesize a minimum cost program (compare w/ Table~\ref{policyEvaluation}). End--to--End: not shown because it times out on 96\% of drawings, and has its median time (63s) calculated only on non-timeouts, wheras the other comparisons include timeouts in their median calculation. $\infty = $ timeout. Red dashed line is median time.}\label{policyHistogram}
\end{table}



%\pagebreak




\section{Applications of graphics program synthesis}\label{applicationsSection}
%% Why synthesize a graphics program,
%% if the spec already suffices to recover the objects in an image?
%% Within our domain of hand-drawn figures, graphics program synthesis has several uses:

\subsection{Correcting errors made by the neural network}\label{synthesizerHelpsParsing}
\begin{wrapfigure}{r}{5cm}\vspace{-0.5cm}
  \includegraphics[width = 5cm]{figures/programSuccess7.png}
  \includegraphics[width = 5cm]{figures/programSuccess16.png}
  \caption{Left: hand drawings. Center: interpretations favored by the deep network. Right: interpretations favored after learning a prior over programs. The prior favors  simpler programs, thus (top) continuing the pattern of not having an arrow is preferred, or (bottom) continuing the ``binary search tree'' is preferred.}\label{exampleOfProgramCorrectingMistake}
\vspace{-0.75cm}  \end{wrapfigure}
The program synthesizer corrects errors made by the neural network by favoring specs which lead to more
concise or general programs.  For example, figures with perfectly aligned objects are preferable, %% to figures whose parts are slightly misaligned,
and precise alignment lends itself to short
programs.
Concretely,
we run the program synthesizer on the
Top-$k$  most likely specs output by the neurally guided sampler.
Then, the system
reranks the Top-$k$  by  the prior probability of their programs.
The prior probability of a program is learned by optimizing the parameters of the prior so as to maximize
the likelihood of the ground truth specs;
see supplement for details.
But, this procedure can only correct errors when
a correct spec is in the Top-$k$.
Our sampler could only do better on
7/100 drawings by looking at the Top-100 samples
(see Fig.~\ref{drawingIntersectionOverUnion}),
precluding a statistically significant analysis of how much
learning a prior over programs could help correct errors.
But,
learning this prior does sometimes 
help correct mistakes made by the neural network; %% , and also
%% occasionally introduces mistakes of its own; 
see
Fig.~\ref{exampleOfProgramCorrectingMistake} for a representative
example of the kinds of corrections that it makes.
See Supplement Section 5 for details.



%% This is equivalent to doing
%% MAP inference in a generative model where the program is first drawn
%% from $\probability_{\beta} [\cdot]$, then the program is executed deterministically,
%% and then we observe a noisy version of the program's output, where $L_\text{learned}(I|\text{render}(\cdot))\times\probability_\theta[\cdot|I]$
%% is our observation model.

%% Learning this prior over programs can
%%  On the whole
%% it modestly improves our Top-1 accuracy from 58\% to 61\%.  Recall that
%% from Fig.~\ref{drawingIntersectionOverUnion} that the best improvement
%% in accuracy we could possibly get is 65\% by looking at the top 10 traces. 


%% \subsection{Modeling similarity between drawings}
%% Modeling drawings using programs opens up new ways to measure similarity between them.
%% For example, we might say that two drawings are similar if they both contain loops of length 4,
%% or if they share a reflectional symmetry,
%% or if they are both organized according to a grid-like structure.

%% We measure the similarity between two drawings by extracting features
%% of the lowest-cost programs that describe them. Our features are counts of the number of times that different components in the
%% DSL were used (Tbl.~\ref{DSL}).
%% We  then find drawings which are either close together or far apart in program feature space.
%% One could use many
%% alternative similarity metrics between drawings which would capture pixel-level similarities while missing high-level geometric similarities.
%% We used our learned distance metric between trace sets, $L_{\text{learned}}(\cdot|\cdot)$,
%% to find drawings that are either close together or far apart according to the learned
%% distance metric over images.
%% Fig.~\ref{similarityMatrix} illustrates the kinds of drawings that these different metrics put closely together,
%% while Sec.~2 of the supplement shows low dimensional projections of
%% program and image features.

%% \newcommand{\similaritySize}{1cm}
%% \newcommand{\similarityArrowSize}{0.35cm}
%% \begin{figure}[t]
%%   \begin{tabular}{|c|c|}
%%     \multicolumn{1}{c}{\textbf{Close in program space, far in image space}}&
%%     \multicolumn{1}{c}{\textbf{Close in image space, far in program space}}\\\hline&\\
    
%%     \begin{tabular}{l}
%%       \begin{minipage}{\similaritySize}\includegraphics[width = \similaritySize]{figures/expert-52-trim.png}\end{minipage}\begin{minipage}{\similarityArrowSize}$\leftrightarrow$\end{minipage}\begin{minipage}{\similaritySize}\includegraphics[width = \similaritySize]{figures/expert-79-trim.png}\end{minipage}\\\\
%%       \begin{minipage}{\similaritySize}\includegraphics[width = \similaritySize]{figures/expert-16-trim.png}\end{minipage}\begin{minipage}{\similarityArrowSize}$\leftrightarrow$\end{minipage}\begin{minipage}{\similaritySize}\includegraphics[width = \similaritySize]{figures/expert-17-trim.png}\end{minipage}
%%     \end{tabular} &
    
%%     \begin{tabular}{l}
%%       \begin{minipage}{\similaritySize}\includegraphics[width = \similaritySize]{figures/expert-22-trim.png}\end{minipage}\begin{minipage}{\similarityArrowSize}$\leftrightarrow$\end{minipage}\begin{minipage}{\similaritySize}\includegraphics[width = \similaritySize]{figures/expert-77-trim.png}\end{minipage}\\\\
%%       \begin{minipage}{\similaritySize}\includegraphics[width = \similaritySize]{figures/expert-64-trim.png}\end{minipage}\begin{minipage}{\similarityArrowSize}$\leftrightarrow$\end{minipage}\begin{minipage}{\similaritySize}\includegraphics[width = \similaritySize]{figures/expert-97-trim.png}\end{minipage}
%%     \end{tabular}
%% \\&\\    \hline
%%     %\\
%% %    \multicolumn{1}{c|}{}&&\\\cline{2-3}
%% %    \multicolumn{1}{c|}{}&&\\ \multicolumn{1}{c|}{}
%%     %& %% \begin{tabular}{l}
%%     %%   \begin{minipage}{\similaritySize}\includegraphics[width = \similaritySize]{figures/expert-52-trim.png}\end{minipage}\begin{minipage}{\similarityArrowSize}$\leftrightarrow$\end{minipage}\begin{minipage}{\similaritySize}\includegraphics[width = \similaritySize]{figures/expert-79-trim.png}\end{minipage}\\\\
%%     %%   \begin{minipage}{\similaritySize}\includegraphics[width = \similaritySize]{figures/expert-16-trim.png}\end{minipage}\begin{minipage}{\similarityArrowSize}$\leftrightarrow$\end{minipage}\begin{minipage}{\similaritySize}\includegraphics[width = \similaritySize]{figures/expert-17-trim.png}\end{minipage}
%%     %% \end{tabular}
%%     %&
%%     %% \begin{tabular}{l}
%%     %%   \begin{minipage}{\similaritySize}\includegraphics[width = \similaritySize]{figures/expert-75-extra-trim.png}\end{minipage}\begin{minipage}{\similarityArrowSize}$\leftrightarrow$\end{minipage}\begin{minipage}{\similaritySize}\includegraphics[width = \similaritySize]{figures/expert-89-trim.png}\end{minipage}\\\\
%%     %%   \begin{minipage}{\similaritySize}\includegraphics[width = \similaritySize]{figures/expert-3-trim.png}\end{minipage}\begin{minipage}{\similarityArrowSize}$\leftrightarrow$\end{minipage}\begin{minipage}{\similaritySize}\includegraphics[width = \similaritySize]{figures/expert-39-trim.png}\end{minipage}
%%     %% \end{tabular}
%% %\\ \multicolumn{1}{c|}{}&&\\\cline{2-3}\cline{2-3}
%%   \end{tabular}
  
%%   \centering
%% \caption{Pairs of images either close together or far apart in different features spaces.  The symbol $\leftrightarrow$ points to the compared images. Features of the program capture abstract notions like symmetry and repetition. Distance metric over images is $L_{\text{learned}}(\cdot|\cdot)$ (see Sec.~\ref{generalizingTheHandDrawings}). Similarity of programs captures high-level features like repetition and symmetry, whereas similarity of images corresponds to similar drawing commands being in similar places. See Sec. 2 of the supplement for more examples.}\label{similarityMatrix}  \end{figure}

%% We project these features down to a
%% 2-dimensional subspace using primary component analysis
%% (PCA); see Fig.\ref{NMF}.  One could use many
%% alternative similarity metrics between drawings which would capture pixel-level similarities while missing high-level geometric similarities.
%% We used our learned distance metric between traces, $L_{\text{learned}}(\cdot|\cdot)$, and projected to a 2-dimensional subspace using multidimensional scaling (MDS:~\cite{cox2008multidimensional}). This reveals similarities between the objects in the drawings,
%% while missing similarities at the level of the program.
%% \begin{figure}[h]
%% \centering  \begin{minipage}{\textwidth}\centering
%%   %\includegraphics[width = 0.69\textwidth]{figures/PCA.png}
%%   \includegraphics[width = 0.69\textwidth]{figures/PCA_improved.png}
%%     \caption{PCA on features of the programs that were synthesized for each drawing. Symmetric figures cluster to the right; ``loopy'' figures cluster to the left; complicated programs are at the top and simple programs are at the bottom.}    \label{NMF}
%%   \end{minipage} %\hfill
%%   \begin{minipage}{\textwidth}\centering
%%     \includegraphics[width = 0.69\textwidth]{figures/imageSimilarity.png} 
%%     \caption{MDS on drawings using the learned distance metric, $L_{\text{learned}}(\cdot|\cdot)$. Drawings with similar looking parts in similar locations are clustered together.}
%%   \end{minipage}
%% \end{figure}

\subsection{Extrapolating figures}
Having access to the source code of a graphics program facilitates coherent, high-level image editing.
For example,
we could change all of the circles to squares or make all of the lines be dashed,
or we can (automatically) extrapolate figures
by increasing the number of times that loops are executed.
Extrapolating repetitive visuals patterns comes naturally to humans,
and is a practical application:
imagine hand drawing a repetitive graphical model structure
and having our system automatically induce and extend the pattern.
Fig.~\ref{extrapolationFigure} shows extrapolations produced by our system.
\begin{figure}[H]\centering
  \includegraphics[width = 0.885\textwidth]{figures/extrapolationMatrix1.png}
  \includegraphics[width = 0.885\textwidth]{figures/extrapolationMatrix2.png}
  \includegraphics[width = 0.885\textwidth]{figures/extrapolationMatrix3.png}  
   \caption{Top, white: drawings. Bottom, black: extrapolations automatically produced by our system.}
  \label{extrapolationFigure}\vspace{-0.5cm}
  \end{figure}
%\input{extrapolations.tex}




%\pagebreak
%\section{Discussion}

\section{Related work}

\textbf{Program Induction:}
Our approach to learning to search for programs draws theoretical
underpinnings from Levin
search~\citep{levin1973universal,solomonoff1984optimum} and
Schmidhuber's OOPS model~\citep{schmidhuber2004optimal}.
DeepCoder~\citep{BalGauBroetal16} is a recent model which, like ours, learns to predict likely program components.
Our work differs by identifying and modeling
the trade-off between tractability and probability of success.
TerpreT~\citep{gaunt2016terpret} 
systematically compares constraint-based program synthesis techniques
against gradient-based search methods, like those used to train
Differentiable Neural Computers~\citep{graves2016hybrid}.  The TerpreT
experiments motivate our use of constraint-based techniques.
Neurally Guided Deductive Search (NGDS:~\cite{ngds})
is a recent neurosymbolic approach;
combining our work with ideas from NGDS could be promising.




\textbf{Deep Learning:} Our neural network combines the architectural ideas of Attend-Infer-Repeat~\citep{eslami1603attend} -- which learns to decompose an image into its constituent objects -- with the training regime and SMC inference  of Neurally Guided Procedural Modeling~\cite{ritchie2016neurally} -- which learns to control procedural graphics programs.
The very recent SPIRAL~\cite{ganin2018synthesizing}  system
learns to infer
procedures for controlling a `pen' to derender highly diverse natural images,
complementing our focus here on more abstract
procedures but less natural images.
%AIR learns an iterative inference scheme which infers objects one by one and also decides when to stop inference.
%; this is similar to our approach's first stage, which parses images into traces. Our approach further produces interpretable, symbolic programs which generate those traces.
%Our network  differs in its architecture and training regime: AIR learns a recurrent auto-encoding model via variational inference, whereas our parsing stage learns an autoregressive-style model from randomly-generated (trace, image) pairs.
%Finally, while AIR was evaluated on multi-MNIST images and synthetic 3D scenes, we focus on hand-drawn sketches.
IM2LATEX~\citep{im2latex} and pix2code~\cite{DBLP:journals/corr/Beltramelli17} are recent works that 
 derender  \LaTeX~ equations and GUIs, respectively,
both recovering a markup-like representation.
Our goal is to go from
noisy input to a high-level program,
which goes beyond markup languages by supporting
programming constructs like loops and conditionals.
%Recovering a high-level program is more challenging than recovering markup
%because it is a highly under constrained symbolic reasoning problem.


%Our image-to-trace parsing architecture builds on prior work on controlling procedural graphics programs~\citep{ritchie2016neurally}.
%Given a program which generates random 2D recursive structures such as vines, that system learns a structurally-identical ``guide program'' whose output can be directed, via neural networks, to resemble a given target image. 
%% We adapt this method to a different visual domain (figures composed of multiple objects), using a broad prior over possible scenes as the initial program and viewing the trace through the guide program as a symbolic parse of the target image.
%% We then show how to efficiently synthesize higher-level programs from these traces.


\textbf{Hand-drawn sketches:} Sketch-n-Sketch is a bi-directional editing system where direct manipulations to a program's output automatically propagate to the program source code~\citep{Hempel:2016:SSP:2984511.2984575}. This work  compliments our own: programs produced by our method could be provided to a Sketch-n-Sketch-like system as a starting point for further editing.
Other systems in the computer graphics literature convert sketches to procedural representations, e.g. using a convolutional network to match a sketch to the output of a parametric 3D modeling system in~\citep{huang2017shape} or supporting interactive sketch-based instantiation of procedural primitives in~\citep{Nishida:2016:ISU:2897824.2925951}
In contrast, we seek to automatically infer a programmatic representation capturing higher-level visual patterns.
The CogSketch system~\citep{forbus2011cogsketch} also aims to have a high-level understanding of hand-drawn figures. Their  goal is cognitive modeling, whereas we are interested in building an automated AI application.% (e.g. in our system the user need not annotate which strokes correspond to which shapes; our neural network produces something equivalent to the annotations).
%\vspace{-0.51cm}
\section{Contributions}

We have presented a system for inferring graphics programs which generate \LaTeX-style figures from hand-drawn images using a combination of learning, stochastic search, and program synthesis.
%We evaluated our model's performance at parsing novel images, and we demonstrated its ability to extrapolate from provided drawings.
In the near future, we believe it will be possible to produce professional-looking figures just by drawing them and then letting an AI write the code.
More generally, we believe the problem of inferring visual programs is a promising direction for research in machine perception.
\subsubsection*{Acknowledgments} We are grateful for advice from Will Grathwohl and Jiajun Wu on  the neural architecture. Funding from NSF GRFP, NSF Award 
\#1753684, the MUSE program (DARPA grant FA8750-14-2-0242),
and AFOSR award FA9550-16-1-0012.

\bibliographystyle{unsrt}
{\small \bibliography{main}}

\end{document}
