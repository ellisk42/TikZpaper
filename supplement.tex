\documentclass{article}

% if you need to pass options to natbib, use, e.g.:
% \PassOptionsToPackage{numbers, compress}{natbib}
% before loading nips_2016
%
% to avoid loading the natbib package, add option nonatbib:
% \usepackage[nonatbib]{nips_2016}


\usepackage{dsfont}

% to compile a camera-ready version, add the [final] option, e.g.:
\usepackage[final]{nips_2017}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography

\usepackage{listings}
\usepackage{amsthm}
% use Times
\usepackage{times}
% For figures
\usepackage{graphicx} % more modern
%\usepackage{epsfig} % less modern
\usepackage{subfig} 
\usepackage{fancyvrb}


\usepackage{caption}
\usepackage{subcaption}

\fvset{fontsize=\footnotesize}

\usepackage{amssymb}
\usepackage{listings}
\usepackage{wrapfig}
\usepackage{tabularx}


\usepackage{verbatim}
 \usepackage{booktabs}
 % For algorithms
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{tikz}
% As of 2011, we use the hyperref package to produce hyperlinks in the
% resulting PDF.  If this breaks your system, please commend out the
% following usepackage line and replace \usepackage{icml2016} with
% \usepackage[nohyperref]{icml2016} above.
\usepackage{amsmath}
\usepackage{hyperref}
\DeclareMathOperator*{\argmin}{arg\,min} % thin space, limits underneath in displays
\DeclareMathOperator*{\argmax}{arg\,max} % thin space, limits underneath in displays
\DeclareMathOperator{\argmin}{argmin} % no space, limits underneath in displays



% Packages hyperref and algorithmic misbehave sometimes.  We can fix
% this with the following command.

\newcommand{\Expect}{\mathds{E}} %{{\rm I\kern-.3em E}}
\newcommand{\probability}{\mathds{P}} %{{\rm I\kern-.3em P}}

\newtheorem{proposition}{Proposition}

\newcommand\modt{\stackrel{\mathclap{\normalfont 2}}{\equiv}}


\title{Supplement to: Learning to Infer Graphics Programs from Hand-Drawn Images}

% The \author macro works with any number of authors. There are two
% commands used to separate the names and addresses of multiple
% authors: \And and \AND.
%
% Using \And between authors leaves it to LaTeX to determine where to
% break the lines. Using \AND forces a line break at that point. So,
% if LaTeX puts 3 of 4 authors names on the first line, and the last
% on the second line, try using \AND instead of \And before the third
% author name.

\author{
Kevin Ellis\\
  MIT\\
  \texttt{ellisk@mit.edu} \\
  \And
  Daniel Ritchie\\
Brown University\\
 \texttt{daniel\_ritchie@stanford.edu} \\
 \And
 Armando Solar-Lezama\\
 MIT\\
\texttt{asolar@csail.mit.edu} \\
\And
Joshua B. Tenenbaum \\
MIT\\
\texttt{jbt@mit.edu}
  %% Affiliation \\
  %% Address \\
  %% \texttt{email} \\
  %% \And
  %% Coauthor \\
  %% Affiliation \\
  %% Address \\
  %% \texttt{email} \\
  %% \And
  %% Coauthor \\
  %% Affiliation \\
  %% Address \\
  %% \texttt{email} \\
}

\begin{document}
% \nipsfinalcopy is no longer used

\maketitle

\section{Correcting errors made by the neural network}\label{synthesizerHelpsParsing}
The program synthesizer can help correct errors from the execution trace proposal network by favoring execution traces which lead to more concise or general programs.
For example, one generally prefers figures with perfectly aligned objects over figures whose parts are slightly misaligned -- and precise alignment lends itself to short programs.
Similarly, figures often have repeated parts,
which the program synthesizer might be able to model as a loop or reflectional symmetry.
So, in considering several candidate traces proposed by the neural network,
we might prefer traces whose best programs have desirable features such being short or having iterated structures.

Concretely, we implemented the following scheme: for an image $I$, the neurally guided sampling scheme of section 3 of the main paper samples a set of candidate traces, written $\mathcal{F}(I)$.
Instead of predicting the most likely trace in $\mathcal{F}(I)$ according to the neural network,
we can take into account the programs that best explain the traces. 
Writing $\hat{T}(I)$ for the trace the model predicts for image $I$,
\begin{equation}
\hat{T}(I) = \argmax_{T\in \mathcal{F}(I)} L_{\text{learned}}(I | \text{render}(T))\times \probability_\theta[T|I] \times\probability_{\beta} [ \text{program}(T)] 
\end{equation}
where $\probability_{\beta} [\cdot]$ is a prior probability
distribution over programs parameterized by $\beta$.
This is equivalent to doing
MAP inference in a generative model where the program is first drawn
from $\probability_{\beta} [\cdot]$, then the program is executed deterministically,
and then we observe a noisy version of the program's output, where $L_\text{learned}(I|\text{render}(\cdot))\times\probability_\theta[\cdot|I]$
is our observation model.

Given a corpus of graphics program synthesis problems with annotated ground truth traces (i.e. $(I,T)$ pairs),
we find a maximum likelihood estimate of $\beta$:
\begin{equation}
  \beta^* = \argmax_{\beta} \expect \left[ \log \frac{\probability_{\beta} [\text{program}(T)]\times L_{\text{learned}}(I|\text{render}(T))\times \probability_\theta[T|I]}{\sum_{T'\in \mathcal{F}(I)} \probability_{\beta} [\text{program}(T')]\times L_{\text{learned}}(I|\text{render}(T'))\times \probability_\theta[T'|I]} \right]
\end{equation}
where the expectation is taken both over the model predictions and the
$(I,T)$ pairs in the training corpus.  We define $\probability_{\beta}
[\cdot]$ to be a log linear distribution $\propto \exp
(\beta\cdot \phi(\text{program}))$, where $\phi(\cdot)$ is a feature
extractor for programs.  We extract a few basic features of a
program, such as its size and how many loops it has, and use these
features to help predict whether a trace is the correct explanation
for an image.

We synthesized programs for the top 10 traces
output by the deep network.  Learning this prior over programs can
help correct mistakes made by the neural network, and also
occasionally introduces mistakes of its own; see
Fig.~\ref{exampleOfProgramCorrectingMistake} for a representative
example of the kinds of corrections that it makes. On the whole
it modestly improves our Top-1 accuracy from 63\% to 67\%.  Recall that
from Fig. 6 of the main paper that the best improvement
in accuracy we could possibly get is 70\% by looking at the top 10 traces. 
\begin{wrapfigure}{R}{5.5cm}
  \includegraphics[width = 5cm]{figures/programSuccess7.png}
  \includegraphics[width = 5cm]{figures/programSuccess16.png}
  \caption{Left: hand drawing. Center: interpretation favored by the deep network. Right: interpretation favored after learning a prior over programs. Our learned prior favors shorter, simpler programs, thus (top example) continuing the pattern of not having an arrow is preferred, or (bottom example) continuing the binary search tree is preferred.}\label{exampleOfProgramCorrectingMistake}
\end{wrapfigure}

\section{Neural network architecture}

\subsection{Convolutional network}
The convolutional network takes as input 2 $256\times 256$ images
represented as a $2\times 256\times 256$ volume. These are
passed through two layers of convolutions separated by ReLU
nonlinearities and max pooling:
\begin{itemize}
\item Layer 1: 20 $8\times 8$ convolutions, 2 $16\times 4$ convolutions, 2 $4\times 16$ convolutions. Followed by $8\times 8$ pooling with a stride size of 4.
\item Layer 2: 10 $8\times 8$ convolutions. Followed by $4\times 4$ pooling with a stride size of 4.
\end{itemize}


\subsection{Autoregressive decoding of drawing commands}

Given the image features $f$, we predict the first token (i.e., the name of the drawing command: \verb|circle|, \verb|rectangle|, \verb|line|, or \verb|STOP|) using logistic regression:
\begin{equation}
  \probability [t_1]\propto \exp\left( W_{t_1}f + b_{t_1}\right)
\end{equation}
where $W_{t_1}$ is a learned weight matrix and  $b_{t_1}$ is a learned bias vector.

Given an attention mechanism $a(\cdot | \cdot)$, subsequent tokens are predicted as:
\begin{equation}
  \probability [t_n|t_{1:(n - 1)}]\propto \text{MLP}_{t_1,n}(a(f|t_{1:(n - 1)}) \oplus \bigoplus_{j < n} \text{oneHot}(t_j))\label{discreteTokenPrediction}
\end{equation}
Thus each token of each drawing primitive has its own learned MLP.
For predicting the coordinates of lines we found that using 32 hidden nodes with sigmoid activations worked well;
for other tokens the MLP's are just logistic regression (no hidden nodes).

We use Spatial Transformer Networks~\cite{jaderberg2015spatial}
as our attention mechanism.
The parameters of the spatial transform are predicted on the basis of previously predicted tokens.
For example, in order to decide where to focus our attention when predicting the $y$ coordinate of a circle,
we condition upon both the identity of the drawing command (\verb|circle|) and upon the value of the previously predicted $x$ coordinate:
\begin{equation}
  a(f|t_{1:(n - 1)}) = \text{AffineTransform}(f, \text{MLP}_{t_1,n}(\bigoplus_{j < n}\text{oneHot}(t_j)))
  \label{spatialTransformEquation}
\end{equation}
So, we learn a different network for predicting special transforms
\emph{for each drawing command} (value of $t_1$) and also \emph{for each token of the drawing command}.
These networks ($\text{MLP}_{t_1,n}$ in equation~\ref{spatialTransformEquation}) have no hidden layers and
output the 6 entries of an affine transformation matrix; see~\cite{jaderberg2015spatial}
for more details.

Training takes a little bit less than a day on a Nvidia TitanX GPU.
The network was trained on $10^5$ synthetic examples.

\subsection{Predicting continuous coordinates using Mixture Density Networks}

Our main paper describes a neural model for parsing hand drawings
into~\LaTeX, but comes with the restriction that all of the
coordinates of all the drawing commands are snapped to a $16\times 16$
grid. Now we describe a variant of our model that
can predict continuous real valued coordinates.

The key obstacle to overcome is \emph{multimodality}: if the network
is unsure whether it should predict a coordinate value of $a$ or $b$,
we wish to predict a distribution that splits its mass between $a$ and
$b$, rather than predicting the single value $\frac{a + b}{2}$.  In
the model where everything is snapped to a discrete grid, supporting
this multimodality falls out naturally from our parameterization of
the network predictions (i.e., the network predicts the parameters of
a multinomial distribution). To predict continuous real valued
coordinates we turn to Mixture Density Networks (MDN:~\cite{MDN}).  In
an MDN, a distribution over a real-valued random variable is
parameterized using a mixture of Gaussians, and the deep network
predicts the parameters of the Gaussian mixture.

Concretely,
we modify our autoregressive drawing command decoder to
predict distributions over real-valued coordinates
as:
\begin{equation}
  p(t_n|t_{1:(n - 1)}) = \text{MDN}_{t_1,n} (a(f|t_{1:(n - 1)}) \oplus \bigoplus_{j < n} t_j)\text{, if }n^{\text{th}}\text{ token is continuous}\label{continuousDensityEquation}
\end{equation}
Compare this equation with Equation~\ref{discreteTokenPrediction} to see that we have only
replaced a draw from a multinomial (whose parameters are predicted by an MLP)
with a draw from a mixture of Gaussians (whose parameters are predicted by an MDN).
Otherwise the two models are identical.

For an MDN $i$ with $K$ mixture components that takes as input $x$, we parameterize it's output distribution as:
\begin{eqnarray}
  \text{MDN}_i(x) &=& \sum_{1\leq k\leq K} \pi_{i,k}(x)\times \mathcal{N}(\mu_{i,k}(x),\sigma^2_{i,k}(x))\\
  \mu_{i,k}(x)& = &\text{sigmoid} (W^{\mu_{i,k}}x + b^{\mu_{i,k}}x)\label{mixtureMean}
  \\\sigma^2_{i,k}(x)& = &\log \left(1 + \exp\left(W^{\sigma^2_{i,k}}x + b^{\sigma^2_{i,k}}x\right) \right)
  \\\pi_{i,k}(x)& = &\frac{\exp\left(W^{\pi_{i,k}}x + b^{\pi_{i,k}}x \right)}{\sum_{k'} \exp\left(W^{\pi_{i,k'}}x + b^{\pi_{i,k'}}x \right)}
  \end{eqnarray}
where $W$ with a superscript is a learned weight matrix and $b$ is a
learned bias vector. Notice that Equation~\ref{mixtureMean} normalizes
the predicted means to be within $[0,1]$: this is because we wish to
constrain the predicted coordinates to lie within the plane
$[0,1]^2$. We used $K = 16$ mixture components.

Figure~\ref{syntheticContinuous}  shows the behavior of the MDN variant of our model on noisy synthetic drawings.
Contrast these mediocre results with Figure~4 of the main paper to see that although
our main model is essentially at ceiling for parsing scenes like these,
the MDN variant fares poorly in comparison.
\begin{figure}
  \includegraphics[width = \textwidth]{figures/syntheticContinuous.png}
  \caption{Left column: \LaTeX~output corrupted by noise process designed to resemble the variability is introduced by hand drawings. Right columns: samples from the MDN variant of the model.}\label{syntheticContinuous}
\end{figure}

We evaluated the MDN version of our model on hand drawings by (1)
performing a beam search in order to find the trace maximizing
$\probability [T|I]$, and (2) constraining the predicted coordinates
to lie on the $16\times 16$ grid. We constrain the predicted coordinates
to lie on the grid by integrating the predicted densities about each grid location.
So, using the probability density $p(t_n|t_{1:(n - 1)})$ in Eq.~\ref{continuousDensityEquation},
we compute a probability mass function $\probability [t_n|t_{1:(n - 1)}]$ as:
\begin{equation}
  \probability [t_n = t|t_{1:(n - 1)}] = \int_{t - \frac{1}{2}}^{t + \frac{1}{2}} \mathrm{d}t_n \; p(t_n|t_{1:(n - 1)})\label{constrainingEquation}
\end{equation}
Even after using Eq.~\ref{constrainingEquation} to constrain the predicted coordinates to lie on the $16\times 16$ grid, the MDN variant of the model only produces finished traces for 48/100 of the drawings.
See Figure~\ref{continuousParses} to see the traces recovered by the beam search.

\begin{figure}
  \includegraphics[width = \textwidth]{figures/continuousParses.png}
  \caption{Parsing hand drawings using the MDN variant of our
    model. Black-on-white: hand drawings. White-on-black, below each
    hand drawing: the parse inferred by the model.}\label{continuousParses} 
  \end{figure}
\subsection{LSTM Baseline}

We  compared our deep network with a baseline that models the problem as a kind of image captioning.
Given the target image, this baseline produces the program trace in one shot by
using a CNN to extract features of the input which are passed to an LSTM which finally predicts
the trace token-by-token.
This general architecture is used in several successful neural models of image captioning (e.g.,~\cite{vinyals2015show}).

Concretely, we kept the image feature extractor architecture (a CNN) as in our model,
but only passed it one image as input (the target image to explain).
Then, instead of using an autoregressive decoder to predict a single drawing command,
we used an LSTM to predict a sequence of drawing commands token-by-token.
This LSTM had 128 memory cells,
and at each time step produced as output the next token in the sequence of drawing commands.
It took as input both the image representation and its previously predicted token.


\subsection{A learned likelihood surrogate}

Our architecture for
$L_{\text{learned}}(\text{render}(T_1)|\text{render}(T_2))$ has the
same series of convolutions as the network that predicts the next
drawing command. We train it to predict two scalars: $|T_1 - T_2|$ and
$|T_2 - T_1|$.  These predictions are made using linear regression
from the image features followed by a ReLU nonlinearity; this
nonlinearity makes sense because the predictions can never be negative
but could be arbitrarily large positive numbers.

We train this network by sampling random synthetic scenes for $T_1$,
and then perturbing them in small ways to produce $T_2$.
We minimize the squared loss between the network's prediction and the ground truth symmetric differences.
$T_1$ is rendered in a ``simulated hand drawing'' style which we describe next.

\section{Simulating hand drawings}

We introduce noise into the \LaTeX~rendering process by:

\begin{itemize}
\item Rescaling the image intensity by a factor chosen uniformly at random from $[0.5,1.5]$
\item Translating the image by $\pm 3$ pixels chosen uniformly random
\item Rendering the \LaTeX~using the \verb|pencildraw| style,
  which adds random perturbations to the paths drawn by \LaTeX in a way designed to resemble a pencil.
\item Randomly perturbing the positions and sizes of primitive  \LaTeX drawing commands
\end{itemize}

\section{Likelihood surrogate for synthetic data}
For synthetic data (e.g., \LaTeX~output)
it is relatively straightforward to engineer an adequate distance measure between images,
because it is possible for the system to discover
drawing commands that exactly match the pixels in the target image.
We use:
\begin{equation}
  -\log L(I_1|I_2) = \sum_{1\leq x\leq 256} \sum_{1\leq y\leq 256} |I_1[x,y] - I_2[x,y]|\begin{cases}
    \alpha\text{, if }$I_1[x,y] > I_2[x,y]$\\
    \beta\text{, if }$I_1[x,y] <  I_2[x,y]$\\
        0\text{, if }$I_1[x,y] = I_2[x,y]$\\
    \end{cases}
  \end{equation}
where $\alpha$, $\beta$ are constants that control the trade-off
between preferring to explain the pixels in the image (at the expense
of having extraneous pixels) and not predicting pixels where they
don't exist (at the expense of leaving some pixels unexplained). Because our sampling procedure incrementally constructs the scene part-by-part,
we want $\alpha > \beta$.
That is, it is preferable to leave some pixels unexplained;
for once a particle in SMC adds a drawing primitive to its trace that is not actually in the latent scene,
it can never recover from this error.
In our experiments on synthetic data we used $\alpha = 0.8$ and $\beta = 0.04$. 
\section{Generating synthetic training data}

We generated synthetic training data for the neural network by
sampling \LaTeX~code according to the following generative process:
First,
the number of objects in the scene are sampled uniformly from 1 to 8.
For each object we uniformly sample its identity (circle, rectangle, or line).
Then we sample the parameters of the circles,
than the parameters of the rectangles,
and finally the parameters of the lines;
this has the effect of teaching the network to first draw the circles in the scene,
then the rectangles,
and finally the lines.
We furthermore put the circle (respectively, rectangle and line) drawing commands in order by
left-to-right, bottom-to-top;
thus the training data enforces a canonical order in which to draw any scene.

To make the training data look more like naturally occurring figures,
we put a Chinese restaurant process prior~\cite{gershman2012tutorial} over
the values of the X and Y coordinates that occur in the execution trace.
This encourages reuse of coordinate values,
and so produces training data that tends to have parts that are nicely aligned.

In the synthetic training data we excluded any sampled scenes that had
overlapping drawing commands.  As shown in the main paper, the network
is then able to generalize to scenes with, for example, intersecting
lines or lines that penetrate a rectangle.

When sampling the endpoints of a line,
we biased the sampling process so that it would
be more likely to start an endpoint
along one of the sides of a rectangle or at the boundary of a circle.
If $n$ is the number of points either along the side of a rectangle or at the boundary of a circle,
we would sample an arbitrary endpoint with probability $\frac{2}{2 + n}$
and sample one of the ``attaching'' endpoints with probability $\frac{1}{2 + n}$.

See figure~\ref{exampleTrainingData} for examples of the kinds of scenes that the network is trained on.
\begin{figure}
  \begin{minipage}[t]{2.2cm}\fbox{\includegraphics[width=2cm]{figures/randomScene-58-7.png}}\end{minipage}
  \begin{minipage}[t]{2.2cm}\fbox{\includegraphics[width=2cm]{figures/randomScene-33-5.png}}\end{minipage}
  \begin{minipage}[t]{2.2cm}\fbox{\includegraphics[width=2cm]{figures/randomScene-4-4.png}}\end{minipage}
  \begin{minipage}[t]{2.2cm}\fbox{\includegraphics[width=2cm]{figures/randomScene-26-7.png}}\end{minipage}
      \begin{minipage}[t]{2.2cm}\fbox{\includegraphics[width=2cm]{figures/randomScene-25-4.png}}\end{minipage}\\
      \begin{minipage}[t]{2.2cm}\fbox{\includegraphics[width=2cm]{figures/randomScene-22-3.png}}\end{minipage}
      \begin{minipage}[t]{2.2cm}\fbox{\includegraphics[width=2cm]{figures/randomScene-20-7.png}}\end{minipage}
      \begin{minipage}[t]{2.2cm}\fbox{\includegraphics[width=2cm]{figures/randomScene-19-3.png}}\end{minipage}
      \begin{minipage}[t]{2.2cm}\fbox{\includegraphics[width=2cm]{figures/randomScene-17-7.png}}\end{minipage}
      \begin{minipage}[t]{2.2cm}\fbox{\includegraphics[width=2cm]{figures/randomScene-1-4.png}}\end{minipage}
      \caption{Example synthetic training data}\label{exampleTrainingData}
\end{figure}

For readers wishing to generate their own synthetic training sets,
we refer them to our source code at:~\url{https://github.com/ellisk42/TikZ}.

\section{The cost function for programs}

We seek the minimum cost program which evaluates to (produces the drawing primitives in) an execution trace $T$:
\begin{equation}
  \text{program}(T) = \argmin_{\substack{p\in \text{DSL}\\p \text{ evaluates to } T}} \text{cost}(p)\label{programObjective}
\end{equation}

Programs incur a cost of 1 for each command (primitive drawing action,
loop, or reflection).  They incur a cost of $\frac{1}{3}$ for each
unique coefficient they use in a linear transformation beyond the
first coefficient. This encourages reuse of coefficients, which leads
to code that has translational symmetry; rather than provide a
translational symmetry operator as we did with reflection, we modify
what is effectively a prior over the space of program so that it tends
to produce programs that have this symmetry.

Programs also incur a cost of 1 for having loops of constant length 2;
otherwise there is often no pressure from the cost function to explain
a repetition of length 2 as being a reflection rather a loop.

%% \section{Neural networks for guiding SMC}



%% Let $L(\cdot | \cdot):\text{image}^2\to \mathcal{R}$ be our likelihood
%% function: it takes two images, an observed target image and a
%% hypothesized program output, and gives the likelihood of the observed
%% image conditioned on the program output. We want to sample from:
%% \begin{equation}
%% \probability [p|x]  \propto L(x | \text{render}(p)) \probability [p]
%% \end{equation}
%% where $\probability [p]$ is the prior probability of program $p$, and $x$ is the observed image.

%% Let $p$ be a program with $L$ lines, which we will write as $p = (p_1,p_2,\cdots,p_L)$. Assume the prior factors into:
%% \begin{equation}
%%   \probability [p]\propto \prod_{l\leq L}\probability [p_l]
%% \end{equation}
%% Define the distribution $q_L(\cdot)$, which happens to be proportional to the above posterior:
%% \begin{equation}
%%   q_L(p_1,p_2,\cdots,p_{L - 1},p_L)\propto q_{L - 1}(p_1,p_2,\cdots,p_{L - 1})\times \frac{L(x | \text{render}(p_1,p_2,\cdots,p_{L - 1},p_L))}{L(x | \text{render}(p_1,p_2,\cdots,p_{L - 1}))}\times\probability [p_L]
%% \end{equation}
%% Now suppose we have some samples from $q_{L - 1}(\cdot)$, and that we
%% then sample a $p_L$ from a distribution proportional to $\frac{L(x |
%%   \text{render}(p_1,p_2,\cdots,p_{L - 1},p_L))}{L(x |
%%   \text{render}(p_1,p_2,\cdots,p_{L - 1}))}\times\probability [p_L]$.
%% The resulting programs $p$ are distributed according to $q_L$, and so
%% are also distributed according to $\probability [p|x]$.

%% How do we sample $p_L$ from a distribution proportional to $\frac{L(x
%%   | \text{render}(p_1,p_2,\cdots,p_{L - 1},p_L))}{L(x |
%%   \text{render}(p_1,p_2,\cdots,p_{L - 1}))}\times\probability [p_L]$?
%% We have a neural network that takes as input the target image $x$ and
%% the program so far, and produces a distribution over next lines of
%% code ($p_L$).  We write $\text{NN}(p_L | p_1,\cdots,p_{L - 1};x)$ for
%% the distribution output by the neural network. So we can sample from NN and then weight the samples by:
%% \begin{equation}
%%   w(p_L) = \frac{\probability [p_L]}{\text{NN}(p_L | p_1,\cdots,p_{L - 1};x)}\times \frac{L(x | \text{render}(p_1,p_2,\cdots,p_{L - 1},p_L))}{L(x | \text{render}(p_1,p_2,\cdots,p_{L - 1}))}
%% \end{equation}
%% Then we can resample from these now weighted samples to get a new
%% population of particles (here programs are particles), where each
%% program now has $L$ lines instead of $L - 1$.

%% This procedure can be seen as a particle filter, where each successive
%% latent variable is another line of code, and the emission
%% probabilities are successive ratios of likelihoods under $L(\cdot |
%% \cdot)$.


%%   \begin{algorithm}[tb]
%%    \caption{Neurally guided SMC}
%%    \label{guideAlgorithm}
%% \begin{algorithmic}
%%   \STATE {\bfseries Input:} Neural network NN, beam size $N$, maximum length $L$, target image $x$
%%   \STATE {\bfseries Output:} Samples of the program trace
%%   \STATE Set $B_0 = \{\text{empty program}\}$
%%   \FOR{$1\leq l\leq L$}
%%   \FOR{$1\leq n\leq N$}
%%   \STATE{ $p_n\sim \text{Uniform}(B_{l - 1})$}
%%   \STATE{ $p'_{n}\sim \text{NN}(\text{render}(p),x)$}
%%   \STATE{ Define $r_n = p'_n\cdot p_n$}
%%   \STATE{ Set $\tilde{w}(r_n) = \frac{L(x|r_n)}{L(x|p_n)}\times\frac{\probability [p'_n]}{\probability [p'_n = \text{NN}(\text{render}(p),x)]}$}
%%   \ENDFOR
%%   \STATE{ Define $w(p) = \frac{\tilde{w}(p)}{\sum_{p'}\tilde{w}(p')}$}
%%   \STATE{ Set $B_l$ to be $N$ samples from $r_n$ distributed according to $w(\cdot)$}
%%   \ENDFOR
%%   \STATE {\bfseries return} $\{p : p\in B_{l\leq L}, p \text{ is finished}\}$
%% \end{algorithmic}
%%   \end{algorithm}

\section{Full results on drawings data set}

Below we show our full data set of drawings. The leftmost column is a hand drawing. The middle column is a rendering of the most likely trace discovered by the neurally guided SMC sampling scheme. The rightmost column is the program we synthesized from a ground truth execution trace of the drawing.
\input{synthesizerOutputs.tex}

 


\bibliographystyle{unsrt}
{\small \bibliography{main}}
 
\end{document}
