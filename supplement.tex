\documentclass{article}

% if you need to pass options to natbib, use, e.g.:
% \PassOptionsToPackage{numbers, compress}{natbib}
% before loading nips_2016
%
% to avoid loading the natbib package, add option nonatbib:
% \usepackage[nonatbib]{nips_2016}


\usepackage{dsfont}

% to compile a camera-ready version, add the [final] option, e.g.:
\usepackage{nips_2017}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography

\usepackage{listings}
\usepackage{amsthm}
% use Times
\usepackage{times}
% For figures
\usepackage{graphicx} % more modern
%\usepackage{epsfig} % less modern
\usepackage{subfig} 
\usepackage{fancyvrb}


\usepackage{caption}
\usepackage{subcaption}

\fvset{fontsize=\footnotesize}

\usepackage{amssymb}
\usepackage{listings}
\usepackage{wrapfig}
\usepackage{tabularx}


\usepackage{verbatim}
 \usepackage{booktabs}
 % For algorithms
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{tikz}
% As of 2011, we use the hyperref package to produce hyperlinks in the
% resulting PDF.  If this breaks your system, please commend out the
% following usepackage line and replace \usepackage{icml2016} with
% \usepackage[nohyperref]{icml2016} above.
\usepackage{amsmath}
\usepackage{hyperref}
\DeclareMathOperator*{\argmin}{arg\,min} % thin space, limits underneath in displays
\DeclareMathOperator{\argmin}{argmin} % no space, limits underneath in displays



% Packages hyperref and algorithmic misbehave sometimes.  We can fix
% this with the following command.

\newcommand{\Expect}{\mathds{E}} %{{\rm I\kern-.3em E}}
\newcommand{\probability}{\mathds{P}} %{{\rm I\kern-.3em P}}

\newtheorem{proposition}{Proposition}

\newcommand\modt{\stackrel{\mathclap{\normalfont 2}}{\equiv}}


\title{Supplement to: Inferring Graphics Programs from Images}

% The \author macro works with any number of authors. There are two
% commands used to separate the names and addresses of multiple
% authors: \And and \AND.
%
% Using \And between authors leaves it to LaTeX to determine where to
% break the lines. Using \AND forces a line break at that point. So,
% if LaTeX puts 3 of 4 authors names on the first line, and the last
% on the second line, try using \AND instead of \And before the third
% author name.

\author{
Kevin Ellis \\
Brain and Cognitive Sciences\\
MIT\\
%Pittsburgh, PA 15213 \\
\texttt{ellisk@mit.edu} \\
Daniel Ritchie\\
Department of Computer Science\\
Brown
\And
Armando Solar-Lezama \\
  CSAIL\\
MIT \\
\texttt{asolar@csail.mit.edu} \\
\And
Joshua B. Tenenbaum \\
Brain and Cognitive Sciences\\
MIT\\
\texttt{jbt@mit.edu} \\
}

\begin{document}
% \nipsfinalcopy is no longer used

\maketitle

\section{Neural network architecture}

\subsection{Convolutional network}
The convolutional network takes as input 2 $256\times 256$ images
represented as a $2\times 256\times 256\times$ volume. These are
passed through two layers of convolutions separated by ReLU
nonlinearities and max pooling:
\begin{itemize}
\item Layer 1: 20 $8\times 8$ convolutions, 2 $16\times 4$ convolutions, 2 $4\times 16$ convolutions. Followed by $8\times 8$ pooling with a stride size of 4.
\item Layer 2: 10 $8\times 8$ convolutions. Followed by $4\times 4$ pooling with a stride size of 4.
\end{itemize}
Training takes a little bit less than a day on a Nvidia TitanX GPU.
The network was trained on $10^5$ synthetic examples.

\subsection{Autoregressive decoding of drawing commands}

Given the image features $f$, we predict the first token using logistic regression:
\begin{equation}
  \probability [T_1]\propto W_{T_1}f
\end{equation}
where $W_{T_1}$ is a learned weight matrix.

Subsequent tokens are predicted as:
\begin{equation}
  \probability [T_n|T_{1:(n - 1)}]\propto \text{MLP}_{T_1,n}(I \otimes \bigotimes_{j < n} \text{oneHot}(T_j))
\end{equation}
Thus each token of each drawing primitive has its own learned MLP.
For predicting the coordinates of lines we found that using 32 hidden nodes with sigmoid activations worked well;
for other tokens the MLP's are just logistic regression (no hidden nodes).

\subsection{A learned likelihood surrogate}

Our architecture for
$L_{\text{learned}}(\text{render}(T_1)|\text{render}(T_2))$ has the
same series of convolutions as the network that predicts the next
drawing command. We train it to predict two scalars: $|T_1 - T_2|$ and
$|T_2 - T_1|$.  These predictions are made using linear regression
from the image features followed by a ReLU nonlinearity; this
nonlinearity makes sense because the predictions can never be negative
but could be arbitrarily large positive numbers.

We train this network by sampling random synthetic scenes for $T_1$,
and then perturbing them in small ways to produce $T_2$.
We minimize the squared loss between the network's prediction and the ground truth symmetric differences.
$T_1$ is rendered in a ``simulated hand drawing'' style which we describe next.

\section{Simulating hand drawings}

We introduce noise into the rendering process by:

\begin{itemize}
\item Rescaling the image intensity by a factor chosen uniformly at random from $[0.5,1.5]$
\item Translating the image by $\pm 3$ pixels chosen uniformly random
\item Rendering the \LaTeX~using the \verb|pencildraw| style,
  which adds random perturbations to the paths drawn by \LaTeX in a way designed to resemble a pencil.
\item Randomly perturbing the positions and sizes of primitive  \LaTeX drawing commands
  \end{itemize}

%% \section{Neural networks for guiding SMC}



%% Let $L(\cdot | \cdot):\text{image}^2\to \mathcal{R}$ be our likelihood
%% function: it takes two images, an observed target image and a
%% hypothesized program output, and gives the likelihood of the observed
%% image conditioned on the program output. We want to sample from:
%% \begin{equation}
%% \probability [p|x]  \propto L(x | \text{render}(p)) \probability [p]
%% \end{equation}
%% where $\probability [p]$ is the prior probability of program $p$, and $x$ is the observed image.

%% Let $p$ be a program with $L$ lines, which we will write as $p = (p_1,p_2,\cdots,p_L)$. Assume the prior factors into:
%% \begin{equation}
%%   \probability [p]\propto \prod_{l\leq L}\probability [p_l]
%% \end{equation}
%% Define the distribution $q_L(\cdot)$, which happens to be proportional to the above posterior:
%% \begin{equation}
%%   q_L(p_1,p_2,\cdots,p_{L - 1},p_L)\propto q_{L - 1}(p_1,p_2,\cdots,p_{L - 1})\times \frac{L(x | \text{render}(p_1,p_2,\cdots,p_{L - 1},p_L))}{L(x | \text{render}(p_1,p_2,\cdots,p_{L - 1}))}\times\probability [p_L]
%% \end{equation}
%% Now suppose we have some samples from $q_{L - 1}(\cdot)$, and that we
%% then sample a $p_L$ from a distribution proportional to $\frac{L(x |
%%   \text{render}(p_1,p_2,\cdots,p_{L - 1},p_L))}{L(x |
%%   \text{render}(p_1,p_2,\cdots,p_{L - 1}))}\times\probability [p_L]$.
%% The resulting programs $p$ are distributed according to $q_L$, and so
%% are also distributed according to $\probability [p|x]$.

%% How do we sample $p_L$ from a distribution proportional to $\frac{L(x
%%   | \text{render}(p_1,p_2,\cdots,p_{L - 1},p_L))}{L(x |
%%   \text{render}(p_1,p_2,\cdots,p_{L - 1}))}\times\probability [p_L]$?
%% We have a neural network that takes as input the target image $x$ and
%% the program so far, and produces a distribution over next lines of
%% code ($p_L$).  We write $\text{NN}(p_L | p_1,\cdots,p_{L - 1};x)$ for
%% the distribution output by the neural network. So we can sample from NN and then weight the samples by:
%% \begin{equation}
%%   w(p_L) = \frac{\probability [p_L]}{\text{NN}(p_L | p_1,\cdots,p_{L - 1};x)}\times \frac{L(x | \text{render}(p_1,p_2,\cdots,p_{L - 1},p_L))}{L(x | \text{render}(p_1,p_2,\cdots,p_{L - 1}))}
%% \end{equation}
%% Then we can resample from these now weighted samples to get a new
%% population of particles (here programs are particles), where each
%% program now has $L$ lines instead of $L - 1$.

%% This procedure can be seen as a particle filter, where each successive
%% latent variable is another line of code, and the emission
%% probabilities are successive ratios of likelihoods under $L(\cdot |
%% \cdot)$.


%%   \begin{algorithm}[tb]
%%    \caption{Neurally guided SMC}
%%    \label{guideAlgorithm}
%% \begin{algorithmic}
%%   \STATE {\bfseries Input:} Neural network NN, beam size $N$, maximum length $L$, target image $x$
%%   \STATE {\bfseries Output:} Samples of the program trace
%%   \STATE Set $B_0 = \{\text{empty program}\}$
%%   \FOR{$1\leq l\leq L$}
%%   \FOR{$1\leq n\leq N$}
%%   \STATE{ $p_n\sim \text{Uniform}(B_{l - 1})$}
%%   \STATE{ $p'_{n}\sim \text{NN}(\text{render}(p),x)$}
%%   \STATE{ Define $r_n = p'_n\cdot p_n$}
%%   \STATE{ Set $\tilde{w}(r_n) = \frac{L(x|r_n)}{L(x|p_n)}\times\frac{\probability [p'_n]}{\probability [p'_n = \text{NN}(\text{render}(p),x)]}$}
%%   \ENDFOR
%%   \STATE{ Define $w(p) = \frac{\tilde{w}(p)}{\sum_{p'}\tilde{w}(p')}$}
%%   \STATE{ Set $B_l$ to be $N$ samples from $r_n$ distributed according to $w(\cdot)$}
%%   \ENDFOR
%%   \STATE {\bfseries return} $\{p : p\in B_{l\leq L}, p \text{ is finished}\}$
%% \end{algorithmic}
%%   \end{algorithm}

\section{Full results on drawings data set}

\input{synthesizerOutputs.tex}
t
\end{document}
